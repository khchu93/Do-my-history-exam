{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c114b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation System for Board Game Manuals\n",
    "============================================\n",
    "A retrieval-augmented generation (RAG) system with coverage-based evaluation.\n",
    "\n",
    "Key Features:\n",
    "- PDF text extraction with normalization\n",
    "- Ground truth Q&A annotation integration using Aho-Corasick pattern matching\n",
    "- Coverage-based relevance scoring (measures how much of a relevant span is in a chunk)\n",
    "- DCG/nDCG metrics for retrieval quality evaluation\n",
    "\n",
    "Adapted from: \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "Author: [Your Name]\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "# import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import ahocorasick\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import Dataset\n",
    "\n",
    "# import gc\n",
    "\n",
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "daa7b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class RAGEvaluationError(Exception):\n",
    "    \"\"\"Base exception for RAG evaluation system\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoadError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document loading fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AnnotationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when Q&A annotation processing fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ChunkingError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document chunking fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class VectorStoreError(RAGEvaluationError):\n",
    "    \"\"\"Raised when vector store operations fail\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EvaluationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when metric calculation fails\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c40c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to handle encoding inconsistencies between PDF and JSON.\n",
    "    \n",
    "    This is critical because:\n",
    "    - PDFs may have curly quotes/apostrophes: \"\", '', '\n",
    "    - JSON files typically use straight quotes: \", '\n",
    "    - Mismatches break pattern matching for ground truth annotation\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text with standardized quotes and collapsed whitespace\n",
    "    \"\"\"\n",
    "    # Convert curly quotes to straight quotes\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    \n",
    "    # Collapse all whitespace (newlines, tabs, multiple spaces) to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF and clean text content.\n",
    "    \n",
    "    Why cleaning matters:\n",
    "    - PDFs often have inconsistent spacing/newlines\n",
    "    - Normalized text improves embedding quality\n",
    "    - Standardized format makes pattern matching reliable\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the board game manual PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects (one per page) with cleaned text\n",
    "        \n",
    "    Raises:\n",
    "        DocumentLoadError: If PDF cannot be loaded or is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not Path(pdf_path).exists():\n",
    "            raise DocumentLoadError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading PDF from: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        if not page_docs:\n",
    "            raise DocumentLoadError(f\"No content extracted from PDF: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(page_docs)} pages from PDF\")\n",
    "        \n",
    "        # Clean text and filter metadata\n",
    "        for page_doc in page_docs:\n",
    "            # Normalize whitespace\n",
    "            clean_text = normalize_text(page_doc.page_content)\n",
    "            page_doc.page_content = clean_text\n",
    "            \n",
    "            # Keep only essential metadata to avoid Chroma serialization issues\n",
    "            allowed_keys = {\"source\", \"page\"}\n",
    "            page_doc.metadata = {\n",
    "                k: v for k, v in page_doc.metadata.items() \n",
    "                if k in allowed_keys\n",
    "            }\n",
    "        \n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, DocumentLoadError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading PDF: {str(e)}\")\n",
    "        raise DocumentLoadError(f\"Failed to load PDF: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35a7e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CHUNKS STORE & RETRIEVE\n",
    "# =============================================================================\n",
    "\n",
    "def save_chunks(chunks, path):\n",
    "    serializable = [\n",
    "        {\n",
    "            \"content\": c.page_content,\n",
    "            \"metadata\": c.metadata\n",
    "        }\n",
    "        for c in chunks\n",
    "    ]\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(serializable, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_saved_chunks(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    chunks = [\n",
    "        Document(page_content=item[\"content\"], metadata=item[\"metadata\"])\n",
    "        for item in raw\n",
    "    ]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "086959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GROUND TRUTH ANNOTATION INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON file containing training Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON data\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If file cannot be loaded or parsed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(json_path).exists():\n",
    "            raise AnnotationError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded JSON from: {json_path}\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to parse JSON: {str(e)}\") from e\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading JSON: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to load JSON: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def load_training_qa_to_docs(training_qas_path: str, page_docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Annotate documents with ground truth relevance spans using Aho-Corasick.\n",
    "    \n",
    "    Why Aho-Corasick?\n",
    "    - Efficient multi-pattern matching: O(n + m + z) vs O(n*m) for naive search\n",
    "    - n = document length, m = total pattern length, z = matches\n",
    "    - Critical when searching 100+ patterns across large documents\n",
    "    \n",
    "    Process:\n",
    "    1. Build automaton with all relevant chunks from training Q&A\n",
    "    2. Scan each page once to find all matching spans\n",
    "    3. Store span metadata (qa_id, page, start/end indices)\n",
    "    \n",
    "    Args:\n",
    "        training_qas_path: Path to JSON with training Q&A pairs\n",
    "        page_docs: List of Document objects from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Documents annotated with relevance_spans in metadata\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If annotation process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = load_json(training_qas_path)\n",
    "        training_qas = training_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not training_qas:\n",
    "            logger.warning(\"No training Q&As found in JSON\")\n",
    "            return page_docs\n",
    "        \n",
    "        logger.info(f\"Processing {len(training_qas)} training Q&A pairs\")\n",
    "        \n",
    "        # Build Aho-Corasick automaton for efficient pattern matching\n",
    "        automaton = ahocorasick.Automaton()\n",
    "        \n",
    "        for qa_idx, qa in enumerate(training_qas):\n",
    "            qa[\"relevance_spans\"] = []  # Initialize spans list\n",
    "            \n",
    "            for chunk_text in qa.get(\"relevant_chunks\", []):\n",
    "                chunk_text_normalized = normalize_text(chunk_text)\n",
    "                \n",
    "                # Store tuple: (qa_index, original_chunk_text)\n",
    "                # qa_index allows us to map back to the question\n",
    "                automaton.add_word(chunk_text_normalized, (qa_idx, chunk_text_normalized))\n",
    "        \n",
    "        automaton.make_automaton()  # Compile the automaton\n",
    "        logger.info(\"Aho-Corasick automaton built successfully\")\n",
    "        \n",
    "        # Search all pages for relevant spans\n",
    "        total_spans = 0\n",
    "        for page_doc in page_docs:\n",
    "            page_text = normalize_text(page_doc.page_content)\n",
    "            page_num = page_doc.metadata.get(\"page\")\n",
    "            page_doc.metadata[\"relevance_spans\"] = []\n",
    "            \n",
    "            # Iterate through all matches in this page\n",
    "            for end_idx, (qa_idx, chunk_text) in automaton.iter(page_text):\n",
    "                start_idx = end_idx - len(chunk_text) + 1  # +1 because end_idx is inclusive\n",
    "                \n",
    "                span = {\n",
    "                    \"qa_id\": training_qas[qa_idx][\"id\"],\n",
    "                    \"page\": page_num,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx + 1  # Make end exclusive for easier indexing\n",
    "                }\n",
    "                page_doc.metadata[\"relevance_spans\"].append(span)\n",
    "                total_spans += 1\n",
    "        \n",
    "        logger.info(f\"Found {total_spans} relevance spans across all pages\")\n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Annotation failed: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to annotate documents: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_text(docs: List[Document], chunk_size: int = 300, chunk_overlap: int = 30) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    \n",
    "    Why chunk?\n",
    "    - Embeddings work better on focused, semantic units\n",
    "    - Smaller chunks = more precise retrieval\n",
    "    - Overlap ensures we don't split important context\n",
    "    \n",
    "    Why these defaults?\n",
    "    - chunk_size=300: ~75 tokens, good for rule-specific content\n",
    "    - chunk_overlap=30: 10% overlap preserves context at boundaries\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects\n",
    "        chunk_size: Target size for each chunk (characters)\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk Documents with start_index in metadata\n",
    "        \n",
    "    Raises:\n",
    "        ChunkingError: If chunking process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chunk_size <= 0:\n",
    "            raise ChunkingError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "        \n",
    "        if chunk_overlap < 0:\n",
    "            raise ChunkingError(f\"chunk_overlap cannot be negative, got {chunk_overlap}\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ChunkingError(\n",
    "                f\"chunk_overlap ({chunk_overlap}) must be less than \"\n",
    "                f\"chunk_size ({chunk_size})\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Splitting documents with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,  # Use character count\n",
    "            add_start_index=True  # Critical: needed for coverage calculation\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ChunkingError):\n",
    "            raise\n",
    "        logger.error(f\"Chunking failed: {str(e)}\")\n",
    "        raise ChunkingError(f\"Failed to split documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COVERAGE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_overlap(span_start: int, span_end: int, chunk_start: int, chunk_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Compute character overlap between a relevance span and a chunk.\n",
    "    \n",
    "    Example:\n",
    "        Span:  [10, 30)  (relevant text from annotation)\n",
    "        Chunk: [20, 50)  (text chunk)\n",
    "        Overlap: [20, 30) = 10 characters\n",
    "    \n",
    "    Args:\n",
    "        span_start: Start index of relevance span\n",
    "        span_end: End index of relevance span (exclusive)\n",
    "        chunk_start: Start index of chunk\n",
    "        chunk_end: End index of chunk (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Number of overlapping characters\n",
    "    \"\"\"\n",
    "    overlap_start = max(span_start, chunk_start)\n",
    "    overlap_end = min(span_end, chunk_end)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "\n",
    "def generate_relevant_chunks_with_coverage(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate coverage scores for chunks containing ground truth spans.\n",
    "    \n",
    "    Coverage = (overlap_length / relevance_span_length)\n",
    "    \n",
    "    Why coverage?\n",
    "    - Measures \"how much of the relevant content is in this chunk\"\n",
    "    - Coverage=1.0: entire relevant span is in the chunk (perfect)\n",
    "    - Coverage=0.5: only half the relevant content is present\n",
    "    - Coverage=0.0: chunk doesn't contain relevant content\n",
    "    \n",
    "    This is better than binary relevance because:\n",
    "    - Distinguishes between partial and complete matches\n",
    "    - Handles cases where spans cross chunk boundaries\n",
    "    - Provides granular relevance scores for nDCG calculation\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of Documents containing only relevant chunks with coverage scores\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If coverage calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = chunk.metadata.get(\"start_index\", 0)\n",
    "            chunk_end = chunk_start + len(chunk.page_content)\n",
    "            relevance_spans = chunk.metadata.get(\"relevance_spans\", [])\n",
    "            \n",
    "            # Skip chunks without any ground truth annotations\n",
    "            if not relevance_spans:\n",
    "                continue\n",
    "            \n",
    "            # Create copy to avoid modifying original\n",
    "            annotated_chunk = copy.deepcopy(chunk)\n",
    "            annotated_chunk.metadata[\"coverage_per_query\"] = []\n",
    "            \n",
    "            for span in relevance_spans:\n",
    "                qa_id = span[\"qa_id\"]\n",
    "                \n",
    "                # Calculate how much of the span overlaps with this chunk\n",
    "                overlap_len = compute_overlap(\n",
    "                    span[\"start\"], span[\"end\"], \n",
    "                    chunk_start, chunk_end\n",
    "                )\n",
    "                \n",
    "                relevance_len = span[\"end\"] - span[\"start\"]\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if relevance_len == 0:\n",
    "                    logger.warning(f\"Zero-length relevance span for qa_id={qa_id}\")\n",
    "                    continue\n",
    "                \n",
    "                coverage = overlap_len / relevance_len\n",
    "                \n",
    "                # Skip queries with no overlap\n",
    "                if coverage == 0:\n",
    "                    continue\n",
    "                \n",
    "                annotated_chunk.metadata[\"coverage_per_query\"].append({\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"coverage\": coverage\n",
    "                })\n",
    "            \n",
    "            # Only keep chunks that have at least one relevant query\n",
    "            if annotated_chunk.metadata[\"coverage_per_query\"]:\n",
    "                annotated_chunk.metadata[\"chunk_id\"] = chunk_idx\n",
    "                relevant_chunks.append(annotated_chunk)\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_chunks)} relevant chunks out of {len(chunks)} total\")\n",
    "        return relevant_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Coverage calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate coverage: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_chunks_for_chroma(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter complex metadata for Chroma compatibility.\n",
    "    \n",
    "    Why needed?\n",
    "    - Chroma only supports simple types (str, int, float, bool)\n",
    "    - Complex types (lists, dicts) cause serialization errors\n",
    "    - We keep complex metadata in separate 'relevant_chunks' list\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Documents with filtered metadata safe for Chroma\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If metadata filtering fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrievable_docs = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Filter to simple metadata types\n",
    "            filtered_doc = filter_complex_metadata([chunk])[0]\n",
    "            \n",
    "            # Add chunk_id for later lookup\n",
    "            filtered_doc.metadata[\"chunk_id\"] = chunk_idx\n",
    "            \n",
    "            retrievable_docs.append(filtered_doc)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(retrievable_docs)} chunks for Chroma\")\n",
    "        return retrievable_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metadata filtering failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to prepare chunks: {str(e)}\") from e\n",
    "\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: List[Document], \n",
    "                   embedding_model: str = \"text-embedding-ada-002\", \n",
    "                   similarity_search: str = \"cosine\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist Chroma vector store.\n",
    "    \n",
    "    Note: This clears existing database!\n",
    "    - Ensures fresh embeddings\n",
    "    - Avoids stale data issues\n",
    "    - For production, consider incremental updates\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks (with simple metadata)\n",
    "        embedding_model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Initialized Chroma vector store\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If vector store creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an isolated temporary directory\n",
    "        tmp_dir = tempfile.mkdtemp(prefix=\"chroma_eval_\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Create the Chroma vector store from documents\n",
    "        db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=tmp_dir,\n",
    "            collection_metadata={\"hnsw:space\": similarity_search}\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Temporary Chroma DB created at: {tmp_dir}\")\n",
    "        return db, tmp_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Vector store creation failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to create vector store: {str(e)}\") from e\n",
    "\n",
    "def retrieve_top_k(\n",
    "    db: Chroma, \n",
    "    query: str, \n",
    "    k: int = 3\n",
    ") -> List[Tuple[str, str, int, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar chunks for a query.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (source, content, chunk_id, relevance_score)\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if k <= 0:\n",
    "            raise VectorStoreError(f\"k must be positive, got {k}\")\n",
    "        \n",
    "        logger.debug(f\"Retrieving top-{k} chunks for query: {query[:50]}...\")\n",
    "        \n",
    "        results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "        \n",
    "        formatted_results = [\n",
    "            (\n",
    "                doc.metadata.get(\"source\", \"unknown\"),\n",
    "                doc.page_content,\n",
    "                doc.metadata.get(\"chunk_id\", -1),\n",
    "                score\n",
    "            )\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to retrieve documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def dcg(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain.\n",
    "    \n",
    "    Formula: DCG = Σ(rel_i / log2(i + 2)) for i in range(len(scores))\n",
    "    \n",
    "    Why log2(i + 2)?\n",
    "    - Position 0: log2(2) = 1 (no discount)\n",
    "    - Position 1: log2(3) = 1.58 (small discount)\n",
    "    - Position 2: log2(4) = 2 (larger discount)\n",
    "    - Later positions are increasingly discounted\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (coverage values)\n",
    "        \n",
    "    Returns:\n",
    "        DCG score\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        dcg_value = np.sum([\n",
    "            rel / np.log2(idx + 2)\n",
    "            for idx, rel in enumerate(relevance_scores)\n",
    "        ])\n",
    "        \n",
    "        return float(dcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"DCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate DCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    nDCG = DCG / IDCG\n",
    "    \n",
    "    Why normalize?\n",
    "    - Makes scores comparable across queries\n",
    "    - Range: [0, 1] where 1 = perfect ranking\n",
    "    - Accounts for different numbers of relevant items\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores\n",
    "        \n",
    "    Returns:\n",
    "        nDCG score between 0 and 1\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG with actual ranking\n",
    "        dcg_value = dcg(relevance_scores)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg_value = dcg(ideal_scores)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg_value == 0:\n",
    "            logger.warning(\"IDCG is 0, returning nDCG=0\")\n",
    "            return 0.0\n",
    "        \n",
    "        ndcg_value = dcg_value / idcg_value\n",
    "        return float(ndcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"nDCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate nDCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def get_coverage(chunk_id: int, qa_id: str, relevant_chunks: List[Document]) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve coverage score for a specific chunk and query.\n",
    "    \n",
    "    This is a lookup function that connects:\n",
    "    - Retrieved chunk (by chunk_id from vector search)\n",
    "    - Query (by qa_id from evaluation set)\n",
    "    - Ground truth coverage (pre-computed in relevant_chunks)\n",
    "    \n",
    "    Args:\n",
    "        chunk_id: ID of the retrieved chunk\n",
    "        qa_id: ID of the query being evaluated\n",
    "        relevant_chunks: List of annotated chunks with coverage scores\n",
    "        \n",
    "    Returns:\n",
    "        Coverage score (0-1), or 0 if not found\n",
    "    \"\"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if chunk.metadata.get(\"chunk_id\") != chunk_id:\n",
    "            continue\n",
    "        \n",
    "        for coverage_entry in chunk.metadata.get(\"coverage_per_query\", []):\n",
    "            if coverage_entry[\"qa_id\"] == qa_id:\n",
    "                return coverage_entry[\"coverage\"]\n",
    "    \n",
    "    # Return 0 if chunk has no coverage for this query\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EVALUATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rag_system(pdf_path: str, training_qa_path: str,chunk_size: int = 300,chunk_overlap: int = 30,\n",
    "    k: int = 3, embedding_model: str = \"text-embedding-ada-002\", similarity_search: str = \"cosine\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete RAG evaluation pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load and clean PDF\n",
    "    2. Annotate with ground truth Q&A\n",
    "    3. Chunk documents\n",
    "    4. Calculate coverage for relevant chunks\n",
    "    5. Create vector store\n",
    "    6. For each query: retrieve top-k and calculate metrics\n",
    "    7. Report average DCG and nDCG\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to board game manual PDF\n",
    "        training_qa_path: Path to training Q&A JSON\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "        embedding_model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "        \n",
    "    Raises:\n",
    "        RAGEvaluationError: If any pipeline stage fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting RAG Evaluation Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load PDF\n",
    "        docs = load_documents(pdf_path)\n",
    "        \n",
    "        # Step 2: Annotate with ground truth\n",
    "        docs_with_qa = load_training_qa_to_docs(training_qa_path, docs)\n",
    "        \n",
    "        if \n",
    "        # Step 3: Chunk documents\n",
    "        chunks = split_text(docs_with_qa, chunk_size, chunk_overlap)\n",
    "\n",
    "        # Step 3.5: Save/Load Chunks\n",
    "        chunks_path = Path(pdf_path).with_name(\"CATAN_chunks.json\")\n",
    "        save_chunks(chunks, chunks_path)\n",
    "        # chunks = load_saved_chunks(chunks_path)\n",
    "        \n",
    "        # Step 4: Calculate coverage for relevant chunks\n",
    "        relevant_chunks = generate_relevant_chunks_with_coverage(chunks)\n",
    "        \n",
    "        # Step 5: Prepare and store in vector DB\n",
    "        chunks_for_chroma = prepare_chunks_for_chroma(chunks)\n",
    "        db, tmp_dir  = save_to_chroma(chunks_for_chroma, embedding_model, similarity_search)\n",
    "\n",
    "        # Step 6: Load evaluation queries\n",
    "        qa_data = load_json(training_qa_path)\n",
    "        evaluation_qas = qa_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not evaluation_qas:\n",
    "            raise EvaluationError(\"No evaluation queries found in JSON\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on {len(evaluation_qas)} queries\")\n",
    "        \n",
    "        # Step 7: Evaluate each query\n",
    "        dcg_values = []\n",
    "        ndcg_values = []\n",
    "        query_results = []\n",
    "        \n",
    "        for qa in evaluation_qas:\n",
    "            qa_id = qa.get(\"id\")\n",
    "            question = qa.get(\"question\")\n",
    "            gt_answer = qa.get(\"answer\")\n",
    "            \n",
    "            if not question:\n",
    "                logger.warning(f\"Skipping query with missing question: {qa_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Retrieve top-k chunks\n",
    "            top_k_results = retrieve_top_k(db, question, k=k)\n",
    "            top_k = []\n",
    "\n",
    "            # Calculate coverage scores for retrieved chunks\n",
    "            coverage_scores = []\n",
    "            for source, content, chunk_id, similarity_score in top_k_results:\n",
    "                coverage = get_coverage(chunk_id, qa_id, relevant_chunks)\n",
    "                coverage_scores.append(coverage)\n",
    "                top_k.append(content)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            query_dcg = dcg(coverage_scores)\n",
    "            query_ndcg = ndcg_at_k(coverage_scores)\n",
    "            \n",
    "            dcg_values.append(query_dcg)\n",
    "            ndcg_values.append(query_ndcg)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"qa_id\": qa_id,\n",
    "                \"question\": question,\n",
    "                \"top_k_content\": top_k,\n",
    "                \"gt_answer\": gt_answer,\n",
    "                \"coverage_scores\": coverage_scores,\n",
    "                \"dcg\": query_dcg,\n",
    "                \"ndcg\": query_ndcg\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_dcg = float(np.mean(dcg_values))\n",
    "        avg_ndcg = float(np.mean(ndcg_values))\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Average DCG:  {avg_dcg:.4f}\")\n",
    "        logger.info(f\"Average nDCG: {avg_ndcg:.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"avg_dcg\": avg_dcg,\n",
    "            \"avg_ndcg\": avg_ndcg,\n",
    "            \"num_queries\": len(evaluation_qas),\n",
    "            \"k\": k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"query_results\": query_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, RAGEvaluationError):\n",
    "            raise\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise RAGEvaluationError(f\"Evaluation pipeline failed: {str(e)}\") from e\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: List[str], isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 22:45:09,956 - INFO - Evaluating with parameters: chunk=300, overlap=30, top-k=3\n",
      "2025-11-14 22:45:09,958 - INFO - ============================================================\n",
      "2025-11-14 22:45:09,958 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-14 22:45:09,959 - INFO - ============================================================\n",
      "2025-11-14 22:45:09,960 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n",
      "2025-11-14 22:45:22,325 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-14 22:45:22,328 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_eval_small.json\n",
      "2025-11-14 22:45:22,329 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-14 22:45:22,330 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-14 22:45:22,332 - INFO - Found 11 relevance spans across all pages\n",
      "2025-11-14 22:45:22,333 - INFO - Splitting documents with chunk_size=300, overlap=30\n",
      "2025-11-14 22:45:22,345 - INFO - Created 100 chunks\n",
      "2025-11-14 22:45:22,355 - INFO - Found 15 relevant chunks out of 100 total\n",
      "2025-11-14 22:45:22,356 - INFO - Prepared 100 chunks for Chroma\n",
      "2025-11-14 22:45:23,978 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-14 22:45:26,236 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:26,825 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_eval_small.json\n",
      "2025-11-14 22:45:26,827 - INFO - Evaluating on 10 queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_5uto0nob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 22:45:27,761 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:28,068 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:28,072 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 22:45:28,280 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:29,096 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:29,101 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 22:45:29,928 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:30,728 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:30,959 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:32,009 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:33,077 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:33,080 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-14 22:45:33,800 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:33,805 - INFO - ============================================================\n",
      "2025-11-14 22:45:33,806 - INFO - Average DCG:  0.6197\n",
      "2025-11-14 22:45:33,806 - INFO - Average nDCG: 0.6182\n",
      "2025-11-14 22:45:33,807 - INFO - ============================================================\n",
      "2025-11-14 22:45:34,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:35,424 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:36,188 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:36,702 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:37,541 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:38,292 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:38,950 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:39,352 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:40,110 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:40,646 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]2025-11-14 22:45:44,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,065 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,067 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,069 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,070 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,074 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,076 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,078 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,080 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,082 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,084 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,087 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,089 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,091 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:46,094 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:56,371 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:45:57,907 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:45:59,410 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:01,209 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:46:02,270 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:03,753 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:06,081 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:46:07,330 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:09,078 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|▏         | 1/50 [00:35<29:09, 35.70s/it]2025-11-14 22:46:19,550 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|█▍        | 7/50 [00:37<02:50,  3.96s/it]2025-11-14 22:46:21,115 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:22,703 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,483 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,485 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,487 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,489 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,491 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,493 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,494 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,496 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,498 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,500 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,502 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:24,518 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:27,468 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:28,424 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:35,115 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:36,095 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|█▌        | 8/50 [01:07<05:56,  8.50s/it]2025-11-14 22:46:50,864 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:50,866 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:50,868 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:50,870 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|██▍       | 12/50 [01:13<03:11,  5.04s/it]2025-11-14 22:46:57,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:57,288 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:46:58,999 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|██▊       | 14/50 [01:18<02:37,  4.36s/it]2025-11-14 22:47:02,201 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:02,209 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|███       | 15/50 [01:21<02:23,  4.09s/it]2025-11-14 22:47:04,859 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:04,889 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:04,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:04,896 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,814 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,817 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,820 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,826 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,831 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:06,834 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|███▏      | 16/50 [01:23<02:11,  3.86s/it]2025-11-14 22:47:07,707 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|███▍      | 17/50 [01:30<02:28,  4.50s/it]2025-11-14 22:47:14,624 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:14,626 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:16,514 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:17,626 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|███▊      | 19/50 [01:45<02:54,  5.63s/it]2025-11-14 22:47:31,212 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:31,219 - INFO - Retrying request to /embeddings in 0.470667 seconds\n",
      "Evaluating:  44%|████▍     | 22/50 [01:48<01:40,  3.60s/it]2025-11-14 22:47:35,803 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:47:37,251 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:38,249 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:38,996 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|████▌     | 23/50 [01:55<01:49,  4.05s/it]2025-11-14 22:47:40,538 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,138 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,143 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,145 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,147 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,154 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,156 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,159 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,161 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,162 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:42,164 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:44,473 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:44,475 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:46,821 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:47:47,882 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:48,878 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|████▊     | 24/50 [02:13<02:59,  6.91s/it]2025-11-14 22:47:57,375 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,377 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,379 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,383 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,386 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:47:57,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|█████▏    | 26/50 [02:19<02:09,  5.41s/it]2025-11-14 22:48:07,444 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:07,446 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:07,447 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|██████    | 30/50 [02:23<01:04,  3.23s/it]2025-11-14 22:48:08,207 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|██████▏   | 31/50 [02:29<01:07,  3.57s/it]2025-11-14 22:48:12,903 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:12,906 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:12,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:12,915 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:12,922 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|██████▌   | 33/50 [02:32<00:50,  2.97s/it]2025-11-14 22:48:16,844 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:48:17,942 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:18,927 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  68%|██████▊   | 34/50 [02:39<00:59,  3.70s/it]2025-11-14 22:48:23,147 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:23,149 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:23,156 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:23,159 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:23,164 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:23,166 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  72%|███████▏  | 36/50 [02:40<00:36,  2.61s/it]2025-11-14 22:48:24,129 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:31,099 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-14 22:48:32,251 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:33,252 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|███████▍  | 37/50 [02:50<00:52,  4.04s/it]2025-11-14 22:48:34,070 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:34,072 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:34,073 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:34,075 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:34,078 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:35,671 - INFO - Retrying request to /chat/completions in 0.395865 seconds\n",
      "2025-11-14 22:48:41,728 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:41,739 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|████████  | 40/50 [02:59<00:35,  3.59s/it]2025-11-14 22:48:43,251 - INFO - Retrying request to /chat/completions in 0.448187 seconds\n",
      "2025-11-14 22:48:43,252 - INFO - Retrying request to /chat/completions in 0.386098 seconds\n",
      "2025-11-14 22:48:46,403 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:46,405 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:46,407 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  84%|████████▍ | 42/50 [03:07<00:29,  3.73s/it]2025-11-14 22:48:50,999 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|████████▊ | 44/50 [03:08<00:15,  2.65s/it]2025-11-14 22:48:52,807 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:53,825 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:53,827 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:53,829 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:48:53,831 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|█████████ | 45/50 [03:15<00:17,  3.50s/it]2025-11-14 22:49:00,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:00,800 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|█████████▏| 46/50 [03:17<00:12,  3.22s/it]2025-11-14 22:49:03,055 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:03,057 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:05,308 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  94%|█████████▍| 47/50 [03:22<00:10,  3.54s/it]2025-11-14 22:49:06,270 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:06,998 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:07,000 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:09,323 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-14 22:49:09,325 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  96%|█████████▌| 48/50 [03:27<00:07,  3.86s/it]2025-11-14 22:49:12,144 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|██████████| 50/50 [03:29<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "📁 Generation results saved to rag_retrieval_eval.csv\n",
      "📁 Generation results saved to rag_generation_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PDF_PATH = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "    TRAINING_QA_PATH = \"data/BoardGamesRuleBook/CATAN_eval_small.json\"\n",
    "    CHUNK_SIZES = [300]\n",
    "    CHUNK_OVERLAPS = [30]\n",
    "    SIMILARITY_SEARCH = \"cosine\"\n",
    "    Ks = [3]\n",
    "    \n",
    "    retrieval_eval_results = []\n",
    "    generation_eval_results = []\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "    \n",
    "    for CHUNK_SIZE, CHUNK_OVERLAP, k in product(CHUNK_SIZES, CHUNK_OVERLAPS, Ks):\n",
    "        logger.info(f\"Evaluating with parameters: chunk={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top-k={k}\")\n",
    "        try:\n",
    "            # Retrieval evaluation\n",
    "            results = evaluate_rag_system(\n",
    "                pdf_path=PDF_PATH,\n",
    "                training_qa_path=TRAINING_QA_PATH,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                similarity_search=SIMILARITY_SEARCH,\n",
    "                k=k\n",
    "            )\n",
    "\n",
    "            retrieval_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZE,\n",
    "                \"overlap\": CHUNK_OVERLAP,\n",
    "                \"top_k\": k,\n",
    "                **results,\n",
    "            })\n",
    "\n",
    "            # Generation evaluation\n",
    "            evaluation_rows = []\n",
    "            for query_result in results.get(\"query_results\"):\n",
    "                question = query_result.get(\"question\")\n",
    "                top_k_content = query_result.get(\"top_k_content\")\n",
    "                gt_answer = query_result.get(\"gt_answer\")\n",
    "\n",
    "                answer = generate_answer(question, top_k_content)\n",
    "                evaluation_rows.append({\n",
    "                            \"question\": question,\n",
    "                            \"contexts\": top_k_content,\n",
    "                            \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "                            \"reference\": gt_answer,\n",
    "                        })\n",
    "            ragas_eval_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "            # Run evaluation\n",
    "            scores = evaluate(\n",
    "                ragas_eval_dataset,\n",
    "                metrics=[\n",
    "                    answer_correctness,\n",
    "                    answer_relevancy,\n",
    "                    faithfulness,\n",
    "                    context_precision,\n",
    "                    context_recall,\n",
    "                ],\n",
    "                llm=llm,  # pass the LLM explicitly\n",
    "            )\n",
    "\n",
    "            df_score = scores.to_pandas()\n",
    "\n",
    "\n",
    "            generation_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZES,\n",
    "                \"chunk_overlap\": CHUNK_OVERLAPS,\n",
    "                \"embedding_model\": [\"text-embedding-3-small\"],\n",
    "                \"top_k\": Ks,\n",
    "                \"answer_correctness\": np.mean(df_score[\"answer_correctness\"]),\n",
    "                \"answer_relevancy\": np.mean(df_score[\"answer_relevancy\"]),\n",
    "                \"faithfulness\": np.mean(df_score[\"faithfulness\"]),\n",
    "                \"context_precision\": np.mean(df_score[\"context_precision\"]),\n",
    "                \"context_recall\": np.mean(df_score[\"context_recall\"]),\n",
    "            })\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "        except RAGEvaluationError as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Convert to DataFrame for easy comparison, save the .csv\n",
    "    df = pd.DataFrame(retrieval_eval_results)\n",
    "    df.to_csv(\"rag_retrieval_eval.csv\", index=False)\n",
    "    print(\"📁 Generation results saved to rag_retrieval_eval.csv\")\n",
    "    df = pd.DataFrame(generation_eval_results)\n",
    "    df.to_csv(\"rag_generation_eval.csv\", index=False)\n",
    "    print(\"📁 Generation results saved to rag_generation_eval.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2623475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[300]</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[text-embedding-3-small]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>0.492861</td>\n",
       "      <td>0.962708</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_size chunk_overlap           embedding_model top_k  \\\n",
       "0      [300]          [30]  [text-embedding-3-small]   [3]   \n",
       "\n",
       "   answer_correctness  answer_relevancy  faithfulness  context_precision  \\\n",
       "0            0.492861          0.962708      0.458333           0.733333   \n",
       "\n",
       "   context_recall  \n",
       "0             0.9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verification.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
