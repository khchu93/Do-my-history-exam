{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c114b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\GitHubProjects\\Do-my-history-exam\\verification.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation System for Board Game Manuals\n",
    "============================================\n",
    "A retrieval-augmented generation (RAG) system with coverage-based evaluation.\n",
    "\n",
    "Key Features:\n",
    "- PDF text extraction with normalization\n",
    "- Ground truth Q&A annotation integration using Aho-Corasick pattern matching\n",
    "- Coverage-based relevance scoring (measures how much of a relevant span is in a chunk)\n",
    "- DCG/nDCG metrics for retrieval quality evaluation\n",
    "\n",
    "Adapted from: \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "Author: [Your Name]\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "# import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import ahocorasick\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import Dataset\n",
    "\n",
    "# import gc\n",
    "\n",
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa7b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class RAGEvaluationError(Exception):\n",
    "    \"\"\"Base exception for RAG evaluation system\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoadError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document loading fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AnnotationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when Q&A annotation processing fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ChunkingError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document chunking fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class VectorStoreError(RAGEvaluationError):\n",
    "    \"\"\"Raised when vector store operations fail\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EvaluationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when metric calculation fails\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c40c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to handle encoding inconsistencies between PDF and JSON.\n",
    "    \n",
    "    This is critical because:\n",
    "    - PDFs may have curly quotes/apostrophes: \"\", '', '\n",
    "    - JSON files typically use straight quotes: \", '\n",
    "    - Mismatches break pattern matching for ground truth annotation\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text with standardized quotes and collapsed whitespace\n",
    "    \"\"\"\n",
    "    # Convert curly quotes to straight quotes\n",
    "    text = text.replace(\"â€œ\", '\"').replace(\"â€\", '\"')\n",
    "    text = text.replace(\"â€˜\", \"'\").replace(\"â€™\", \"'\")\n",
    "    \n",
    "    # Collapse all whitespace (newlines, tabs, multiple spaces) to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF and clean text content.\n",
    "    \n",
    "    Why cleaning matters:\n",
    "    - PDFs often have inconsistent spacing/newlines\n",
    "    - Normalized text improves embedding quality\n",
    "    - Standardized format makes pattern matching reliable\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the board game manual PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects (one per page) with cleaned text\n",
    "        \n",
    "    Raises:\n",
    "        DocumentLoadError: If PDF cannot be loaded or is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not Path(pdf_path).exists():\n",
    "            raise DocumentLoadError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading PDF from: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        if not page_docs:\n",
    "            raise DocumentLoadError(f\"No content extracted from PDF: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(page_docs)} pages from PDF\")\n",
    "        \n",
    "        # Clean text and filter metadata\n",
    "        for page_doc in page_docs:\n",
    "            # Normalize whitespace\n",
    "            clean_text = normalize_text(page_doc.page_content)\n",
    "            page_doc.page_content = clean_text\n",
    "            \n",
    "            # Keep only essential metadata to avoid Chroma serialization issues\n",
    "            allowed_keys = {\"source\", \"page\"}\n",
    "            page_doc.metadata = {\n",
    "                k: v for k, v in page_doc.metadata.items() \n",
    "                if k in allowed_keys\n",
    "            }\n",
    "        \n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, DocumentLoadError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading PDF: {str(e)}\")\n",
    "        raise DocumentLoadError(f\"Failed to load PDF: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GROUND TRUTH ANNOTATION INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON file containing training Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON data\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If file cannot be loaded or parsed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(json_path).exists():\n",
    "            raise AnnotationError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded JSON from: {json_path}\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to parse JSON: {str(e)}\") from e\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading JSON: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to load JSON: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def load_training_qa_to_docs(training_qas_path: str, page_docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Annotate documents with ground truth relevance spans using Aho-Corasick.\n",
    "    \n",
    "    Why Aho-Corasick?\n",
    "    - Efficient multi-pattern matching: O(n + m + z) vs O(n*m) for naive search\n",
    "    - n = document length, m = total pattern length, z = matches\n",
    "    - Critical when searching 100+ patterns across large documents\n",
    "    \n",
    "    Process:\n",
    "    1. Build automaton with all relevant chunks from training Q&A\n",
    "    2. Scan each page once to find all matching spans\n",
    "    3. Store span metadata (qa_id, page, start/end indices)\n",
    "    \n",
    "    Args:\n",
    "        training_qas_path: Path to JSON with training Q&A pairs\n",
    "        page_docs: List of Document objects from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Documents annotated with relevance_spans in metadata\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If annotation process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = load_json(training_qas_path)\n",
    "        training_qas = training_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not training_qas:\n",
    "            logger.warning(\"No training Q&As found in JSON\")\n",
    "            return page_docs\n",
    "        \n",
    "        logger.info(f\"Processing {len(training_qas)} training Q&A pairs\")\n",
    "        \n",
    "        # Build Aho-Corasick automaton for efficient pattern matching\n",
    "        automaton = ahocorasick.Automaton()\n",
    "        \n",
    "        for qa_idx, qa in enumerate(training_qas):\n",
    "            qa[\"relevance_spans\"] = []  # Initialize spans list\n",
    "            \n",
    "            for chunk_text in qa.get(\"relevant_chunks\", []):\n",
    "                chunk_text_normalized = normalize_text(chunk_text)\n",
    "                \n",
    "                # Store tuple: (qa_index, original_chunk_text)\n",
    "                # qa_index allows us to map back to the question\n",
    "                automaton.add_word(chunk_text_normalized, (qa_idx, chunk_text_normalized))\n",
    "        \n",
    "        automaton.make_automaton()  # Compile the automaton\n",
    "        logger.info(\"Aho-Corasick automaton built successfully\")\n",
    "        \n",
    "        # Search all pages for relevant spans\n",
    "        total_spans = 0\n",
    "        for page_doc in page_docs:\n",
    "            page_text = normalize_text(page_doc.page_content)\n",
    "            page_num = page_doc.metadata.get(\"page\")\n",
    "            page_doc.metadata[\"relevance_spans\"] = []\n",
    "            \n",
    "            # Iterate through all matches in this page\n",
    "            for end_idx, (qa_idx, chunk_text) in automaton.iter(page_text):\n",
    "                start_idx = end_idx - len(chunk_text) + 1  # +1 because end_idx is inclusive\n",
    "                \n",
    "                span = {\n",
    "                    \"qa_id\": training_qas[qa_idx][\"id\"],\n",
    "                    \"page\": page_num,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx + 1  # Make end exclusive for easier indexing\n",
    "                }\n",
    "                page_doc.metadata[\"relevance_spans\"].append(span)\n",
    "                total_spans += 1\n",
    "        \n",
    "        logger.info(f\"Found {total_spans} relevance spans across all pages\")\n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Annotation failed: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to annotate documents: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6787e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:20:24,321 - INFO - Evaluating with parameters: chunk=300, overlap=30, top-k=3\n",
      "2025-11-11 11:20:24,321 - INFO - ============================================================\n",
      "2025-11-11 11:20:24,321 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-11 11:20:24,321 - INFO - ============================================================\n",
      "2025-11-11 11:20:24,321 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n",
      "2025-11-11 11:20:35,991 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-11 11:20:36,008 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-11 11:20:36,009 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-11 11:20:36,009 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-11 11:20:36,013 - INFO - Found 11 relevance spans across all pages\n",
      "2025-11-11 11:20:36,014 - INFO - Splitting documents with chunk_size=300, overlap=30\n",
      "2025-11-11 11:20:36,024 - INFO - Created 100 chunks\n",
      "2025-11-11 11:20:36,027 - INFO - Found 15 relevant chunks out of 100 total\n",
      "2025-11-11 11:20:36,029 - INFO - Prepared 100 chunks for Chroma\n",
      "2025-11-11 11:20:37,303 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-11 11:20:41,987 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:42,295 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-11 11:20:42,295 - INFO - Evaluating on 10 queries\n",
      "2025-11-11 11:20:42,476 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_4nhmfnwk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 11:20:42,790 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:42,801 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-11 11:20:42,984 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:43,207 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:43,211 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-11 11:20:43,435 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:43,564 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:43,696 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:44,089 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:44,575 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:44,581 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-11 11:20:44,761 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:44,773 - INFO - ============================================================\n",
      "2025-11-11 11:20:44,774 - INFO - Average DCG:  0.6197\n",
      "2025-11-11 11:20:44,776 - INFO - Average nDCG: 0.6182\n",
      "2025-11-11 11:20:44,778 - INFO - ============================================================\n",
      "2025-11-11 11:20:45,810 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:46,391 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:46,987 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:47,687 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:48,318 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:48,972 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:49,908 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:50,536 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:51,322 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:52,167 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]2025-11-11 11:20:55,519 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,763 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,765 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,767 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,770 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,773 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,776 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,779 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,782 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,785 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,788 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:56,789 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:20:58,688 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:20:59,526 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:00,321 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:02,190 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:02,191 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|â–         | 1/50 [00:07<06:09,  7.53s/it]2025-11-11 11:21:02,197 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:02,198 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:05,583 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:21:06,439 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:07,365 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:12,659 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:21:13,626 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:14,396 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|â–         | 2/50 [00:21<09:07, 11.40s/it]2025-11-11 11:21:16,305 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:17,617 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:24,312 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:25,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:25,854 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:28,561 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:29,495 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|â–ˆâ–        | 7/50 [00:35<03:11,  4.45s/it]2025-11-11 11:21:31,603 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,611 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,613 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,614 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,616 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,618 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,620 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,622 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,628 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,637 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:31,649 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:38,119 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:21:39,071 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:39,919 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:47,074 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|â–ˆâ–Œ        | 8/50 [00:52<04:44,  6.77s/it]2025-11-11 11:21:47,082 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:47,083 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:47,085 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:47,087 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:53,430 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:53,433 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 12/50 [01:00<02:41,  4.26s/it]2025-11-11 11:21:54,744 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:57,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:57,298 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:04<02:35,  4.21s/it]2025-11-11 11:21:58,672 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,673 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,675 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,676 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,679 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:21:58,679 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:01,173 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:01,175 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 14/50 [01:09<02:37,  4.36s/it]2025-11-11 11:22:03,731 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:03,736 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:03,738 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:03,746 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:09<01:41,  2.98s/it]2025-11-11 11:22:04,376 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:07,669 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:08,652 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [01:16<01:59,  3.64s/it]2025-11-11 11:22:10,696 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:10,698 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:11,417 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:19<01:30,  2.91s/it]2025-11-11 11:22:13,951 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:23<01:07,  2.40s/it]2025-11-11 11:22:19,857 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [01:26<00:52,  2.03s/it]2025-11-11 11:22:22,574 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:22,580 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:30<01:00,  2.42s/it]2025-11-11 11:22:25,182 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,185 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,190 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,192 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,195 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,198 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,201 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:25,204 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:35,428 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,193 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:42<01:48,  4.51s/it]2025-11-11 11:22:36,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,896 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,900 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,903 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,910 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,913 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:36,915 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:42,541 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:22:43,400 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:44,286 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:51<01:41,  4.60s/it]2025-11-11 11:22:46,334 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:46,336 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:46,339 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [01:53<00:51,  2.72s/it]2025-11-11 11:22:47,784 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [01:57<00:53,  2.95s/it]2025-11-11 11:22:51,796 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:51,798 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:53,631 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:53,632 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:54,871 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:54,872 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:54,874 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:54,877 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:54,880 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [02:03<00:39,  2.60s/it]2025-11-11 11:22:58,227 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:58,230 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:58,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:22:58,241 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:01,826 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:23:02,722 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:03,549 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [02:10<00:38,  2.96s/it]2025-11-11 11:23:05,427 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:05,429 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:05,430 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:07,268 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-11 11:23:08,387 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:09,584 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:17<00:45,  3.83s/it]2025-11-11 11:23:12,651 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:12,653 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:12,656 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [02:20<00:38,  3.52s/it]2025-11-11 11:23:16,425 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:25<00:39,  3.91s/it]2025-11-11 11:23:20,224 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:20,226 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:20,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:22,797 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:22,802 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:22,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:22,808 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [02:29<00:34,  3.88s/it]2025-11-11 11:23:25,323 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:25,326 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:33<00:22,  3.16s/it]2025-11-11 11:23:28,409 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:28,411 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:28,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:28,416 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [02:36<00:11,  2.35s/it]2025-11-11 11:23:30,918 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:38<00:09,  2.42s/it]2025-11-11 11:23:33,549 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:33,551 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:40<00:01,  1.47s/it]2025-11-11 11:23:37,122 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-11 11:23:37,908 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:43<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸ“ Generation results saved to rag_retrieval_eval.csv\n",
      "ðŸ“ Generation results saved to rag_generation_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_text(docs: List[Document], chunk_size: int = 300, chunk_overlap: int = 30) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    \n",
    "    Why chunk?\n",
    "    - Embeddings work better on focused, semantic units\n",
    "    - Smaller chunks = more precise retrieval\n",
    "    - Overlap ensures we don't split important context\n",
    "    \n",
    "    Why these defaults?\n",
    "    - chunk_size=300: ~75 tokens, good for rule-specific content\n",
    "    - chunk_overlap=30: 10% overlap preserves context at boundaries\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects\n",
    "        chunk_size: Target size for each chunk (characters)\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk Documents with start_index in metadata\n",
    "        \n",
    "    Raises:\n",
    "        ChunkingError: If chunking process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chunk_size <= 0:\n",
    "            raise ChunkingError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "        \n",
    "        if chunk_overlap < 0:\n",
    "            raise ChunkingError(f\"chunk_overlap cannot be negative, got {chunk_overlap}\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ChunkingError(\n",
    "                f\"chunk_overlap ({chunk_overlap}) must be less than \"\n",
    "                f\"chunk_size ({chunk_size})\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Splitting documents with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,  # Use character count\n",
    "            add_start_index=True  # Critical: needed for coverage calculation\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ChunkingError):\n",
    "            raise\n",
    "        logger.error(f\"Chunking failed: {str(e)}\")\n",
    "        raise ChunkingError(f\"Failed to split documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COVERAGE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_overlap(span_start: int, span_end: int, chunk_start: int, chunk_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Compute character overlap between a relevance span and a chunk.\n",
    "    \n",
    "    Example:\n",
    "        Span:  [10, 30)  (relevant text from annotation)\n",
    "        Chunk: [20, 50)  (text chunk)\n",
    "        Overlap: [20, 30) = 10 characters\n",
    "    \n",
    "    Args:\n",
    "        span_start: Start index of relevance span\n",
    "        span_end: End index of relevance span (exclusive)\n",
    "        chunk_start: Start index of chunk\n",
    "        chunk_end: End index of chunk (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Number of overlapping characters\n",
    "    \"\"\"\n",
    "    overlap_start = max(span_start, chunk_start)\n",
    "    overlap_end = min(span_end, chunk_end)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "\n",
    "def generate_relevant_chunks_with_coverage(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate coverage scores for chunks containing ground truth spans.\n",
    "    \n",
    "    Coverage = (overlap_length / relevance_span_length)\n",
    "    \n",
    "    Why coverage?\n",
    "    - Measures \"how much of the relevant content is in this chunk\"\n",
    "    - Coverage=1.0: entire relevant span is in the chunk (perfect)\n",
    "    - Coverage=0.5: only half the relevant content is present\n",
    "    - Coverage=0.0: chunk doesn't contain relevant content\n",
    "    \n",
    "    This is better than binary relevance because:\n",
    "    - Distinguishes between partial and complete matches\n",
    "    - Handles cases where spans cross chunk boundaries\n",
    "    - Provides granular relevance scores for nDCG calculation\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of Documents containing only relevant chunks with coverage scores\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If coverage calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = chunk.metadata.get(\"start_index\", 0)\n",
    "            chunk_end = chunk_start + len(chunk.page_content)\n",
    "            relevance_spans = chunk.metadata.get(\"relevance_spans\", [])\n",
    "            \n",
    "            # Skip chunks without any ground truth annotations\n",
    "            if not relevance_spans:\n",
    "                continue\n",
    "            \n",
    "            # Create copy to avoid modifying original\n",
    "            annotated_chunk = copy.deepcopy(chunk)\n",
    "            annotated_chunk.metadata[\"coverage_per_query\"] = []\n",
    "            \n",
    "            for span in relevance_spans:\n",
    "                qa_id = span[\"qa_id\"]\n",
    "                \n",
    "                # Calculate how much of the span overlaps with this chunk\n",
    "                overlap_len = compute_overlap(\n",
    "                    span[\"start\"], span[\"end\"], \n",
    "                    chunk_start, chunk_end\n",
    "                )\n",
    "                \n",
    "                relevance_len = span[\"end\"] - span[\"start\"]\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if relevance_len == 0:\n",
    "                    logger.warning(f\"Zero-length relevance span for qa_id={qa_id}\")\n",
    "                    continue\n",
    "                \n",
    "                coverage = overlap_len / relevance_len\n",
    "                \n",
    "                # Skip queries with no overlap\n",
    "                if coverage == 0:\n",
    "                    continue\n",
    "                \n",
    "                annotated_chunk.metadata[\"coverage_per_query\"].append({\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"coverage\": coverage\n",
    "                })\n",
    "            \n",
    "            # Only keep chunks that have at least one relevant query\n",
    "            if annotated_chunk.metadata[\"coverage_per_query\"]:\n",
    "                annotated_chunk.metadata[\"chunk_id\"] = chunk_idx\n",
    "                relevant_chunks.append(annotated_chunk)\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_chunks)} relevant chunks out of {len(chunks)} total\")\n",
    "        return relevant_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Coverage calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate coverage: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_chunks_for_chroma(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter complex metadata for Chroma compatibility.\n",
    "    \n",
    "    Why needed?\n",
    "    - Chroma only supports simple types (str, int, float, bool)\n",
    "    - Complex types (lists, dicts) cause serialization errors\n",
    "    - We keep complex metadata in separate 'relevant_chunks' list\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Documents with filtered metadata safe for Chroma\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If metadata filtering fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrievable_docs = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Filter to simple metadata types\n",
    "            filtered_doc = filter_complex_metadata([chunk])[0]\n",
    "            \n",
    "            # Add chunk_id for later lookup\n",
    "            filtered_doc.metadata[\"chunk_id\"] = chunk_idx\n",
    "            \n",
    "            retrievable_docs.append(filtered_doc)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(retrievable_docs)} chunks for Chroma\")\n",
    "        return retrievable_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metadata filtering failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to prepare chunks: {str(e)}\") from e\n",
    "\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: List[Document], embedding_model: str = \"text-embedding-ada-002\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist Chroma vector store.\n",
    "    \n",
    "    Note: This clears existing database!\n",
    "    - Ensures fresh embeddings\n",
    "    - Avoids stale data issues\n",
    "    - For production, consider incremental updates\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks (with simple metadata)\n",
    "        embedding_model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Initialized Chroma vector store\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If vector store creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an isolated temporary directory\n",
    "        tmp_dir = tempfile.mkdtemp(prefix=\"chroma_eval_\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Create the Chroma vector store from documents\n",
    "        db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=tmp_dir\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Temporary Chroma DB created at: {tmp_dir}\")\n",
    "        return db, tmp_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Vector store creation failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to create vector store: {str(e)}\") from e\n",
    "\n",
    "def retrieve_top_k(\n",
    "    db: Chroma, \n",
    "    query: str, \n",
    "    k: int = 3\n",
    ") -> List[Tuple[str, str, int, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar chunks for a query.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (source, content, chunk_id, relevance_score)\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if k <= 0:\n",
    "            raise VectorStoreError(f\"k must be positive, got {k}\")\n",
    "        \n",
    "        logger.debug(f\"Retrieving top-{k} chunks for query: {query[:50]}...\")\n",
    "        \n",
    "        results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "        \n",
    "        formatted_results = [\n",
    "            (\n",
    "                doc.metadata.get(\"source\", \"unknown\"),\n",
    "                doc.page_content,\n",
    "                doc.metadata.get(\"chunk_id\", -1),\n",
    "                score\n",
    "            )\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to retrieve documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def dcg(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain.\n",
    "    \n",
    "    Formula: DCG = Î£(rel_i / log2(i + 2)) for i in range(len(scores))\n",
    "    \n",
    "    Why log2(i + 2)?\n",
    "    - Position 0: log2(2) = 1 (no discount)\n",
    "    - Position 1: log2(3) = 1.58 (small discount)\n",
    "    - Position 2: log2(4) = 2 (larger discount)\n",
    "    - Later positions are increasingly discounted\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (coverage values)\n",
    "        \n",
    "    Returns:\n",
    "        DCG score\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        dcg_value = np.sum([\n",
    "            rel / np.log2(idx + 2)\n",
    "            for idx, rel in enumerate(relevance_scores)\n",
    "        ])\n",
    "        \n",
    "        return float(dcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"DCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate DCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    nDCG = DCG / IDCG\n",
    "    \n",
    "    Why normalize?\n",
    "    - Makes scores comparable across queries\n",
    "    - Range: [0, 1] where 1 = perfect ranking\n",
    "    - Accounts for different numbers of relevant items\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores\n",
    "        \n",
    "    Returns:\n",
    "        nDCG score between 0 and 1\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG with actual ranking\n",
    "        dcg_value = dcg(relevance_scores)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg_value = dcg(ideal_scores)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg_value == 0:\n",
    "            logger.warning(\"IDCG is 0, returning nDCG=0\")\n",
    "            return 0.0\n",
    "        \n",
    "        ndcg_value = dcg_value / idcg_value\n",
    "        return float(ndcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"nDCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate nDCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def get_coverage(chunk_id: int, qa_id: str, relevant_chunks: List[Document]) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve coverage score for a specific chunk and query.\n",
    "    \n",
    "    This is a lookup function that connects:\n",
    "    - Retrieved chunk (by chunk_id from vector search)\n",
    "    - Query (by qa_id from evaluation set)\n",
    "    - Ground truth coverage (pre-computed in relevant_chunks)\n",
    "    \n",
    "    Args:\n",
    "        chunk_id: ID of the retrieved chunk\n",
    "        qa_id: ID of the query being evaluated\n",
    "        relevant_chunks: List of annotated chunks with coverage scores\n",
    "        \n",
    "    Returns:\n",
    "        Coverage score (0-1), or 0 if not found\n",
    "    \"\"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if chunk.metadata.get(\"chunk_id\") != chunk_id:\n",
    "            continue\n",
    "        \n",
    "        for coverage_entry in chunk.metadata.get(\"coverage_per_query\", []):\n",
    "            if coverage_entry[\"qa_id\"] == qa_id:\n",
    "                return coverage_entry[\"coverage\"]\n",
    "    \n",
    "    # Return 0 if chunk has no coverage for this query\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EVALUATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rag_system(pdf_path: str, training_qa_path: str,chunk_size: int = 300,chunk_overlap: int = 30,\n",
    "    k: int = 3, embedding_model: str = \"text-embedding-ada-002\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete RAG evaluation pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load and clean PDF\n",
    "    2. Annotate with ground truth Q&A\n",
    "    3. Chunk documents\n",
    "    4. Calculate coverage for relevant chunks\n",
    "    5. Create vector store\n",
    "    6. For each query: retrieve top-k and calculate metrics\n",
    "    7. Report average DCG and nDCG\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to board game manual PDF\n",
    "        training_qa_path: Path to training Q&A JSON\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "        embedding_model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "        \n",
    "    Raises:\n",
    "        RAGEvaluationError: If any pipeline stage fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting RAG Evaluation Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load PDF\n",
    "        docs = load_documents(pdf_path)\n",
    "        \n",
    "        # Step 2: Annotate with ground truth\n",
    "        docs_with_qa = load_training_qa_to_docs(training_qa_path, docs)\n",
    "        \n",
    "        # Step 3: Chunk documents\n",
    "        chunks = split_text(docs_with_qa, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Step 4: Calculate coverage for relevant chunks\n",
    "        relevant_chunks = generate_relevant_chunks_with_coverage(chunks)\n",
    "        \n",
    "        # Step 5: Prepare and store in vector DB\n",
    "        chunks_for_chroma = prepare_chunks_for_chroma(chunks)\n",
    "        db, tmp_dir  = save_to_chroma(chunks_for_chroma, embedding_model)\n",
    "\n",
    "        # Step 6: Load evaluation queries\n",
    "        qa_data = load_json(training_qa_path)\n",
    "        evaluation_qas = qa_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not evaluation_qas:\n",
    "            raise EvaluationError(\"No evaluation queries found in JSON\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on {len(evaluation_qas)} queries\")\n",
    "        \n",
    "        # Step 7: Evaluate each query\n",
    "        dcg_values = []\n",
    "        ndcg_values = []\n",
    "        query_results = []\n",
    "        \n",
    "        for qa in evaluation_qas:\n",
    "            qa_id = qa.get(\"id\")\n",
    "            question = qa.get(\"question\")\n",
    "            gt_answer = qa.get(\"answer\")\n",
    "            \n",
    "            if not question:\n",
    "                logger.warning(f\"Skipping query with missing question: {qa_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Retrieve top-k chunks\n",
    "            top_k_results = retrieve_top_k(db, question, k=k)\n",
    "            top_k = []\n",
    "\n",
    "            # Calculate coverage scores for retrieved chunks\n",
    "            coverage_scores = []\n",
    "            for source, content, chunk_id, similarity_score in top_k_results:\n",
    "                coverage = get_coverage(chunk_id, qa_id, relevant_chunks)\n",
    "                coverage_scores.append(coverage)\n",
    "                top_k.append(content)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            query_dcg = dcg(coverage_scores)\n",
    "            query_ndcg = ndcg_at_k(coverage_scores)\n",
    "            \n",
    "            dcg_values.append(query_dcg)\n",
    "            ndcg_values.append(query_ndcg)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"qa_id\": qa_id,\n",
    "                \"question\": question,\n",
    "                \"top_k_content\": top_k,\n",
    "                \"gt_answer\": gt_answer,\n",
    "                \"coverage_scores\": coverage_scores,\n",
    "                \"dcg\": query_dcg,\n",
    "                \"ndcg\": query_ndcg\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_dcg = float(np.mean(dcg_values))\n",
    "        avg_ndcg = float(np.mean(ndcg_values))\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Average DCG:  {avg_dcg:.4f}\")\n",
    "        logger.info(f\"Average nDCG: {avg_ndcg:.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"avg_dcg\": avg_dcg,\n",
    "            \"avg_ndcg\": avg_ndcg,\n",
    "            \"num_queries\": len(evaluation_qas),\n",
    "            \"k\": k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"query_results\": query_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, RAGEvaluationError):\n",
    "            raise\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise RAGEvaluationError(f\"Evaluation pipeline failed: {str(e)}\") from e\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: List[str], isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) \n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PDF_PATH = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "    TRAINING_QA_PATH = \"data/BoardGamesRuleBook/CATAN_train_small.json\"\n",
    "    CHUNK_SIZES = [300]\n",
    "    CHUNK_OVERLAPS = [30]\n",
    "    Ks = [3]\n",
    "    \n",
    "    retrieval_eval_results = []\n",
    "    generation_eval_results = []\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "    \n",
    "    for CHUNK_SIZE, CHUNK_OVERLAP, k in product(CHUNK_SIZES, CHUNK_OVERLAPS, Ks):\n",
    "        logger.info(f\"Evaluating with parameters: chunk={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top-k={k}\")\n",
    "        try:\n",
    "            # Retrieval evaluation\n",
    "            results = evaluate_rag_system(\n",
    "                pdf_path=PDF_PATH,\n",
    "                training_qa_path=TRAINING_QA_PATH,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                k=k\n",
    "            )\n",
    "\n",
    "            retrieval_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZE,\n",
    "                \"overlap\": CHUNK_OVERLAP,\n",
    "                \"top_k\": k,\n",
    "                **results,\n",
    "            })\n",
    "\n",
    "            # Generation evaluation\n",
    "            evaluation_rows = []\n",
    "            for query_result in results.get(\"query_results\"):\n",
    "                question = query_result.get(\"question\")\n",
    "                top_k_content = query_result.get(\"top_k_content\")\n",
    "                gt_answer = query_result.get(\"gt_answer\")\n",
    "\n",
    "                answer = generate_answer(question, top_k_content)\n",
    "                evaluation_rows.append({\n",
    "                            \"question\": question,\n",
    "                            \"contexts\": top_k_content,\n",
    "                            \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "                            \"reference\": gt_answer,\n",
    "                        })\n",
    "            ragas_eval_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "            # Run evaluation\n",
    "            scores = evaluate(\n",
    "                ragas_eval_dataset,\n",
    "                metrics=[\n",
    "                    answer_correctness,\n",
    "                    answer_relevancy,\n",
    "                    faithfulness,\n",
    "                    context_precision,\n",
    "                    context_recall,\n",
    "                ],\n",
    "                llm=llm,  # pass the LLM explicitly\n",
    "            )\n",
    "\n",
    "            generation_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZES,\n",
    "                \"chunk_overlap\": CHUNK_OVERLAPS,\n",
    "                \"embedding_model\": [\"text-embedding-3-small\"],\n",
    "                \"top_k\": Ks,\n",
    "                \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "                \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "                \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "                \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "                \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "                \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "                \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "                \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "                \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "                \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "            })\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "        except RAGEvaluationError as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Convert to DataFrame for easy comparison\n",
    "    df = pd.DataFrame(retrieval_eval_results)\n",
    "    df.to_csv(\"rag_retrieval_eval.csv\", index=False)\n",
    "    print(\"ðŸ“ Generation results saved to rag_retrieval_eval.csv\")\n",
    "    # --- Step 4: Save and inspect ---\n",
    "    df = pd.DataFrame(generation_eval_results)\n",
    "    df.to_csv(\"rag_generation_eval.csv\", index=False)\n",
    "    print(\"ðŸ“ Generation results saved to rag_generation_eval.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623475b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>answer_correctness_mean</th>\n",
       "      <th>answer_correctness_std</th>\n",
       "      <th>answer_relevancy_mean</th>\n",
       "      <th>answer_relevancy_std</th>\n",
       "      <th>faithfulness_mean</th>\n",
       "      <th>faithfulness_std</th>\n",
       "      <th>context_precision_mean</th>\n",
       "      <th>context_precision_std</th>\n",
       "      <th>context_recall_mean</th>\n",
       "      <th>context_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[300]</td>\n",
       "      <td>[30]</td>\n",
       "      <td>[text-embedding-3-small]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>0.532749</td>\n",
       "      <td>0.177887</td>\n",
       "      <td>0.96096</td>\n",
       "      <td>0.048016</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.390512</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.395811</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_size chunk_overlap           embedding_model top_k  \\\n",
       "0      [300]          [30]  [text-embedding-3-small]   [3]   \n",
       "\n",
       "   answer_correctness_mean  answer_correctness_std  answer_relevancy_mean  \\\n",
       "0                 0.532749                0.177887                0.96096   \n",
       "\n",
       "   answer_relevancy_std  faithfulness_mean  faithfulness_std  \\\n",
       "0              0.048016           0.483333          0.390512   \n",
       "\n",
       "   context_precision_mean  context_precision_std  context_recall_mean  \\\n",
       "0                0.733333               0.395811                  0.9   \n",
       "\n",
       "   context_recall_std  \n",
       "0                 0.3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verification.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
