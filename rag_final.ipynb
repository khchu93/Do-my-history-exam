{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation System for Board Game Manuals\n",
    "============================================\n",
    "A retrieval-augmented generation (RAG) system with coverage-based evaluation.\n",
    "\n",
    "Key Features:\n",
    "- PDF text extraction with normalization\n",
    "- Ground truth Q&A annotation integration using Aho-Corasick pattern matching\n",
    "- Coverage-based relevance scoring (measures how much of a relevant span is in a chunk)\n",
    "- DCG/nDCG metrics for retrieval quality evaluation\n",
    "\n",
    "Adapted from: \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "Author: [Your Name]\n",
    "\"\"\"\n",
    "\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "# import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import ahocorasick\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datasets import Dataset\n",
    "\n",
    "# import gc\n",
    "\n",
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa7b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class RAGEvaluationError(Exception):\n",
    "    \"\"\"Base exception for RAG evaluation system\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoadError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document loading fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AnnotationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when Q&A annotation processing fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ChunkingError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document chunking fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class VectorStoreError(RAGEvaluationError):\n",
    "    \"\"\"Raised when vector store operations fail\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EvaluationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when metric calculation fails\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c40c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to handle encoding inconsistencies between PDF and JSON.\n",
    "    \n",
    "    This is critical because:\n",
    "    - PDFs may have curly quotes/apostrophes: \"\", '', '\n",
    "    - JSON files typically use straight quotes: \", '\n",
    "    - Mismatches break pattern matching for ground truth annotation\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text with standardized quotes and collapsed whitespace\n",
    "    \"\"\"\n",
    "    # Convert curly quotes to straight quotes\n",
    "    text = text.replace(\"\"\", '\"').replace(\"\"\", '\"')\n",
    "    text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "    \n",
    "    # Collapse all whitespace (newlines, tabs, multiple spaces) to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF and clean text content.\n",
    "    \n",
    "    Why cleaning matters:\n",
    "    - PDFs often have inconsistent spacing/newlines\n",
    "    - Normalized text improves embedding quality\n",
    "    - Standardized format makes pattern matching reliable\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the board game manual PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects (one per page) with cleaned text\n",
    "        \n",
    "    Raises:\n",
    "        DocumentLoadError: If PDF cannot be loaded or is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not Path(pdf_path).exists():\n",
    "            raise DocumentLoadError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading PDF from: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        if not page_docs:\n",
    "            raise DocumentLoadError(f\"No content extracted from PDF: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(page_docs)} pages from PDF\")\n",
    "        \n",
    "        # Clean text and filter metadata\n",
    "        for page_doc in page_docs:\n",
    "            # Normalize whitespace\n",
    "            clean_text = normalize_text(page_doc.page_content)\n",
    "            page_doc.page_content = clean_text\n",
    "            \n",
    "            # Keep only essential metadata to avoid Chroma serialization issues\n",
    "            allowed_keys = {\"source\", \"page\"}\n",
    "            page_doc.metadata = {\n",
    "                k: v for k, v in page_doc.metadata.items() \n",
    "                if k in allowed_keys\n",
    "            }\n",
    "        \n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, DocumentLoadError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading PDF: {str(e)}\")\n",
    "        raise DocumentLoadError(f\"Failed to load PDF: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "086959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GROUND TRUTH ANNOTATION INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON file containing training Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON data\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If file cannot be loaded or parsed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(json_path).exists():\n",
    "            raise AnnotationError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded JSON from: {json_path}\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to parse JSON: {str(e)}\") from e\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading JSON: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to load JSON: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def load_training_qa_to_docs(training_qas_path: str, page_docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Annotate documents with ground truth relevance spans using Aho-Corasick.\n",
    "    \n",
    "    Why Aho-Corasick?\n",
    "    - Efficient multi-pattern matching: O(n + m + z) vs O(n*m) for naive search\n",
    "    - n = document length, m = total pattern length, z = matches\n",
    "    - Critical when searching 100+ patterns across large documents\n",
    "    \n",
    "    Process:\n",
    "    1. Build automaton with all relevant chunks from training Q&A\n",
    "    2. Scan each page once to find all matching spans\n",
    "    3. Store span metadata (qa_id, page, start/end indices)\n",
    "    \n",
    "    Args:\n",
    "        training_qas_path: Path to JSON with training Q&A pairs\n",
    "        page_docs: List of Document objects from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Documents annotated with relevance_spans in metadata\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If annotation process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = load_json(training_qas_path)\n",
    "        training_qas = training_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not training_qas:\n",
    "            logger.warning(\"No training Q&As found in JSON\")\n",
    "            return page_docs\n",
    "        \n",
    "        logger.info(f\"Processing {len(training_qas)} training Q&A pairs\")\n",
    "        \n",
    "        # Build Aho-Corasick automaton for efficient pattern matching\n",
    "        automaton = ahocorasick.Automaton()\n",
    "        \n",
    "        for qa_idx, qa in enumerate(training_qas):\n",
    "            qa[\"relevance_spans\"] = []  # Initialize spans list\n",
    "            \n",
    "            for chunk_text in qa.get(\"relevant_chunks\", []):\n",
    "                chunk_text_normalized = normalize_text(chunk_text)\n",
    "                \n",
    "                # Store tuple: (qa_index, original_chunk_text)\n",
    "                # qa_index allows us to map back to the question\n",
    "                automaton.add_word(chunk_text_normalized, (qa_idx, chunk_text_normalized))\n",
    "        \n",
    "        automaton.make_automaton()  # Compile the automaton\n",
    "        logger.info(\"Aho-Corasick automaton built successfully\")\n",
    "        \n",
    "        # Search all pages for relevant spans\n",
    "        total_spans = 0\n",
    "        for page_doc in page_docs:\n",
    "            page_text = normalize_text(page_doc.page_content)\n",
    "            page_num = page_doc.metadata.get(\"page\")\n",
    "            page_doc.metadata[\"relevance_spans\"] = []\n",
    "            \n",
    "            # Iterate through all matches in this page\n",
    "            for end_idx, (qa_idx, chunk_text) in automaton.iter(page_text):\n",
    "                start_idx = end_idx - len(chunk_text) + 1  # +1 because end_idx is inclusive\n",
    "                \n",
    "                span = {\n",
    "                    \"qa_id\": training_qas[qa_idx][\"id\"],\n",
    "                    \"page\": page_num,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx + 1  # Make end exclusive for easier indexing\n",
    "                }\n",
    "                page_doc.metadata[\"relevance_spans\"].append(span)\n",
    "                total_spans += 1\n",
    "        \n",
    "        logger.info(f\"Found {total_spans} relevance spans across all pages\")\n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Annotation failed: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to annotate documents: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 21:08:41,138 - INFO - Evaluating with parameters: chunk=300, overlap=30, top-k=3\n",
      "2025-11-10 21:08:41,139 - INFO - ============================================================\n",
      "2025-11-10 21:08:41,139 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-10 21:08:41,139 - INFO - ============================================================\n",
      "2025-11-10 21:08:41,140 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n",
      "2025-11-10 21:08:45,416 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-10 21:08:45,421 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-10 21:08:45,423 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-10 21:08:45,424 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-10 21:08:45,429 - INFO - Found 10 relevance spans across all pages\n",
      "2025-11-10 21:08:45,430 - INFO - Splitting documents with chunk_size=300, overlap=30\n",
      "2025-11-10 21:08:45,445 - INFO - Created 100 chunks\n",
      "2025-11-10 21:08:45,450 - INFO - Found 14 relevant chunks out of 100 total\n",
      "2025-11-10 21:08:45,451 - INFO - Prepared 100 chunks for Chroma\n",
      "2025-11-10 21:08:46,685 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-10 21:08:47,971 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:48,312 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-10 21:08:48,313 - INFO - Evaluating on 10 queries\n",
      "2025-11-10 21:08:48,471 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_ri6nen9h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 21:08:48,692 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:48,698 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-10 21:08:48,881 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:49,170 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:49,174 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-10 21:08:49,475 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:49,715 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:50,296 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:50,652 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:51,163 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:51,168 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-10 21:08:51,480 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:51,485 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-10 21:08:51,486 - INFO - ============================================================\n",
      "2025-11-10 21:08:51,486 - INFO - Average DCG:  0.4697\n",
      "2025-11-10 21:08:51,487 - INFO - Average nDCG: 0.5262\n",
      "2025-11-10 21:08:51,487 - INFO - ============================================================\n",
      "2025-11-10 21:08:52,591 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:53,106 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:55,365 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:56,244 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:08:57,134 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:01,053 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:01,471 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:02,248 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:03,323 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:03,949 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]2025-11-10 21:09:07,053 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:07,713 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:09:08,578 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:09,378 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   2%|â–         | 1/50 [00:03<02:57,  3.62s/it]2025-11-10 21:09:10,004 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,006 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,007 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,009 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,011 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,012 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,015 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,017 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,019 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,021 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,023 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,025 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,026 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,029 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:10,031 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:27,943 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:09:28,890 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:29,688 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:31,086 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:09:32,005 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:33,228 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:34,136 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:   4%|â–         | 2/50 [00:27<12:33, 15.69s/it]2025-11-10 21:09:35,828 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:36,640 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  14%|â–ˆâ–        | 7/50 [00:30<02:31,  3.53s/it]2025-11-10 21:09:37,306 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:38,775 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,228 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,232 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,235 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,238 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,242 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,246 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,249 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,252 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:40,255 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:43,495 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:44,340 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,545 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,548 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,553 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,557 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,560 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  16%|â–ˆâ–Œ        | 8/50 [00:48<04:16,  6.10s/it]2025-11-10 21:09:54,585 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:09:54,590 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  20%|â–ˆâ–ˆ        | 10/50 [00:56<03:39,  5.49s/it]2025-11-10 21:10:03,268 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:03,271 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:03,284 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:58<02:02,  3.32s/it]2025-11-10 21:10:08,696 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:08,724 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 14/50 [01:03<02:14,  3.74s/it]2025-11-10 21:10:10,308 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,314 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,316 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,319 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,322 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,325 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:10,327 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:11,796 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:14<02:57,  5.07s/it]2025-11-10 21:10:20,885 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:20,894 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:20,899 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [01:18<01:32,  2.99s/it]2025-11-10 21:10:25,380 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:25,381 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:25,383 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:25,384 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:25,386 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:25,387 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [01:20<01:08,  2.35s/it]2025-11-10 21:10:26,647 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:28,778 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:10:29,698 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:30,465 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [01:27<01:26,  3.10s/it]2025-11-10 21:10:34,799 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:34,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:34,809 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:34,816 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [01:29<01:20,  2.98s/it]2025-11-10 21:10:36,074 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [01:30<00:54,  2.16s/it]2025-11-10 21:10:40,060 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:40,930 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,659 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,661 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,663 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,665 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,666 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,668 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,670 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,671 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:41,674 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [01:36<01:05,  2.74s/it]2025-11-10 21:10:43,114 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [01:36<00:52,  2.29s/it]2025-11-10 21:10:47,531 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:10:48,347 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:49,175 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [01:47<01:33,  4.24s/it]2025-11-10 21:10:53,481 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:53,483 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [01:48<00:57,  2.90s/it]2025-11-10 21:10:55,388 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:55,395 - INFO - Retrying request to /embeddings in 0.470150 seconds\n",
      "2025-11-10 21:10:56,665 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:56,668 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:57,881 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:59,237 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:59,240 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:59,244 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:10:59,246 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:02,465 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,418 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,419 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,424 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,425 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,428 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,432 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:06,438 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [02:01<01:15,  4.20s/it]2025-11-10 21:11:08,275 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:10,157 - WARNING - LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "2025-11-10 21:11:10,977 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:11,808 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:13,997 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:14,923 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [02:12<01:32,  5.44s/it]2025-11-10 21:11:19,798 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,800 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,805 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,806 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:19,808 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [02:15<00:30,  2.51s/it]2025-11-10 21:11:21,678 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:28,136 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:28,802 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:28,804 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [02:24<00:31,  3.20s/it]2025-11-10 21:11:31,321 - INFO - Retrying request to /embeddings in 0.423058 seconds\n",
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [02:26<00:17,  2.23s/it]2025-11-10 21:11:32,591 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:32,594 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:32,598 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:34,461 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [02:30<00:18,  2.70s/it]2025-11-10 21:11:37,013 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [02:31<00:13,  2.21s/it]2025-11-10 21:11:38,891 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:40,261 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:40,263 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:40,905 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:42,162 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:42,164 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:42,165 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:42,167 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [02:40<00:13,  3.31s/it]2025-11-10 21:11:47,308 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:47,310 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-10 21:11:47,312 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [02:43<00:01,  1.94s/it]2025-11-10 21:11:49,768 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [02:43<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ Generation results saved to rag_retrieval_eval.csv\n",
      "\n",
      "ðŸ“ Generation results saved to rag_generation_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_text(docs: List[Document], chunk_size: int = 300, chunk_overlap: int = 30) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    \n",
    "    Why chunk?\n",
    "    - Embeddings work better on focused, semantic units\n",
    "    - Smaller chunks = more precise retrieval\n",
    "    - Overlap ensures we don't split important context\n",
    "    \n",
    "    Why these defaults?\n",
    "    - chunk_size=300: ~75 tokens, good for rule-specific content\n",
    "    - chunk_overlap=30: 10% overlap preserves context at boundaries\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects\n",
    "        chunk_size: Target size for each chunk (characters)\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk Documents with start_index in metadata\n",
    "        \n",
    "    Raises:\n",
    "        ChunkingError: If chunking process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chunk_size <= 0:\n",
    "            raise ChunkingError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "        \n",
    "        if chunk_overlap < 0:\n",
    "            raise ChunkingError(f\"chunk_overlap cannot be negative, got {chunk_overlap}\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ChunkingError(\n",
    "                f\"chunk_overlap ({chunk_overlap}) must be less than \"\n",
    "                f\"chunk_size ({chunk_size})\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Splitting documents with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,  # Use character count\n",
    "            add_start_index=True  # Critical: needed for coverage calculation\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ChunkingError):\n",
    "            raise\n",
    "        logger.error(f\"Chunking failed: {str(e)}\")\n",
    "        raise ChunkingError(f\"Failed to split documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COVERAGE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_overlap(span_start: int, span_end: int, chunk_start: int, chunk_end: int) -> int:\n",
    "    \"\"\"\n",
    "    Compute character overlap between a relevance span and a chunk.\n",
    "    \n",
    "    Example:\n",
    "        Span:  [10, 30)  (relevant text from annotation)\n",
    "        Chunk: [20, 50)  (text chunk)\n",
    "        Overlap: [20, 30) = 10 characters\n",
    "    \n",
    "    Args:\n",
    "        span_start: Start index of relevance span\n",
    "        span_end: End index of relevance span (exclusive)\n",
    "        chunk_start: Start index of chunk\n",
    "        chunk_end: End index of chunk (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Number of overlapping characters\n",
    "    \"\"\"\n",
    "    overlap_start = max(span_start, chunk_start)\n",
    "    overlap_end = min(span_end, chunk_end)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "\n",
    "def generate_relevant_chunks_with_coverage(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate coverage scores for chunks containing ground truth spans.\n",
    "    \n",
    "    Coverage = (overlap_length / relevance_span_length)\n",
    "    \n",
    "    Why coverage?\n",
    "    - Measures \"how much of the relevant content is in this chunk\"\n",
    "    - Coverage=1.0: entire relevant span is in the chunk (perfect)\n",
    "    - Coverage=0.5: only half the relevant content is present\n",
    "    - Coverage=0.0: chunk doesn't contain relevant content\n",
    "    \n",
    "    This is better than binary relevance because:\n",
    "    - Distinguishes between partial and complete matches\n",
    "    - Handles cases where spans cross chunk boundaries\n",
    "    - Provides granular relevance scores for nDCG calculation\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of Documents containing only relevant chunks with coverage scores\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If coverage calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = chunk.metadata.get(\"start_index\", 0)\n",
    "            chunk_end = chunk_start + len(chunk.page_content)\n",
    "            relevance_spans = chunk.metadata.get(\"relevance_spans\", [])\n",
    "            \n",
    "            # Skip chunks without any ground truth annotations\n",
    "            if not relevance_spans:\n",
    "                continue\n",
    "            \n",
    "            # Create copy to avoid modifying original\n",
    "            annotated_chunk = copy.deepcopy(chunk)\n",
    "            annotated_chunk.metadata[\"coverage_per_query\"] = []\n",
    "            \n",
    "            for span in relevance_spans:\n",
    "                qa_id = span[\"qa_id\"]\n",
    "                \n",
    "                # Calculate how much of the span overlaps with this chunk\n",
    "                overlap_len = compute_overlap(\n",
    "                    span[\"start\"], span[\"end\"], \n",
    "                    chunk_start, chunk_end\n",
    "                )\n",
    "                \n",
    "                relevance_len = span[\"end\"] - span[\"start\"]\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if relevance_len == 0:\n",
    "                    logger.warning(f\"Zero-length relevance span for qa_id={qa_id}\")\n",
    "                    continue\n",
    "                \n",
    "                coverage = overlap_len / relevance_len\n",
    "                \n",
    "                # Skip queries with no overlap\n",
    "                if coverage == 0:\n",
    "                    continue\n",
    "                \n",
    "                annotated_chunk.metadata[\"coverage_per_query\"].append({\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"coverage\": coverage\n",
    "                })\n",
    "            \n",
    "            # Only keep chunks that have at least one relevant query\n",
    "            if annotated_chunk.metadata[\"coverage_per_query\"]:\n",
    "                annotated_chunk.metadata[\"chunk_id\"] = chunk_idx\n",
    "                relevant_chunks.append(annotated_chunk)\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_chunks)} relevant chunks out of {len(chunks)} total\")\n",
    "        return relevant_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Coverage calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate coverage: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_chunks_for_chroma(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter complex metadata for Chroma compatibility.\n",
    "    \n",
    "    Why needed?\n",
    "    - Chroma only supports simple types (str, int, float, bool)\n",
    "    - Complex types (lists, dicts) cause serialization errors\n",
    "    - We keep complex metadata in separate 'relevant_chunks' list\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Documents with filtered metadata safe for Chroma\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If metadata filtering fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrievable_docs = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Filter to simple metadata types\n",
    "            filtered_doc = filter_complex_metadata([chunk])[0]\n",
    "            \n",
    "            # Add chunk_id for later lookup\n",
    "            filtered_doc.metadata[\"chunk_id\"] = chunk_idx\n",
    "            \n",
    "            retrievable_docs.append(filtered_doc)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(retrievable_docs)} chunks for Chroma\")\n",
    "        return retrievable_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metadata filtering failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to prepare chunks: {str(e)}\") from e\n",
    "\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: List[Document], embedding_model: str = \"text-embedding-ada-002\") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist Chroma vector store.\n",
    "    \n",
    "    Note: This clears existing database!\n",
    "    - Ensures fresh embeddings\n",
    "    - Avoids stale data issues\n",
    "    - For production, consider incremental updates\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks (with simple metadata)\n",
    "        embedding_model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Initialized Chroma vector store\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If vector store creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an isolated temporary directory\n",
    "        tmp_dir = tempfile.mkdtemp(prefix=\"chroma_eval_\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Create the Chroma vector store from documents\n",
    "        db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=tmp_dir\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Temporary Chroma DB created at: {tmp_dir}\")\n",
    "        return db, tmp_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Vector store creation failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to create vector store: {str(e)}\") from e\n",
    "\n",
    "def retrieve_top_k(\n",
    "    db: Chroma, \n",
    "    query: str, \n",
    "    k: int = 3\n",
    ") -> List[Tuple[str, str, int, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar chunks for a query.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (source, content, chunk_id, relevance_score)\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if k <= 0:\n",
    "            raise VectorStoreError(f\"k must be positive, got {k}\")\n",
    "        \n",
    "        logger.debug(f\"Retrieving top-{k} chunks for query: {query[:50]}...\")\n",
    "        \n",
    "        results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "        \n",
    "        formatted_results = [\n",
    "            (\n",
    "                doc.metadata.get(\"source\", \"unknown\"),\n",
    "                doc.page_content,\n",
    "                doc.metadata.get(\"chunk_id\", -1),\n",
    "                score\n",
    "            )\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to retrieve documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def dcg(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain.\n",
    "    \n",
    "    Formula: DCG = Î£(rel_i / log2(i + 2)) for i in range(len(scores))\n",
    "    \n",
    "    Why log2(i + 2)?\n",
    "    - Position 0: log2(2) = 1 (no discount)\n",
    "    - Position 1: log2(3) = 1.58 (small discount)\n",
    "    - Position 2: log2(4) = 2 (larger discount)\n",
    "    - Later positions are increasingly discounted\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (coverage values)\n",
    "        \n",
    "    Returns:\n",
    "        DCG score\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        dcg_value = np.sum([\n",
    "            rel / np.log2(idx + 2)\n",
    "            for idx, rel in enumerate(relevance_scores)\n",
    "        ])\n",
    "        \n",
    "        return float(dcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"DCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate DCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    nDCG = DCG / IDCG\n",
    "    \n",
    "    Why normalize?\n",
    "    - Makes scores comparable across queries\n",
    "    - Range: [0, 1] where 1 = perfect ranking\n",
    "    - Accounts for different numbers of relevant items\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores\n",
    "        \n",
    "    Returns:\n",
    "        nDCG score between 0 and 1\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG with actual ranking\n",
    "        dcg_value = dcg(relevance_scores)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg_value = dcg(ideal_scores)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg_value == 0:\n",
    "            logger.warning(\"IDCG is 0, returning nDCG=0\")\n",
    "            return 0.0\n",
    "        \n",
    "        ndcg_value = dcg_value / idcg_value\n",
    "        return float(ndcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"nDCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate nDCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def get_coverage(chunk_id: int, qa_id: str, relevant_chunks: List[Document]) -> float:\n",
    "    \"\"\"\n",
    "    Retrieve coverage score for a specific chunk and query.\n",
    "    \n",
    "    This is a lookup function that connects:\n",
    "    - Retrieved chunk (by chunk_id from vector search)\n",
    "    - Query (by qa_id from evaluation set)\n",
    "    - Ground truth coverage (pre-computed in relevant_chunks)\n",
    "    \n",
    "    Args:\n",
    "        chunk_id: ID of the retrieved chunk\n",
    "        qa_id: ID of the query being evaluated\n",
    "        relevant_chunks: List of annotated chunks with coverage scores\n",
    "        \n",
    "    Returns:\n",
    "        Coverage score (0-1), or 0 if not found\n",
    "    \"\"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if chunk.metadata.get(\"chunk_id\") != chunk_id:\n",
    "            continue\n",
    "        \n",
    "        for coverage_entry in chunk.metadata.get(\"coverage_per_query\", []):\n",
    "            if coverage_entry[\"qa_id\"] == qa_id:\n",
    "                return coverage_entry[\"coverage\"]\n",
    "    \n",
    "    # Return 0 if chunk has no coverage for this query\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EVALUATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rag_system(pdf_path: str, training_qa_path: str,chunk_size: int = 300,chunk_overlap: int = 30,\n",
    "    k: int = 3, embedding_model: str = \"text-embedding-ada-002\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete RAG evaluation pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load and clean PDF\n",
    "    2. Annotate with ground truth Q&A\n",
    "    3. Chunk documents\n",
    "    4. Calculate coverage for relevant chunks\n",
    "    5. Create vector store\n",
    "    6. For each query: retrieve top-k and calculate metrics\n",
    "    7. Report average DCG and nDCG\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to board game manual PDF\n",
    "        training_qa_path: Path to training Q&A JSON\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "        embedding_model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "        \n",
    "    Raises:\n",
    "        RAGEvaluationError: If any pipeline stage fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting RAG Evaluation Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load PDF\n",
    "        docs = load_documents(pdf_path)\n",
    "        \n",
    "        # Step 2: Annotate with ground truth\n",
    "        docs_with_qa = load_training_qa_to_docs(training_qa_path, docs)\n",
    "        \n",
    "        # Step 3: Chunk documents\n",
    "        chunks = split_text(docs_with_qa, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Step 4: Calculate coverage for relevant chunks\n",
    "        relevant_chunks = generate_relevant_chunks_with_coverage(chunks)\n",
    "        \n",
    "        # Step 5: Prepare and store in vector DB\n",
    "        chunks_for_chroma = prepare_chunks_for_chroma(chunks)\n",
    "        db, tmp_dir  = save_to_chroma(chunks_for_chroma, embedding_model)\n",
    "\n",
    "        # Step 6: Load evaluation queries\n",
    "        qa_data = load_json(training_qa_path)\n",
    "        evaluation_qas = qa_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not evaluation_qas:\n",
    "            raise EvaluationError(\"No evaluation queries found in JSON\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on {len(evaluation_qas)} queries\")\n",
    "        \n",
    "        # Step 7: Evaluate each query\n",
    "        dcg_values = []\n",
    "        ndcg_values = []\n",
    "        query_results = []\n",
    "        \n",
    "        for qa in evaluation_qas:\n",
    "            qa_id = qa.get(\"id\")\n",
    "            question = qa.get(\"question\")\n",
    "            gt_answer = qa.get(\"answer\")\n",
    "            \n",
    "            if not question:\n",
    "                logger.warning(f\"Skipping query with missing question: {qa_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Retrieve top-k chunks\n",
    "            top_k_results = retrieve_top_k(db, question, k=k)\n",
    "            top_k = []\n",
    "\n",
    "            # Calculate coverage scores for retrieved chunks\n",
    "            coverage_scores = []\n",
    "            for source, content, chunk_id, similarity_score in top_k_results:\n",
    "                coverage = get_coverage(chunk_id, qa_id, relevant_chunks)\n",
    "                coverage_scores.append(coverage)\n",
    "                top_k.append(content)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            query_dcg = dcg(coverage_scores)\n",
    "            query_ndcg = ndcg_at_k(coverage_scores)\n",
    "            \n",
    "            dcg_values.append(query_dcg)\n",
    "            ndcg_values.append(query_ndcg)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"qa_id\": qa_id,\n",
    "                \"question\": question,\n",
    "                \"top_k_content\": top_k,\n",
    "                \"gt_answer\": gt_answer,\n",
    "                \"coverage_scores\": coverage_scores,\n",
    "                \"dcg\": query_dcg,\n",
    "                \"ndcg\": query_ndcg\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_dcg = float(np.mean(dcg_values))\n",
    "        avg_ndcg = float(np.mean(ndcg_values))\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Average DCG:  {avg_dcg:.4f}\")\n",
    "        logger.info(f\"Average nDCG: {avg_ndcg:.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"avg_dcg\": avg_dcg,\n",
    "            \"avg_ndcg\": avg_ndcg,\n",
    "            \"num_queries\": len(evaluation_qas),\n",
    "            \"k\": k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"query_results\": query_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, RAGEvaluationError):\n",
    "            raise\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise RAGEvaluationError(f\"Evaluation pipeline failed: {str(e)}\") from e\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: List[str], isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context)\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) \n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PDF_PATH = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "    TRAINING_QA_PATH = \"data/BoardGamesRuleBook/CATAN_train_small.json\"\n",
    "    CHUNK_SIZES = [300]\n",
    "    CHUNK_OVERLAPS = [30]\n",
    "    Ks = [3]\n",
    "    \n",
    "    retrieval_eval_results = []\n",
    "    generation_eval_results = []\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "    \n",
    "    for CHUNK_SIZE, CHUNK_OVERLAP, k in product(CHUNK_SIZES, CHUNK_OVERLAPS, Ks):\n",
    "        logger.info(f\"Evaluating with parameters: chunk={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top-k={k}\")\n",
    "        try:\n",
    "            # Retrieval evaluation\n",
    "            results = evaluate_rag_system(\n",
    "                pdf_path=PDF_PATH,\n",
    "                training_qa_path=TRAINING_QA_PATH,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                k=k\n",
    "            )\n",
    "\n",
    "            retrieval_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZE,\n",
    "                \"overlap\": CHUNK_OVERLAP,\n",
    "                \"top_k\": k,\n",
    "                **results,\n",
    "            })\n",
    "\n",
    "            # Generation evaluation\n",
    "            evaluation_rows = []\n",
    "            for query_result in results.get(\"query_results\"):\n",
    "                question = query_result.get(\"question\")\n",
    "                top_k_content = query_result.get(\"top_k_content\")\n",
    "                gt_answer = query_result.get(\"gt_answer\")\n",
    "\n",
    "                answer = generate_answer(question, top_k_content)\n",
    "                evaluation_rows.append({\n",
    "                            \"question\": question,\n",
    "                            \"contexts\": top_k_content,\n",
    "                            \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "                            \"reference\": gt_answer,\n",
    "                        })\n",
    "            ragas_eval_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "            # Run evaluation\n",
    "            scores = evaluate(\n",
    "                ragas_eval_dataset,\n",
    "                metrics=[\n",
    "                    answer_correctness,\n",
    "                    answer_relevancy,\n",
    "                    faithfulness,\n",
    "                    context_precision,\n",
    "                    context_recall,\n",
    "                ],\n",
    "                llm=llm,  # pass the LLM explicitly\n",
    "            )\n",
    "\n",
    "            generation_eval_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZES,\n",
    "                \"chunk_overlap\": CHUNK_OVERLAPS,\n",
    "                \"embedding_model\": [\"text-embedding-3-small\"],\n",
    "                \"top_k\": Ks,\n",
    "                \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "                \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "                \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "                \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "                \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "                \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "                \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "                \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "                \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "                \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "            })\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "        except RAGEvaluationError as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Convert to DataFrame for easy comparison\n",
    "    df = pd.DataFrame(retrieval_eval_results)\n",
    "    df.to_csv(\"rag_retrieval_eval.csv\", index=False)\n",
    "    print(\"ðŸ“ Generation results saved to rag_retrieval_eval.csv\")\n",
    "    # --- Step 4: Save and inspect ---\n",
    "    df = pd.DataFrame(generation_eval_results)\n",
    "    df.to_csv(\"rag_generation_eval.csv\", index=False)\n",
    "    print(\"ðŸ“ Generation results saved to rag_generation_eval.csv\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
