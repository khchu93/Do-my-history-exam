# Board Game Manual Q&A System ğŸ²

A production-ready **Retrieval-Augmented Generation (RAG)** system for answering questions about board game manuals. This project demonstrates best practices in RAG system design, evaluation, and deployment.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸŒŸ Key Features

- **Production RAG System**: Clean, modular architecture ready for deployment
- **Comprehensive Evaluation**: Coverage-based metrics (DCG/nDCG) + RAGAS generation metrics
- **Ground Truth Integration**: Aho-Corasick pattern matching for efficient annotation
- **Interactive Demo**: User-friendly CLI for testing the system
- **Parameter Optimization**: Grid search over chunking and retrieval parameters

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ config.py              # Configuration and settings
â”œâ”€â”€ exceptions.py          # Custom exception classes
â”œâ”€â”€ document_loader.py     # PDF loading and preprocessing
â”œâ”€â”€ annotation.py          # Ground truth Q&A annotation
â”œâ”€â”€ chunking.py            # Document chunking and coverage calculation
â”œâ”€â”€ vector_store.py        # Vector store operations (Chroma)
â”œâ”€â”€ metrics.py             # Evaluation metrics (DCG/nDCG)
â”œâ”€â”€ rag_system.py          # Core RAG system (production)
â”œâ”€â”€ evaluation.py          # Evaluation pipeline
â”œâ”€â”€ demo.py               # Interactive demo
â”œâ”€â”€ run_evaluation.py     # Evaluation runner
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md            # This file
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/rag-board-game-qa.git
cd rag-board-game-qa

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Setup

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_api_key_here
```

Place your data files in the correct location:
```
data/
â””â”€â”€ BoardGamesRuleBook/
    â”œâ”€â”€ CATAN.pdf
    â””â”€â”€ CATAN_train_small.json
```

### 3. Run Interactive Demo

```bash
python demo.py
```

Example interaction:
```
â“ Your question: How do you win the game?

ğŸ” Searching for relevant information...

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’¡ ANSWER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
To win the game, you need to be the first player to reach 10 victory 
points. Victory points are earned through building settlements (1 point), 
cities (2 points), having the longest road (2 points), the largest army 
(2 points), and certain development cards.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### 4. Run Evaluation

```bash
python run_evaluation.py
```

This will:
- Test different parameter configurations
- Evaluate retrieval quality (DCG/nDCG)
- Evaluate generation quality (RAGAS metrics)
- Save results to CSV files

## ğŸ“Š Evaluation Methodology

### Retrieval Evaluation

**Coverage-Based Relevance**:
- Uses ground truth Q&A annotations
- Calculates coverage: `overlap_length / relevance_span_length`
- Accounts for partial matches across chunk boundaries

**Metrics**:
- **DCG** (Discounted Cumulative Gain): Position-aware relevance scoring
- **nDCG** (Normalized DCG): Comparable across queries (0-1 scale)

### Generation Evaluation

**RAGAS Metrics**:
- **Answer Correctness**: Semantic similarity to ground truth
- **Answer Relevancy**: How well answer addresses the question
- **Faithfulness**: Whether answer is grounded in retrieved context
- **Context Precision**: Relevance of retrieved chunks to question
- **Context Recall**: Coverage of ground truth in retrieved context

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding Model â”‚ (OpenAI Ada-002)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Search  â”‚ (ChromaDB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Top-K Chunks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LLM (GPT-3.5) â”‚ + Context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Answer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Technical Highlights

### 1. Efficient Pattern Matching
Uses **Aho-Corasick algorithm** for ground truth annotation:
- Time complexity: O(n + m + z) vs O(nÃ—m) for naive search
- Handles 100+ patterns efficiently
- Critical for large-scale evaluation

### 2. Coverage-Based Scoring
Novel approach to relevance measurement:
- Handles spans that cross chunk boundaries
- Provides granular scores (not just binary)
- Enables meaningful nDCG calculation

### 3. Modular Design
Clean separation of concerns:
- Easy to swap components (different embeddings, LLMs, vector stores)
- Testable individual modules
- Production-ready code structure

## ğŸ“ˆ Example Results

```
Retrieval Evaluation:
â”œâ”€â”€ Average DCG:   0.4697
â””â”€â”€ Average nDCG:  0.5262

Generation Evaluation:
â”œâ”€â”€ Answer Correctness:  0.7145 Â± 0.12
â”œâ”€â”€ Answer Relevancy:    0.8234 Â± 0.09
â”œâ”€â”€ Faithfulness:        0.8567 Â± 0.11
â”œâ”€â”€ Context Precision:   0.6891 Â± 0.15
â””â”€â”€ Context Recall:      0.7423 Â± 0.13
```

## ğŸ¯ Use Cases

- **Customer Support**: Automated Q&A for product manuals
- **Education**: Interactive learning from textbooks
- **Legal/Compliance**: Quick reference for policy documents
- **Technical Documentation**: Developer Q&A systems

## ğŸ› ï¸ Future Enhancements

- [ ] Add support for multi-document retrieval
- [ ] Implement hybrid search (dense + sparse)
- [ ] Add streaming responses
- [ ] Build web interface (Streamlit/Gradio)
- [ ] Add citation/source tracking
- [ ] Implement feedback loop for continuous improvement

## ğŸ“š References

- **RAG + Langchain Tutorial**: [YouTube](https://www.youtube.com/watch?v=tcqEUSNCn8I)
- **RAGAS Framework**: [Docs](https://docs.ragas.io/)
- **LangChain**: [Docs](https://python.langchain.com/)

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ for junior LLM engineer positions**