{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c114b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG Evaluation System for Board Game Manuals\n",
    "============================================\n",
    "A retrieval-augmented generation (RAG) system with coverage-based evaluation.\n",
    "\n",
    "Key Features:\n",
    "- PDF text extraction with normalization\n",
    "- Ground truth Q&A annotation integration using Aho-Corasick pattern matching\n",
    "- Coverage-based relevance scoring (measures how much of a relevant span is in a chunk)\n",
    "- DCG/nDCG metrics for retrieval quality evaluation\n",
    "\n",
    "Adapted from: \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "Author: [Your Name]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import ahocorasick\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa7b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOGGING & CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM EXCEPTIONS\n",
    "# =============================================================================\n",
    "\n",
    "class RAGEvaluationError(Exception):\n",
    "    \"\"\"Base exception for RAG evaluation system\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class DocumentLoadError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document loading fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class AnnotationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when Q&A annotation processing fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class ChunkingError(RAGEvaluationError):\n",
    "    \"\"\"Raised when document chunking fails\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class VectorStoreError(RAGEvaluationError):\n",
    "    \"\"\"Raised when vector store operations fail\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class EvaluationError(RAGEvaluationError):\n",
    "    \"\"\"Raised when metric calculation fails\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c40c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT LOADING & PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text to handle encoding inconsistencies between PDF and JSON.\n",
    "    \n",
    "    This is critical because:\n",
    "    - PDFs may have curly quotes/apostrophes: \"\", '', '\n",
    "    - JSON files typically use straight quotes: \", '\n",
    "    - Mismatches break pattern matching for ground truth annotation\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text with standardized quotes and collapsed whitespace\n",
    "    \"\"\"\n",
    "    # Convert curly quotes to straight quotes\n",
    "    text = text.replace(\"\"\", '\"').replace(\"\"\", '\"')\n",
    "    text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "    \n",
    "    # Collapse all whitespace (newlines, tabs, multiple spaces) to single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_documents(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF and clean text content.\n",
    "    \n",
    "    Why cleaning matters:\n",
    "    - PDFs often have inconsistent spacing/newlines\n",
    "    - Normalized text improves embedding quality\n",
    "    - Standardized format makes pattern matching reliable\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the board game manual PDF\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects (one per page) with cleaned text\n",
    "        \n",
    "    Raises:\n",
    "        DocumentLoadError: If PDF cannot be loaded or is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate file exists\n",
    "        if not Path(pdf_path).exists():\n",
    "            raise DocumentLoadError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loading PDF from: {pdf_path}\")\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        page_docs = loader.load()\n",
    "        \n",
    "        if not page_docs:\n",
    "            raise DocumentLoadError(f\"No content extracted from PDF: {pdf_path}\")\n",
    "        \n",
    "        logger.info(f\"Loaded {len(page_docs)} pages from PDF\")\n",
    "        \n",
    "        # Clean text and filter metadata\n",
    "        for page_doc in page_docs:\n",
    "            # Normalize whitespace\n",
    "            clean_text = normalize_text(page_doc.page_content)\n",
    "            page_doc.page_content = clean_text\n",
    "            \n",
    "            # Keep only essential metadata to avoid Chroma serialization issues\n",
    "            allowed_keys = {\"source\", \"page\"}\n",
    "            page_doc.metadata = {\n",
    "                k: v for k, v in page_doc.metadata.items() \n",
    "                if k in allowed_keys\n",
    "            }\n",
    "        \n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, DocumentLoadError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading PDF: {str(e)}\")\n",
    "        raise DocumentLoadError(f\"Failed to load PDF: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "086959a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GROUND TRUTH ANNOTATION INTEGRATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_json(json_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON file containing training Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Parsed JSON data\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If file cannot be loaded or parsed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not Path(json_path).exists():\n",
    "            raise AnnotationError(f\"JSON file not found: {json_path}\")\n",
    "        \n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded JSON from: {json_path}\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON format: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to parse JSON: {str(e)}\") from e\n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Unexpected error loading JSON: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to load JSON: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def load_training_qa_to_docs(\n",
    "    training_qas_path: str, \n",
    "    page_docs: List[Document]\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Annotate documents with ground truth relevance spans using Aho-Corasick.\n",
    "    \n",
    "    Why Aho-Corasick?\n",
    "    - Efficient multi-pattern matching: O(n + m + z) vs O(n*m) for naive search\n",
    "    - n = document length, m = total pattern length, z = matches\n",
    "    - Critical when searching 100+ patterns across large documents\n",
    "    \n",
    "    Process:\n",
    "    1. Build automaton with all relevant chunks from training Q&A\n",
    "    2. Scan each page once to find all matching spans\n",
    "    3. Store span metadata (qa_id, page, start/end indices)\n",
    "    \n",
    "    Args:\n",
    "        training_qas_path: Path to JSON with training Q&A pairs\n",
    "        page_docs: List of Document objects from PDF\n",
    "        \n",
    "    Returns:\n",
    "        Documents annotated with relevance_spans in metadata\n",
    "        \n",
    "    Raises:\n",
    "        AnnotationError: If annotation process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        training_data = load_json(training_qas_path)\n",
    "        training_qas = training_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not training_qas:\n",
    "            logger.warning(\"No training Q&As found in JSON\")\n",
    "            return page_docs\n",
    "        \n",
    "        logger.info(f\"Processing {len(training_qas)} training Q&A pairs\")\n",
    "        \n",
    "        # Build Aho-Corasick automaton for efficient pattern matching\n",
    "        automaton = ahocorasick.Automaton()\n",
    "        \n",
    "        for qa_idx, qa in enumerate(training_qas):\n",
    "            qa[\"relevance_spans\"] = []  # Initialize spans list\n",
    "            \n",
    "            for chunk_text in qa.get(\"relevant_chunks\", []):\n",
    "                chunk_text_normalized = normalize_text(chunk_text)\n",
    "                \n",
    "                # Store tuple: (qa_index, original_chunk_text)\n",
    "                # qa_index allows us to map back to the question\n",
    "                automaton.add_word(chunk_text_normalized, (qa_idx, chunk_text_normalized))\n",
    "        \n",
    "        automaton.make_automaton()  # Compile the automaton\n",
    "        logger.info(\"Aho-Corasick automaton built successfully\")\n",
    "        \n",
    "        # Search all pages for relevant spans\n",
    "        total_spans = 0\n",
    "        for page_doc in page_docs:\n",
    "            page_text = normalize_text(page_doc.page_content)\n",
    "            page_num = page_doc.metadata.get(\"page\")\n",
    "            page_doc.metadata[\"relevance_spans\"] = []\n",
    "            \n",
    "            # Iterate through all matches in this page\n",
    "            for end_idx, (qa_idx, chunk_text) in automaton.iter(page_text):\n",
    "                start_idx = end_idx - len(chunk_text) + 1  # +1 because end_idx is inclusive\n",
    "                \n",
    "                span = {\n",
    "                    \"qa_id\": training_qas[qa_idx][\"id\"],\n",
    "                    \"page\": page_num,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": end_idx + 1  # Make end exclusive for easier indexing\n",
    "                }\n",
    "                page_doc.metadata[\"relevance_spans\"].append(span)\n",
    "                total_spans += 1\n",
    "        \n",
    "        logger.info(f\"Found {total_spans} relevance spans across all pages\")\n",
    "        return page_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, AnnotationError):\n",
    "            raise\n",
    "        logger.error(f\"Annotation failed: {str(e)}\")\n",
    "        raise AnnotationError(f\"Failed to annotate documents: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787e772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:32:48,148 - INFO - Evaluating with parameters: chunk=200, overlap=30, top-k=3\n",
      "2025-11-09 21:32:48,149 - INFO - ============================================================\n",
      "2025-11-09 21:32:48,150 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-09 21:32:48,150 - INFO - ============================================================\n",
      "2025-11-09 21:32:48,151 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n",
      "2025-11-09 21:32:51,962 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-09 21:32:51,965 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:32:51,966 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-09 21:32:51,966 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-09 21:32:51,968 - INFO - Found 10 relevance spans across all pages\n",
      "2025-11-09 21:32:51,969 - INFO - Splitting documents with chunk_size=200, overlap=30\n",
      "2025-11-09 21:32:51,976 - INFO - Created 157 chunks\n",
      "2025-11-09 21:32:51,978 - INFO - Found 17 relevant chunks out of 157 total\n",
      "2025-11-09 21:32:51,979 - INFO - Prepared 157 chunks for Chroma\n",
      "2025-11-09 21:32:52,945 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-09 21:32:53,420 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:53,818 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:32:53,818 - INFO - Evaluating on 10 queries\n",
      "2025-11-09 21:32:53,960 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:53,992 - WARNING - IDCG is 0, returning nDCG=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_39y_b47w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:32:54,203 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:54,207 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:54,406 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:54,410 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:54,592 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:54,596 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:54,838 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:54,842 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:54,994 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:55,149 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:55,491 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:55,500 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:55,706 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:55,710 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:55,902 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:32:55,906 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:32:55,907 - INFO - ============================================================\n",
      "2025-11-09 21:32:55,907 - INFO - Average DCG:  0.1631\n",
      "2025-11-09 21:32:55,908 - INFO - Average nDCG: 0.1631\n",
      "2025-11-09 21:32:55,908 - INFO - ============================================================\n",
      "2025-11-09 21:32:55,909 - INFO - Evaluating with parameters: chunk=300, overlap=30, top-k=3\n",
      "2025-11-09 21:32:55,909 - INFO - ============================================================\n",
      "2025-11-09 21:32:55,910 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-09 21:32:55,910 - INFO - ============================================================\n",
      "2025-11-09 21:32:55,911 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Average DCG:  0.1631\n",
      "Average nDCG: 0.1631\n",
      "Evaluated on 10 queries with k=3\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:33:00,045 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-09 21:33:00,048 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:33:00,049 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-09 21:33:00,049 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-09 21:33:00,052 - INFO - Found 10 relevance spans across all pages\n",
      "2025-11-09 21:33:00,052 - INFO - Splitting documents with chunk_size=300, overlap=30\n",
      "2025-11-09 21:33:00,059 - INFO - Created 100 chunks\n",
      "2025-11-09 21:33:00,062 - INFO - Found 14 relevant chunks out of 100 total\n",
      "2025-11-09 21:33:00,063 - INFO - Prepared 100 chunks for Chroma\n",
      "2025-11-09 21:33:01,034 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-09 21:33:01,974 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:02,328 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:33:02,329 - INFO - Evaluating on 10 queries\n",
      "2025-11-09 21:33:02,540 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_tkajbgot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:33:02,748 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:02,752 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:03,000 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,118 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,122 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:03,249 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,389 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,542 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,686 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,938 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:03,949 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:04,247 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:04,251 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:04,251 - INFO - ============================================================\n",
      "2025-11-09 21:33:04,252 - INFO - Average DCG:  0.4697\n",
      "2025-11-09 21:33:04,252 - INFO - Average nDCG: 0.5262\n",
      "2025-11-09 21:33:04,252 - INFO - ============================================================\n",
      "2025-11-09 21:33:04,253 - INFO - Evaluating with parameters: chunk=400, overlap=30, top-k=3\n",
      "2025-11-09 21:33:04,253 - INFO - ============================================================\n",
      "2025-11-09 21:33:04,254 - INFO - Starting RAG Evaluation Pipeline\n",
      "2025-11-09 21:33:04,255 - INFO - ============================================================\n",
      "2025-11-09 21:33:04,256 - INFO - Loading PDF from: data/BoardGamesRuleBook/CATAN.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Average DCG:  0.4697\n",
      "Average nDCG: 0.5262\n",
      "Evaluated on 10 queries with k=3\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:33:08,210 - INFO - Loaded 12 pages from PDF\n",
      "2025-11-09 21:33:08,212 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:33:08,213 - INFO - Processing 10 training Q&A pairs\n",
      "2025-11-09 21:33:08,213 - INFO - Aho-Corasick automaton built successfully\n",
      "2025-11-09 21:33:08,216 - INFO - Found 10 relevance spans across all pages\n",
      "2025-11-09 21:33:08,216 - INFO - Splitting documents with chunk_size=400, overlap=30\n",
      "2025-11-09 21:33:08,226 - INFO - Created 76 chunks\n",
      "2025-11-09 21:33:08,227 - INFO - Found 11 relevant chunks out of 76 total\n",
      "2025-11-09 21:33:08,227 - INFO - Prepared 76 chunks for Chroma\n",
      "2025-11-09 21:33:09,207 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2025-11-09 21:33:09,860 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:10,240 - INFO - Loaded JSON from: data/BoardGamesRuleBook/CATAN_train_small.json\n",
      "2025-11-09 21:33:10,240 - INFO - Evaluating on 10 queries\n",
      "2025-11-09 21:33:10,428 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Temporary Chroma DB created at: C:\\Users\\khchu\\AppData\\Local\\Temp\\chroma_eval_fbdefhw7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 21:33:10,821 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:10,825 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:10,991 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:10,995 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:11,126 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:11,130 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:11,308 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:11,629 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:11,909 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:11,913 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:12,101 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:12,105 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:13,027 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:13,032 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:13,275 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-09 21:33:13,285 - WARNING - IDCG is 0, returning nDCG=0\n",
      "2025-11-09 21:33:13,287 - INFO - ============================================================\n",
      "2025-11-09 21:33:13,288 - INFO - Average DCG:  0.3187\n",
      "2025-11-09 21:33:13,288 - INFO - Average nDCG: 0.2860\n",
      "2025-11-09 21:33:13,289 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Average DCG:  0.3187\n",
      "Average nDCG: 0.2860\n",
      "Evaluated on 10 queries with k=3\n",
      "============================================================\n",
      "\n",
      "\n",
      "ðŸ“ Saved results to rag_hyperparam_eval.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DOCUMENT CHUNKING\n",
    "# =============================================================================\n",
    "\n",
    "def split_text(\n",
    "    docs: List[Document], \n",
    "    chunk_size: int = 300, \n",
    "    chunk_overlap: int = 30\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for embedding.\n",
    "    \n",
    "    Why chunk?\n",
    "    - Embeddings work better on focused, semantic units\n",
    "    - Smaller chunks = more precise retrieval\n",
    "    - Overlap ensures we don't split important context\n",
    "    \n",
    "    Why these defaults?\n",
    "    - chunk_size=300: ~75 tokens, good for rule-specific content\n",
    "    - chunk_overlap=30: 10% overlap preserves context at boundaries\n",
    "    \n",
    "    Args:\n",
    "        docs: List of Document objects\n",
    "        chunk_size: Target size for each chunk (characters)\n",
    "        chunk_overlap: Overlap between consecutive chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk Documents with start_index in metadata\n",
    "        \n",
    "    Raises:\n",
    "        ChunkingError: If chunking process fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if chunk_size <= 0:\n",
    "            raise ChunkingError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "        \n",
    "        if chunk_overlap < 0:\n",
    "            raise ChunkingError(f\"chunk_overlap cannot be negative, got {chunk_overlap}\")\n",
    "        \n",
    "        if chunk_overlap >= chunk_size:\n",
    "            raise ChunkingError(\n",
    "                f\"chunk_overlap ({chunk_overlap}) must be less than \"\n",
    "                f\"chunk_size ({chunk_size})\"\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Splitting documents with chunk_size={chunk_size}, overlap={chunk_overlap}\")\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,  # Use character count\n",
    "            add_start_index=True  # Critical: needed for coverage calculation\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        logger.info(f\"Created {len(chunks)} chunks\")\n",
    "        \n",
    "        return chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, ChunkingError):\n",
    "            raise\n",
    "        logger.error(f\"Chunking failed: {str(e)}\")\n",
    "        raise ChunkingError(f\"Failed to split documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# COVERAGE CALCULATION\n",
    "# =============================================================================\n",
    "\n",
    "def compute_overlap(\n",
    "    span_start: int, \n",
    "    span_end: int, \n",
    "    chunk_start: int, \n",
    "    chunk_end: int\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Compute character overlap between a relevance span and a chunk.\n",
    "    \n",
    "    Example:\n",
    "        Span:  [10, 30)  (relevant text from annotation)\n",
    "        Chunk: [20, 50)  (text chunk)\n",
    "        Overlap: [20, 30) = 10 characters\n",
    "    \n",
    "    Args:\n",
    "        span_start: Start index of relevance span\n",
    "        span_end: End index of relevance span (exclusive)\n",
    "        chunk_start: Start index of chunk\n",
    "        chunk_end: End index of chunk (exclusive)\n",
    "        \n",
    "    Returns:\n",
    "        Number of overlapping characters\n",
    "    \"\"\"\n",
    "    overlap_start = max(span_start, chunk_start)\n",
    "    overlap_end = min(span_end, chunk_end)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "\n",
    "def generate_relevant_chunks_with_coverage(\n",
    "    chunks: List[Document]\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Calculate coverage scores for chunks containing ground truth spans.\n",
    "    \n",
    "    Coverage = (overlap_length / relevance_span_length)\n",
    "    \n",
    "    Why coverage?\n",
    "    - Measures \"how much of the relevant content is in this chunk\"\n",
    "    - Coverage=1.0: entire relevant span is in the chunk (perfect)\n",
    "    - Coverage=0.5: only half the relevant content is present\n",
    "    - Coverage=0.0: chunk doesn't contain relevant content\n",
    "    \n",
    "    This is better than binary relevance because:\n",
    "    - Distinguishes between partial and complete matches\n",
    "    - Handles cases where spans cross chunk boundaries\n",
    "    - Provides granular relevance scores for nDCG calculation\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        List of Documents containing only relevant chunks with coverage scores\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If coverage calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relevant_chunks = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            chunk_start = chunk.metadata.get(\"start_index\", 0)\n",
    "            chunk_end = chunk_start + len(chunk.page_content)\n",
    "            relevance_spans = chunk.metadata.get(\"relevance_spans\", [])\n",
    "            \n",
    "            # Skip chunks without any ground truth annotations\n",
    "            if not relevance_spans:\n",
    "                continue\n",
    "            \n",
    "            # Create copy to avoid modifying original\n",
    "            annotated_chunk = copy.deepcopy(chunk)\n",
    "            annotated_chunk.metadata[\"coverage_per_query\"] = []\n",
    "            \n",
    "            for span in relevance_spans:\n",
    "                qa_id = span[\"qa_id\"]\n",
    "                \n",
    "                # Calculate how much of the span overlaps with this chunk\n",
    "                overlap_len = compute_overlap(\n",
    "                    span[\"start\"], span[\"end\"], \n",
    "                    chunk_start, chunk_end\n",
    "                )\n",
    "                \n",
    "                relevance_len = span[\"end\"] - span[\"start\"]\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if relevance_len == 0:\n",
    "                    logger.warning(f\"Zero-length relevance span for qa_id={qa_id}\")\n",
    "                    continue\n",
    "                \n",
    "                coverage = overlap_len / relevance_len\n",
    "                \n",
    "                # Skip queries with no overlap\n",
    "                if coverage == 0:\n",
    "                    continue\n",
    "                \n",
    "                annotated_chunk.metadata[\"coverage_per_query\"].append({\n",
    "                    \"qa_id\": qa_id,\n",
    "                    \"coverage\": coverage\n",
    "                })\n",
    "            \n",
    "            # Only keep chunks that have at least one relevant query\n",
    "            if annotated_chunk.metadata[\"coverage_per_query\"]:\n",
    "                annotated_chunk.metadata[\"chunk_id\"] = chunk_idx\n",
    "                relevant_chunks.append(annotated_chunk)\n",
    "        \n",
    "        logger.info(f\"Found {len(relevant_chunks)} relevant chunks out of {len(chunks)} total\")\n",
    "        return relevant_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Coverage calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate coverage: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# VECTOR STORE OPERATIONS\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_chunks_for_chroma(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter complex metadata for Chroma compatibility.\n",
    "    \n",
    "    Why needed?\n",
    "    - Chroma only supports simple types (str, int, float, bool)\n",
    "    - Complex types (lists, dicts) cause serialization errors\n",
    "    - We keep complex metadata in separate 'relevant_chunks' list\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Documents with filtered metadata safe for Chroma\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If metadata filtering fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        retrievable_docs = []\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Filter to simple metadata types\n",
    "            filtered_doc = filter_complex_metadata([chunk])[0]\n",
    "            \n",
    "            # Add chunk_id for later lookup\n",
    "            filtered_doc.metadata[\"chunk_id\"] = chunk_idx\n",
    "            \n",
    "            retrievable_docs.append(filtered_doc)\n",
    "        \n",
    "        logger.info(f\"Prepared {len(retrievable_docs)} chunks for Chroma\")\n",
    "        return retrievable_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Metadata filtering failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to prepare chunks: {str(e)}\") from e\n",
    "\n",
    "\n",
    "\n",
    "def save_to_chroma(\n",
    "    chunks: List[Document], \n",
    "    embedding_model: str = \"text-embedding-ada-002\"\n",
    ") -> Chroma:\n",
    "    \"\"\"\n",
    "    Create and persist Chroma vector store.\n",
    "    \n",
    "    Note: This clears existing database!\n",
    "    - Ensures fresh embeddings\n",
    "    - Avoids stale data issues\n",
    "    - For production, consider incremental updates\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of Document chunks (with simple metadata)\n",
    "        embedding_model: OpenAI embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        Initialized Chroma vector store\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If vector store creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an isolated temporary directory\n",
    "        tmp_dir = tempfile.mkdtemp(prefix=\"chroma_eval_\")\n",
    "\n",
    "        # Initialize embedding model\n",
    "        embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "\n",
    "        # Create the Chroma vector store from documents\n",
    "        db = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=tmp_dir\n",
    "        )\n",
    "\n",
    "        print(f\"[INFO] Temporary Chroma DB created at: {tmp_dir}\")\n",
    "        return db, tmp_dir\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Vector store creation failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to create vector store: {str(e)}\") from e\n",
    "\n",
    "def retrieve_top_k(\n",
    "    db: Chroma, \n",
    "    query: str, \n",
    "    k: int = 3\n",
    ") -> List[Tuple[str, str, int, float]]:\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar chunks for a query.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (source, content, chunk_id, relevance_score)\n",
    "        \n",
    "    Raises:\n",
    "        VectorStoreError: If retrieval fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if k <= 0:\n",
    "            raise VectorStoreError(f\"k must be positive, got {k}\")\n",
    "        \n",
    "        logger.debug(f\"Retrieving top-{k} chunks for query: {query[:50]}...\")\n",
    "        \n",
    "        results = db.similarity_search_with_relevance_scores(query, k=k)\n",
    "        \n",
    "        formatted_results = [\n",
    "            (\n",
    "                doc.metadata.get(\"source\", \"unknown\"),\n",
    "                doc.page_content,\n",
    "                doc.metadata.get(\"chunk_id\", -1),\n",
    "                score\n",
    "            )\n",
    "            for doc, score in results\n",
    "        ]\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Retrieval failed: {str(e)}\")\n",
    "        raise VectorStoreError(f\"Failed to retrieve documents: {str(e)}\") from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION METRICS\n",
    "# =============================================================================\n",
    "\n",
    "def dcg(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Discounted Cumulative Gain.\n",
    "    \n",
    "    Formula: DCG = Î£(rel_i / log2(i + 2)) for i in range(len(scores))\n",
    "    \n",
    "    Why log2(i + 2)?\n",
    "    - Position 0: log2(2) = 1 (no discount)\n",
    "    - Position 1: log2(3) = 1.58 (small discount)\n",
    "    - Position 2: log2(4) = 2 (larger discount)\n",
    "    - Later positions are increasingly discounted\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores (coverage values)\n",
    "        \n",
    "    Returns:\n",
    "        DCG score\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        dcg_value = np.sum([\n",
    "            rel / np.log2(idx + 2)\n",
    "            for idx, rel in enumerate(relevance_scores)\n",
    "        ])\n",
    "        \n",
    "        return float(dcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"DCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate DCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def ndcg_at_k(relevance_scores: List[float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Normalized Discounted Cumulative Gain.\n",
    "    \n",
    "    nDCG = DCG / IDCG\n",
    "    \n",
    "    Why normalize?\n",
    "    - Makes scores comparable across queries\n",
    "    - Range: [0, 1] where 1 = perfect ranking\n",
    "    - Accounts for different numbers of relevant items\n",
    "    \n",
    "    Args:\n",
    "        relevance_scores: List of relevance scores\n",
    "        \n",
    "    Returns:\n",
    "        nDCG score between 0 and 1\n",
    "        \n",
    "    Raises:\n",
    "        EvaluationError: If calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not relevance_scores:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate DCG with actual ranking\n",
    "        dcg_value = dcg(relevance_scores)\n",
    "        \n",
    "        # Calculate ideal DCG (perfect ranking)\n",
    "        ideal_scores = sorted(relevance_scores, reverse=True)\n",
    "        idcg_value = dcg(ideal_scores)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if idcg_value == 0:\n",
    "            logger.warning(\"IDCG is 0, returning nDCG=0\")\n",
    "            return 0.0\n",
    "        \n",
    "        ndcg_value = dcg_value / idcg_value\n",
    "        return float(ndcg_value)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"nDCG calculation failed: {str(e)}\")\n",
    "        raise EvaluationError(f\"Failed to calculate nDCG: {str(e)}\") from e\n",
    "\n",
    "\n",
    "def get_coverage(\n",
    "    chunk_id: int, \n",
    "    qa_id: str, \n",
    "    relevant_chunks: List[Document]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Retrieve coverage score for a specific chunk and query.\n",
    "    \n",
    "    This is a lookup function that connects:\n",
    "    - Retrieved chunk (by chunk_id from vector search)\n",
    "    - Query (by qa_id from evaluation set)\n",
    "    - Ground truth coverage (pre-computed in relevant_chunks)\n",
    "    \n",
    "    Args:\n",
    "        chunk_id: ID of the retrieved chunk\n",
    "        qa_id: ID of the query being evaluated\n",
    "        relevant_chunks: List of annotated chunks with coverage scores\n",
    "        \n",
    "    Returns:\n",
    "        Coverage score (0-1), or 0 if not found\n",
    "    \"\"\"\n",
    "    for chunk in relevant_chunks:\n",
    "        if chunk.metadata.get(\"chunk_id\") != chunk_id:\n",
    "            continue\n",
    "        \n",
    "        for coverage_entry in chunk.metadata.get(\"coverage_per_query\", []):\n",
    "            if coverage_entry[\"qa_id\"] == qa_id:\n",
    "                return coverage_entry[\"coverage\"]\n",
    "    \n",
    "    # Return 0 if chunk has no coverage for this query\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EVALUATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_rag_system(\n",
    "    pdf_path: str,\n",
    "    training_qa_path: str,\n",
    "    chunk_size: int = 300,\n",
    "    chunk_overlap: int = 30,\n",
    "    k: int = 3,\n",
    "    embedding_model: str = \"text-embedding-ada-002\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run complete RAG evaluation pipeline.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. Load and clean PDF\n",
    "    2. Annotate with ground truth Q&A\n",
    "    3. Chunk documents\n",
    "    4. Calculate coverage for relevant chunks\n",
    "    5. Create vector store\n",
    "    6. For each query: retrieve top-k and calculate metrics\n",
    "    7. Report average DCG and nDCG\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to board game manual PDF\n",
    "        training_qa_path: Path to training Q&A JSON\n",
    "        chunk_size: Size of text chunks\n",
    "        chunk_overlap: Overlap between chunks\n",
    "        k: Number of documents to retrieve\n",
    "        embedding_model: OpenAI embedding model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with evaluation results\n",
    "        \n",
    "    Raises:\n",
    "        RAGEvaluationError: If any pipeline stage fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"Starting RAG Evaluation Pipeline\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Load PDF\n",
    "        docs = load_documents(pdf_path)\n",
    "        \n",
    "        # Step 2: Annotate with ground truth\n",
    "        docs_with_qa = load_training_qa_to_docs(training_qa_path, docs)\n",
    "        \n",
    "        # Step 3: Chunk documents\n",
    "        chunks = split_text(docs_with_qa, chunk_size, chunk_overlap)\n",
    "        \n",
    "        # Step 4: Calculate coverage for relevant chunks\n",
    "        relevant_chunks = generate_relevant_chunks_with_coverage(chunks)\n",
    "        \n",
    "        # Step 5: Prepare and store in vector DB\n",
    "        chunks_for_chroma = prepare_chunks_for_chroma(chunks)\n",
    "        db, tmp_dir  = save_to_chroma(chunks_for_chroma, embedding_model)\n",
    "\n",
    "        # Step 6: Load evaluation queries\n",
    "        qa_data = load_json(training_qa_path)\n",
    "        evaluation_qas = qa_data.get(\"training_qas\", [])\n",
    "        \n",
    "        if not evaluation_qas:\n",
    "            raise EvaluationError(\"No evaluation queries found in JSON\")\n",
    "        \n",
    "        logger.info(f\"Evaluating on {len(evaluation_qas)} queries\")\n",
    "        \n",
    "        # Step 7: Evaluate each query\n",
    "        dcg_values = []\n",
    "        ndcg_values = []\n",
    "        query_results = []\n",
    "        \n",
    "        for qa in evaluation_qas:\n",
    "            qa_id = qa.get(\"id\")\n",
    "            question = qa.get(\"question\")\n",
    "            \n",
    "            if not question:\n",
    "                logger.warning(f\"Skipping query with missing question: {qa_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Retrieve top-k chunks\n",
    "            top_k_results = retrieve_top_k(db, question, k=k)\n",
    "            \n",
    "            # Calculate coverage scores for retrieved chunks\n",
    "            coverage_scores = []\n",
    "            for source, content, chunk_id, similarity_score in top_k_results:\n",
    "                coverage = get_coverage(chunk_id, qa_id, relevant_chunks)\n",
    "                coverage_scores.append(coverage)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            query_dcg = dcg(coverage_scores)\n",
    "            query_ndcg = ndcg_at_k(coverage_scores)\n",
    "            \n",
    "            dcg_values.append(query_dcg)\n",
    "            ndcg_values.append(query_ndcg)\n",
    "            \n",
    "            query_results.append({\n",
    "                \"qa_id\": qa_id,\n",
    "                \"question\": question,\n",
    "                \"dcg\": query_dcg,\n",
    "                \"ndcg\": query_ndcg,\n",
    "                \"coverage_scores\": coverage_scores\n",
    "            })\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_dcg = float(np.mean(dcg_values))\n",
    "        avg_ndcg = float(np.mean(ndcg_values))\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Average DCG:  {avg_dcg:.4f}\")\n",
    "        logger.info(f\"Average nDCG: {avg_ndcg:.4f}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            \"avg_dcg\": avg_dcg,\n",
    "            \"avg_ndcg\": avg_ndcg,\n",
    "            \"num_queries\": len(evaluation_qas),\n",
    "            \"k\": k,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": chunk_overlap,\n",
    "            \"query_results\": query_results\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if isinstance(e, RAGEvaluationError):\n",
    "            raise\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        raise RAGEvaluationError(f\"Evaluation pipeline failed: {str(e)}\") from e\n",
    "\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PDF_PATH = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "    TRAINING_QA_PATH = \"data/BoardGamesRuleBook/CATAN_train_small.json\"\n",
    "    CHUNK_SIZES = [200, 300, 400]\n",
    "    CHUNK_OVERLAPS = [30]\n",
    "    Ks = [3]\n",
    "    \n",
    "    all_results = []\n",
    "    for CHUNK_SIZE, CHUNK_OVERLAP, k in product(CHUNK_SIZES, CHUNK_OVERLAPS, Ks):\n",
    "        logger.info(f\"Evaluating with parameters: chunk={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}, top-k={k}\")\n",
    "        try:\n",
    "            # Run evaluation\n",
    "            results = evaluate_rag_system(\n",
    "                pdf_path=PDF_PATH,\n",
    "                training_qa_path=TRAINING_QA_PATH,\n",
    "                chunk_size=CHUNK_SIZE,\n",
    "                chunk_overlap=CHUNK_OVERLAP,\n",
    "                k=k\n",
    "            )\n",
    "\n",
    "            all_results.append({\n",
    "                \"chunk_size\": CHUNK_SIZE,\n",
    "                \"overlap\": CHUNK_OVERLAP,\n",
    "                \"top_k\": k,\n",
    "                **results,\n",
    "            })\n",
    "\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"EVALUATION RESULTS\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Average DCG:  {results['avg_dcg']:.4f}\")\n",
    "            print(f\"Average nDCG: {results['avg_ndcg']:.4f}\")\n",
    "            print(f\"Evaluated on {results['num_queries']} queries with k={results['k']}\")\n",
    "            print(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "        except RAGEvaluationError as e:\n",
    "            logger.error(f\"Evaluation failed: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "    # Convert to DataFrame for easy comparison\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"rag_hyperparam_eval.csv\", index=False)\n",
    "    print(\"\\nðŸ“ Saved results to rag_hyperparam_eval.csv\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
