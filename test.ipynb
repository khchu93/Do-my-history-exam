{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9741f3",
   "metadata": {},
   "source": [
    "### Building a Local Knowledge Assistant with LangChain & OpenAI\n",
    "\n",
    "In this project, I implemented a `Retrieval-Augmented Generation (RAG)` pipeline using `LangChain`, `Chroma`, and `OpenAI embeddings`. The system retrieves relevant chunks of information from my document collection to provide more accurate and context-aware answers. By combining retrieval with generation, I can leverage a smaller, pre-trained LLM while still achieving detailed and precise responses, without the need for costly fine-tuning. This setup allows me to experiment with AI-driven Q&A in a practical, hands-on way.\n",
    "\n",
    "This project was inspired by [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch?v=tcqEUSNCn8I) and adapted for personal learning and experimentation.\n",
    "\n",
    "### Why RAG Matters?\n",
    "\n",
    "`Fine-tuning` an LLM can be extremely **expensive** and **resource-heavy** — it requires access to large compute clusters, massive datasets, and careful optimization. Beyond computational costs, fine-tuning introduces **deployment complexity** (maintaining multiple model versions), **knowledge staleness** (requires retraining to update information), and **catastrophic forgetting** risks where the model loses general capabilities while adapting to specific domains.\n",
    "\n",
    "While techniques like `LoRA (Low-Rank Adaptation)` make fine-tuning more **efficient** by training only low-rank decomposition matrices rather than full weight updates, they still **demand GPU resources**, **model-specific expertise**, and **careful hyperparameter tuning**. More importantly, fine-tuning bakes knowledge into model weights, making it **opaque and non-auditable** — you can't easily trace which training examples influenced a specific output or update individual facts without full retraining.\n",
    "\n",
    "`RAG (Retrieval-Augmented Generation)`, on the other hand, is a **lightweight yet powerful alternative** that addresses these fundamental limitations. Instead of changing the model itself, it **augments the prompt dynamically** by retrieving relevant **external knowledge** from vector databases at inference time. This architectural choice provides several critical advantages: **knowledge remains external and auditable** (you can inspect exactly what context was retrieved), **updates are instantaneous** (add new documents without retraining), **citations are traceable** (ground responses in source material), and **domain adaptation requires no GPU compute** (just embed and index your documents).\n",
    "\n",
    "RAG also offers **better separation of concerns**: the LLM handles reasoning and language generation while the retrieval system manages domain knowledge. This makes systems more **maintainable**, **debuggable**, and **cost-effective** — you pay only for embedding and inference costs rather than full training runs. For production applications requiring up-to-date information, compliance with data lineage requirements, or rapid iteration on domain knowledge, RAG often proves more practical than fine-tuning. The trade-off is handling **retrieval quality** (chunking strategies, embedding models, similarity metrics) and **context window management**, but these challenges are generally more tractable than the complexities of fine-tuning at scale.\n",
    "\n",
    "### Why LangChain?\n",
    "`LangChain` is an orchestration framework that dramatically **simplifies building LLM applications** by providing **high-level abstractions for common patterns**. Instead of manually handling API calls, text chunking, embeddings, and vector database operations, LangChain condenses what would typically take **100+ lines** of integration code into **10-15 lines** of declarative operations. It offers **unified interfaces** across different LLM providers and vector databases, allowing you to swap components without rewriting your application, while including production-ready features like retry logic, error handling, and observability.\n",
    "\n",
    "For a RAG exploration project, LangChain **accelerates development from days to hours**, letting you focus on understanding core concepts like chunking strategies and retrieval methods rather than API plumbing. It **provides battle-tested components for document loading, text splitting, embeddings, and retrieval chains with integrations for major vector stores and LLM providers**, making it ideal for rapid prototyping and experimentation.\n",
    "\n",
    "However, these abstractions come with **trade-offs**. The framework can **obscure what's happening** under the hood, making debugging more complex, and has a **learning curve to understand** its conventions. The abstraction layers can introduce **performance overhead problematic** for high-throughput production systems, and the framework has experienced breaking **changes across versions**. For production at scale, teams often replace LangChain components with leaner implementations where they need finer control.\n",
    "\n",
    "### Why Chroma?\n",
    "\n",
    "`Vector databases` are essential for RAG systems, but many solutions like `Pinecone` or `Weaviate` require **external infrastructure**, **API dependencies**, and **ongoing costs** that can complicate development and deployment workflows.\n",
    "\n",
    "While cloud-based vector databases offer **scalability** and **managed services**, they introduce **network latency**, **vendor lock-in**, and **data privacy concerns** — your embeddings and documents live on third-party servers, which may not be acceptable for sensitive applications or offline use cases.\n",
    "\n",
    "`Chroma`, on the other hand, is a **lightweight yet powerful embedded database** designed specifically for AI applications. It runs **locally in-process** with your Python application, requiring **no separate server** or external dependencies. Chroma stores embeddings and metadata on disk with optional persistence, making it perfect for development, prototyping, and production deployments where you need full control over your data.\n",
    "\n",
    "Chroma also offers **developer-friendly simplicity**: minimal setup (just `pip install chromadb`), intuitive APIs for adding and querying documents, built-in support for metadata filtering, and seamless integration with LangChain and other frameworks. The architecture provides **flexible deployment options** — start with local embedded mode for development, then scale to client-server mode for production if needed. For exploration projects and applications requiring **fast iteration**, **data locality**, and **zero infrastructure overhead**, Chroma eliminates the operational complexity of managed vector databases while maintaining production-ready performance. The trade-off is handling **horizontal scaling** and **high-availability** yourself if you outgrow single-node deployments, but for most RAG applications, Chroma's simplicity and local-first design make it the ideal starting point.RetryClaude can make mistakes. Please double-check responses.\n",
    "\n",
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deae934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "# https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca6c1",
   "metadata": {},
   "source": [
    "The project begins by **converting the target documents into LangChain** `Document` **objects** using the `DirectoryLoader` function. This preserves both the **text** and **metadata**, such as the source path and start index, which are required for LangChain's downstream functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de2c35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw files into Document\n",
    "def load_documents(DATA_PATH):\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e062cf",
   "metadata": {},
   "source": [
    "Next, the documents are **split into smaller chunks**. This is necessary primarily for **retrieval precision in RAG systems**. Smaller chunks allow the system to retrieve **only the most relevant information** rather than entire documents, improving semantic similarity matching. **Large chunks** can **dilute semantic meaning** and introduce **irrelevant context**, while **smaller chunks** enable more **focused retrieval**. Additionally, chunking helps manage **LLM context window limitations**, though retrieval quality is the primary consideration.\n",
    "\n",
    "The 'RecursiveCharacterTextSplitter' controls how text is divided using two key parameters: 'chunk_size' and 'chunk_overlap'. 'chunk_size' defines the *maximum length* of each chunk, while 'chunk_overlap' *repeats a portion* of the previous chunk in the next one to **reduce the chance of splitting related information** across chunk boundaries. Choosing appropriate values for these parameters directly impacts retrieval quality, and thus the overall performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0be34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into small chunks\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,         #nb of characters in each chunk\n",
    "        chunk_overlap=100,      #nb of characters to overlap between chunks\n",
    "        length_function=len,    #decide how to measure the chunk, e.g., character, token, etc\n",
    "        add_start_index=True,   #add the starting index of the chunk\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\\n\")\n",
    "\n",
    "    print(f\"Print chunk 10 content: \")\n",
    "    document = chunks[10]\n",
    "    print(f\"Content: \\\"{document.page_content}\\\"\")\n",
    "    print(f\"Metadata: {document.metadata}\\n\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d6bb",
   "metadata": {},
   "source": [
    "Once the documents are split, each chunk is converted into a **vector embedding** using OpenAI's 'text-embedding-3-small' model. These **embeddings** are then stored in a 'Chroma' vector database along with the **original chunk text and metadata**. The 'Chroma' database enables efficient **similarity search** by **comparing query embeddings against stored chunk embeddings**.\n",
    "\n",
    "It is crucial to use the **same embedding model** for both **indexing chunks** and **embedding queries** because different models produce **incompatible vector representations** that exist in different semantic spaces, making retrieval unreliable or impossible. For this project, I use the default embedding model `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vector embedding to chunks and save the embedding vector along with the content and metadata to database\n",
    "def save_to_chroma(chunks: list[Document], CHROMA_PATH):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(model=\"text-embedding-ada-002\"), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c29e2",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b93f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 818 chunks.\n",
      "\n",
      "Print chunk 10 content: \n",
      "Content: \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\"\n",
      "Metadata: {'source': 'data\\\\books\\\\alice_in_wonderland.md', 'start_index': 1653}\n",
      "\n",
      "Saved 818 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key \n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "\n",
    "# split the document in chunks and save it to database along with its embedded vector\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_text(documents)\n",
    "db = save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac1a",
   "metadata": {},
   "source": [
    "The **prompt template** determines how **retrieved context** and the **user query** are **structured** for the **language model**. While prompt quality is difficult to quantify precisely, adhering to **established prompting principles significantly improves model responses**. Effective LLM prompting requires **clear, specific instructions** - similar to providing detailed directions to a new team member. Well-defined prompts guide the model toward desired outputs, while vague or ambiguous instructions increase output unpredictability, often resulting in irrelevant or inaccurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ed89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d87c",
   "metadata": {},
   "source": [
    "Once the Chroma database and prompt template are configured, the system can process user queries through the following pipeline: First, the query is embedded using the **same embedding model** applied to the document chunks, ensuring vector space consistency. The system then retrieves the **top-k most similar chunks** from the vector database (using L2 (Euclidean) distance by default). These retrieved chunks are combined with the original query according to the prompt template structure and passed to the LLM (default LLM: `gpt-3.5-turbo`) for response generation.\n",
    "\n",
    "Implementing **quality safeguards** is essential for production systems. This includes **rejecting empty** or **malformed queries** and filtering results when similarity scores fall **below a confidence threshold**, as low-similarity retrievals typically indicate insufficient relevant context and lead to unreliable outputs. With these components in place, the RAG pipeline can effectively retrieve pertinent information and generate well-informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17466451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k similarity:\n",
      "Top 1:\n",
      "Content: So Alice began telling them her adventures from the time when she first saw the White Rabbit. She was a little nervous about it just at first, the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide, but she gained courage as she went on. Her listeners\n",
      "L2 similarity: 0.8063262538137091\n",
      "\n",
      "Top 2:\n",
      "Content: “In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "L2 similarity: 0.8054134795155087\n",
      "\n",
      "Top 3:\n",
      "Content: “Is that the way you manage?” Alice asked.\n",
      "\n",
      "The Hatter shook his head mournfully. “Not I!” he replied. “We quarrelled last March—just before he went mad, you know—” (pointing with his tea spoon at the March Hare,) “—it was at the great concert given by the Queen of Hearts, and I had to sing\n",
      "L2 similarity: 0.7902825080190067\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "So Alice began telling them her adventures from the time when she first saw the White Rabbit. She was a little nervous about it just at first, the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide, but she gained courage as she went on. Her listeners\n",
      "\n",
      "---\n",
      "\n",
      "“In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "\n",
      "---\n",
      "\n",
      "“Is that the way you manage?” Alice asked.\n",
      "\n",
      "The Hatter shook his head mournfully. “Not I!” he replied. “We quarrelled last March—just before he went mad, you know—” (pointing with his tea spoon at the March Hare,) “—it was at the great concert given by the Queen of Hearts, and I had to sing\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How does Alice meet the Mad Hatter?\n",
      "\n",
      "Response: Alice meets the Mad Hatter after being directed by the Cheshire Cat to visit either the Hatter or the March Hare, both of whom are described as mad. She chooses to visit the Mad Hatter, who she finds at a tea party with the March Hare.\n",
      "Sources: ['data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md']\n"
     ]
    }
   ],
   "source": [
    "# query_text = input(\"Enter your query: \")\n",
    "query_text = \"How does Alice meet the Mad Hatter?\"\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "    # return\n",
    "\n",
    "print(\"Top k similarity:\")\n",
    "for i, k in enumerate(results):\n",
    "    print(f\"Top {i + 1}:\\nContent: {k[0].page_content}\\nL2 similarity: {k[1]}\\n\")\n",
    "\n",
    "# Generate the prompt template with context and query\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(prompt)\n",
    "\n",
    "# Implement the LLM and feed it with the prompt\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "response_text = model.invoke(prompt)\n",
    "\n",
    "# Print the formatted response\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text.content}\\nSources: {sources}\"\n",
    "print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722ea44",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397a080",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "llm_factory() requires a client instance. Text-only mode has been removed.\n\nTo migrate:\n  from openai import OpenAI\n  client = OpenAI(api_key='...')\n  llm = llm_factory('gpt-4o-mini', client=client)\n\nFor more details: https://docs.ragas.io/en/latest/llm-factory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnotebook_tqdm\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# generator_llm = LangchainLLMWrapper(model)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# openai_client = openai.OpenAI()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m generator_llm = \u001b[43mllm_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# generator_embeddings = OpenAIEmbeddings(client=openai_client)\u001b[39;00m\n\u001b[32m     10\u001b[39m generator_embeddings = OpenAIEmbeddings(client=openai)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\ragas\\llms\\base.py:484\u001b[39m, in \u001b[36mllm_factory\u001b[39m\u001b[34m(model, provider, client, **kwargs)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03mCreate an LLM instance for structured output generation using Instructor.\u001b[39;00m\n\u001b[32m    451\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    response = await llm.agenerate(prompt, ResponseModel)\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    485\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mllm_factory() requires a client instance. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mText-only mode has been removed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo migrate:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  from openai import OpenAI\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    489\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  client = OpenAI(api_key=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m  llm = llm_factory(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, client=client)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFor more details: https://docs.ragas.io/en/latest/llm-factory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    492\u001b[39m     )\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[32m    495\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodel parameter is required\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: llm_factory() requires a client instance. Text-only mode has been removed.\n\nTo migrate:\n  from openai import OpenAI\n  client = OpenAI(api_key='...')\n  llm = llm_factory('gpt-4o-mini', client=client)\n\nFor more details: https://docs.ragas.io/en/latest/llm-factory"
     ]
    }
   ],
   "source": [
    "# from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import OpenAIEmbeddings\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# generator_llm = LangchainLLMWrapper(model)\n",
    "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "# openai_client = openai.OpenAI()\n",
    "generator_llm = llm_factory(model)\n",
    "# generator_embeddings = OpenAIEmbeddings(client=openai_client)\n",
    "generator_embeddings = OpenAIEmbeddings(client=openai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
