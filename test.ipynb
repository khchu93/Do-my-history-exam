{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9741f3",
   "metadata": {},
   "source": [
    "### Building a Local Knowledge Assistant with LangChain & OpenAI\n",
    "\n",
    "In this project, I implemented a `Retrieval-Augmented Generation (RAG)` pipeline using `LangChain`, `Chroma`, and `OpenAI embeddings`. The system retrieves relevant chunks of information from my document collection to provide more accurate and context-aware answers. By combining retrieval with generation, I can leverage a smaller, pre-trained LLM while still achieving detailed and precise responses, without the need for costly fine-tuning. This setup allows me to experiment with AI-driven Q&A in a practical, hands-on way.\n",
    "\n",
    "This project was inspired by [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch?v=tcqEUSNCn8I) and adapted for personal learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf581640",
   "metadata": {},
   "source": [
    "### Why RAG Matters?\n",
    "\n",
    "As language models grow larger, updating and controlling their knowledge becomes increasingly impractical through fine-tuning alone. `RAG` introduces a retrieval-based paradigm that preserves model generality while enabling precise, real-time knowledge integration. &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8965fb",
   "metadata": {},
   "source": [
    "**Traditional Approaches** <br>\n",
    "**`Fine-tuning`** works but comes with serious baggage: massive **compute requirements**, maintaining multiple **model versions**, knowledge that goes **stale** (requiring full retraining), and catastrophic **forgetting** where models lose general capabilities. **`LoRA`** makes this more efficient by training **only low-rank matrices** instead of full weights, but you're still **burning GPU hours**, tuning **hyperparameters**, and—critically—**baking knowledge** into opaque model weights. Want to update a single fact? Good luck auditing which training examples influenced what, or making changes without retraining from scratch.\n",
    "\n",
    "**RAG: Knowledge Without Retraining** <br>\n",
    "`RAG` flips the script. Instead of modifying the model, it retrieves **relevant context** from **vector databases at inference time**. The advantages are game-changing: **instant knowledge updates** without retraining, **traceable citations** grounding outputs in source material, and **zero training compute**—just embed and index your documents. Your knowledge stays **external** and **auditable**, perfect for rapidly changing domains like customer support or compliance where information updates weekly. Plus, you get clean separation of concerns: **the LLM handles reasoning while retrieval manages knowledge**.\n",
    "\n",
    "**Trade-offs** <br>\n",
    "The catch? You're now in the **retrieval** game. Chunking strategies, embedding quality, and similarity metrics directly impact what context reaches your model. Poor retrieval means **hallucinations**. **Context windows** limit how much you can include, requiring **smart ranking** when relevant info spans many documents. But these challenges beat managing expensive retraining pipelines—RAG trades GPU clusters for vector database tuning, a much more tractable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47d63f",
   "metadata": {},
   "source": [
    "### Why LangChain?\n",
    "\n",
    "LLM applications in production need to integrate with dozens of **services—vector databases**, **document loaders**, **model providers**, **monitoring tools**. Alternatives like `Mirascope` or `LiteLLM` focus on **specific integration** challenges, but `LangChain`'s ecosystem is unmatched: **100+ integrations** maintained by both the core team and community, reducing custom boilerplate significantly.  &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225221e2",
   "metadata": {},
   "source": [
    "**Going Solo: Reinventing the Wheel** <br>\n",
    "**Going solo means building infrastructure from scratch**. `Raw API` calls seem simple until you're managing conversation history, implementing retry logic, handling streaming responses, and coordinating multi-step chains. **Custom frameworks give you control but demand serious engineering**: designing abstractions, integrating new model providers, building observability, maintaining compatibility. Every `RAG` pipeline or `agent system` requires **solving the same orchestration problems** teams everywhere have already tackled.\n",
    "\n",
    "**What LangChain Brings to the Table** <br>\n",
    "`LangChain` consolidates years of collective learning into **reusable patterns**. The framework shines in three areas: **provider abstraction** (switch between OpenAI, Anthropic, local models without rewriting logic), **composable chains** (LCEL makes complex workflows readable and debuggable), and **ecosystem integrations** (vector stores, document loaders, agent tools—all plug-and-play). You're not just getting code—you're getting architectural patterns that scale, from simple prompt chains to sophisticated agent loops with memory and tool use.\n",
    "\n",
    "**What You're Trading For: Velocity vs Transparency** <br>\n",
    "The tradeoff is **framework dependency**. `LangChain` adds abstraction layers that can **obscure what's happening under the hood**. Simple tasks might feel overengineered, and rapid framework evolution means staying current with breaking changes. Debugging requires understanding LangChain's execution model, not just your application logic. But the math usually works out: teams shipping complex LLM features fast versus teams stuck building plumbing. For most production use cases, LangChain's velocity wins outweigh the framework overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef501fb",
   "metadata": {},
   "source": [
    "### Why Chroma?\n",
    "\n",
    "When working on LLM projects that involve `semantic search` or `RAG`, vector databases are essential for storing and querying embeddings efficiently. `Chroma` stands out because it **balances ease-of-use, flexibility, and functionality**. Unlike `FAISS` (a low-level similarity search library), `Pinecone` (managed cloud service), or `Weaviate` (complex self-hosted solution), `Chroma` lets you run embedded in **your application** or as a **standalone server**, supports metadata and persistence out-of-the-box, and integrates seamlessly with frameworks like LangChain—making it ideal for developers who want to move fast without infrastructure overhead. &nbsp;&nbsp; ⇾(Expand the section for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b1427",
   "metadata": {},
   "source": [
    "**The Vector Database Landscape** <br>\n",
    "Many other solutions have their trade-offs. `FAISS` is extremely fast at large-scale similarity search but requires **manual management of metadata and persistence**. `Pinecone` offers a fully managed solution with advanced features but comes with **ongoing costs** and **cloud-only** limitations. `Weaviate` provides hybrid search and ML model integration but has a **steeper learning curve** and more **complex setup**. For engineers, these hurdles can slow down project development.\n",
    "\n",
    "**Chroma: Simplicity Without Sacrifice** <br>\n",
    "`Chroma`, on the other hand, simplifies the workflow. Its Python API is intuitive, it integrates naturally with `LangChain`, and it supports both **embedded** and **client-server deployment** modes. You can store embeddings along with metadata, filter results easily, and get your RAG pipelines running quickly. For small-to-medium scale projects, it allows you to focus on building intelligent LLM applications rather than spending time on database plumbing.\n",
    "\n",
    "**Where Chroma Makes Trade-offs** <br>\n",
    "Of course, `Chroma` has limitations. It **isn't optimized for extremely large datasets** with hundreds of millions of vectors and lacks some of the **advanced indexing options** that `FAISS` offers. However, for engineers who want to rapidly prototype and deploy LLM-powered solutions, `Chroma` provides a perfect balance of simplicity, power, and flexibility to get real results without unnecessary friction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e4eca",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Adapted from \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "# https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "import openai \n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca6c1",
   "metadata": {},
   "source": [
    "The project begins by **converting the target documents into LangChain** `Document` **objects** using the `DirectoryLoader` function. This preserves both the **text** and **metadata**, such as the source path and start index, which are required for LangChain's downstream functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw files into Document\n",
    "def load_documents(DATA_PATH: str):\n",
    "    documents = []\n",
    "    for pdf_file in glob.glob(f\"{DATA_PATH}/*.pdf\"):\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e062cf",
   "metadata": {},
   "source": [
    "Next, the documents are **split into smaller chunks**. This is necessary primarily for **retrieval precision in RAG systems**. Smaller chunks allow the system to retrieve **only the most relevant information** rather than entire documents, improving semantic similarity matching. **Large chunks** can **dilute semantic meaning** and introduce **irrelevant context**, while **smaller chunks** enable more **focused retrieval**. Additionally, chunking helps manage **LLM context window limitations**, though retrieval quality is the primary consideration.\n",
    "\n",
    "The 'RecursiveCharacterTextSplitter' controls how text is divided using two key parameters: 'chunk_size' and 'chunk_overlap'. 'chunk_size' defines the *maximum length* of each chunk, while 'chunk_overlap' *repeats a portion* of the previous chunk in the next one to **reduce the chance of splitting related information** across chunk boundaries. Choosing appropriate values for these parameters directly impacts retrieval quality, and thus the overall performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into small chunks\n",
    "def split_text(documents: list[Document], chunk_size: int=300, chunk_overlap: int=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,          #nb of characters in each chunk\n",
    "        chunk_overlap=chunk_overlap,    #nb of characters to overlap between chunks\n",
    "        length_function=len,            #decide how to measure the chunk, e.g., character, token, etc\n",
    "        add_start_index=True,           #add the starting index of the chunk\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} pages into {len(chunks)} chunks.\\n\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d6bb",
   "metadata": {},
   "source": [
    "Once the documents are split, each chunk is converted into a **vector embedding** using OpenAI's 'text-embedding-3-small' model. These **embeddings** are then stored in a 'Chroma' vector database along with the **original chunk text and metadata**. The 'Chroma' database enables efficient **similarity search** by **comparing query embeddings against stored chunk embeddings**.\n",
    "\n",
    "It is crucial to use the **same embedding model** for both **indexing chunks** and **embedding queries** because different models produce **incompatible vector representations** that exist in different semantic spaces, making retrieval unreliable or impossible. For this project, I use the default embedding model `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vector embedding to chunks and save the embedding vector along with the content and metadata to database\n",
    "def save_to_chroma(chunks: list[Document], CHROMA_PATH: str, model=\"text-embedding-ada-002\"):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(model=model), persist_directory=None\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c29e2",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b93f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 48 pages into 620 chunks.\n",
      "\n",
      "Print chunk 10 content: \n",
      "Content: \"3\n",
      "CN3081CATAN5 of 5\n",
      "v6.250401\n",
      "LARGEST ARMY\n",
      "LONGEST ROUTE\n",
      "The first player to play 3 Knight cards receives this tile. If another player plays more Knight cards, they immediately receive this tile.\"\n",
      "Metadata: {'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2025-03-05T13:56:17-06:00', 'moddate': '2025-03-05T13:56:17-06:00', 'source': 'data/BoardGamesRuleBook\\\\CATAN.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'start_index': 0}\n",
      "\n",
      "Saved 620 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"    # LLMs paper\n",
    "\n",
    "# split the document in chunks and save it to database along with its embedded vector\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_text(documents)\n",
    "\n",
    "print(f\"Print chunk 10 content: \")\n",
    "document = chunks[10]\n",
    "print(f\"Content: \\\"{document.page_content}\\\"\")\n",
    "print(f\"Metadata: {document.metadata}\\n\")\n",
    "\n",
    "db = save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f3459",
   "metadata": {},
   "source": [
    "test, pick the relevant paragraphs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e1f685d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "© 2025 C\n",
      "ATAN GmbH\n",
      "5\n",
      "6\n",
      "4   Place the Robber and Player Pieces\n",
      "• Place the robber on the desert hex.\n",
      "• Each player selects a color and takes the roads and buildings \n",
      "(settlements and cities) in that color along with a player aid.  \n",
      "In a 3-player game, do not use the white pieces.\n",
      "• Place 2 starting settlements and roads for each player  \n",
      "as shown.\n",
      "5   Collect Y our Starting Resources \n",
      "Each player takes the resource cards from the supply that \n",
      "match the hexes adjacent to their second settlement, \n",
      "highlighted in black. Keep these cards hidden  \n",
      "in your hand.\n",
      "6   Choose the First Player \n",
      "Each player rolls the dice. The player with the \n",
      "highest roll is the first player.\n"
     ]
    }
   ],
   "source": [
    "pdf_file = \"data/BoardGamesRuleBook/CATAN.pdf\"\n",
    "loader = PyPDFLoader(pdf_file)\n",
    "docs = loader.load()\n",
    "    \n",
    "print(docs[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65909ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph found on page 5\n",
      "Start index: 773, End index: 796\n"
     ]
    }
   ],
   "source": [
    "# Choose a specific page and paragraph text\n",
    "page_index = 4\n",
    "target_paragraph = \"Choose the First Player\"\n",
    "\n",
    "# Get the page text\n",
    "page_text = docs[page_index].page_content\n",
    "\n",
    "# Find where this paragraph appears\n",
    "start_idx = page_text.find(target_paragraph)\n",
    "if start_idx != -1:\n",
    "    end_idx = start_idx + len(target_paragraph)\n",
    "    print(f\"Paragraph found on page {page_index + 1}\")\n",
    "    print(f\"Start index: {start_idx}, End index: {end_idx}\")\n",
    "else:\n",
    "    print(\"Paragraph not found in this page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac1a",
   "metadata": {},
   "source": [
    "The **prompt template** determines how **retrieved context** and the **user query** are **structured** for the **language model**. While prompt quality is difficult to quantify precisely, adhering to **established prompting principles significantly improves model responses**. Effective LLM prompting requires **clear, specific instructions** - similar to providing detailed directions to a new team member. Well-defined prompts guide the model toward desired outputs, while vague or ambiguous instructions increase output unpredictability, often resulting in irrelevant or inaccurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ed89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d87c",
   "metadata": {},
   "source": [
    "Once the Chroma database and prompt template are configured, the system can process user queries through the following pipeline: First, the query is embedded using the **same embedding model** applied to the document chunks, ensuring vector space consistency. The system then retrieves the **top-k most similar chunks** from the vector database (using L2 (Euclidean) distance by default). These retrieved chunks are combined with the original query according to the prompt template structure and passed to the LLM (default LLM: `gpt-3.5-turbo`) for response generation.\n",
    "\n",
    "Implementing **quality safeguards** is essential for production systems. This includes **rejecting empty** or **malformed queries** and filtering results when similarity scores fall **below a confidence threshold**, as low-similarity retrievals typically indicate insufficient relevant context and lead to unreliable outputs. With these components in place, the RAG pipeline can effectively retrieve pertinent information and generate well-informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bfcd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the answer by feeding the LLM with prompt\n",
    "def generate_answer(question: str, context: str, isprintprompt: bool=False) :\n",
    "    # Generate the prompt template with context and query\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in context])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=question)\n",
    "    if isprintprompt:\n",
    "        print(prompt)\n",
    "\n",
    "    # Implement the LLM and feed it with the prompt\n",
    "    model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    return model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d28aae",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17466451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 similarity:\n",
      "Top 1:\n",
      "Content: does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "L2 similarity: 0.7823288076491218\n",
      "\n",
      "Top 2:\n",
      "Content: If a player has more than one building on the new hex, only steal 1 card. You may only play this \n",
      "card after the first barbarian attack (when the robber is placed on the board).\n",
      "© 2025 CATAN GmbH\n",
      "VICTORY POINT\n",
      "CONSTITUTION\n",
      "Play immediately into your  \n",
      "player area, even if it is  \n",
      "not your turn.\n",
      "L2 similarity: 0.7805705254229444\n",
      "\n",
      "Top 3:\n",
      "Content: You may not move the pirate until you have at least one route \n",
      "between one of your buildings and a village on a small island. When \n",
      "you move the pirate, you may either steal a resource card or a cloth \n",
      "token from a player whose ship is on an edge of the pirate’s new hex.\n",
      "WINNING THE GAME\n",
      "L2 similarity: 0.7649600913777823\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "\n",
      "---\n",
      "\n",
      "If a player has more than one building on the new hex, only steal 1 card. You may only play this \n",
      "card after the first barbarian attack (when the robber is placed on the board).\n",
      "© 2025 CATAN GmbH\n",
      "VICTORY POINT\n",
      "CONSTITUTION\n",
      "Play immediately into your  \n",
      "player area, even if it is  \n",
      "not your turn.\n",
      "\n",
      "---\n",
      "\n",
      "You may not move the pirate until you have at least one route \n",
      "between one of your buildings and a village on a small island. When \n",
      "you move the pirate, you may either steal a resource card or a cloth \n",
      "token from a player whose ship is on an edge of the pirate’s new hex.\n",
      "WINNING THE GAME\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: Can you steal from a city or settlement you just built this turn?\n",
      "\n",
      "Response: No, you cannot steal from a city or settlement you just built this turn.\n",
      "Sources: ['data/BoardGamesRuleBook\\\\CATAN_Cities&Knights.pdf', 'data/BoardGamesRuleBook\\\\CATAN_Cities&Knights.pdf', 'data/BoardGamesRuleBook\\\\CATAN_Seafarers.pdf']\n"
     ]
    }
   ],
   "source": [
    "# query_text = input(\"Enter your query: \")\n",
    "query_text = \"Can you steal from a city or settlement you just built this turn?\"\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results, similarity too low.\")\n",
    "    # return\n",
    "\n",
    "print(\"Top 3 similarity:\")\n",
    "for i, k in enumerate(results):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(f\"Top {i + 1}:\\nContent: {k[0].page_content}\\nL2 similarity: {k[1]}\\n\")\n",
    "\n",
    "response_text = generate_answer(query_text, results, True)\n",
    "\n",
    "# Print the formatted response\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text.content}\\nSources: {sources}\"\n",
    "print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8722ea44",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "`RAG` evaluation breaks down into two critical stages: **retrieval** and **generation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d92424",
   "metadata": {},
   "source": [
    "**Retrieval Evaluation**<br>\n",
    "In retrieval evaluation, we **measure how effectively the system identifies and ranks relevant documents for a given query**. Standard metrics like `Precision@k`, `Recall@k`, and `Mean Reciprocal Rank (MRR)` quantify whether the retriever surfaces useful chunks in its top results. High precision means most retrieved chunks are relevant; high recall means we're not missing critical information. These metrics guide optimization of chunk size, overlap, and embedding model selection—essentially tuning what context reaches the generator.\n",
    "\n",
    "**Generation Evaluation**<br>\n",
    "Generation evaluation **assesses the quality and factual grounding of responses given the retrieved context**. We use `RAGAS (Retrieval-Augmented Generation Assessment)` metrics: `faithfulness` (does the answer align with retrieved evidence?), answer correctness (how close to ground truth?), `answer relevancy` (is it on-topic?), and `context precision/recall` (did we retrieve the right supporting passages?). These metrics reveal not just what the model generates, but how well it uses retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbc06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "# Extract content from Q&A file generated with ChatGPT\n",
    "def read_qa_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Reads a minimal JSON file for Q&A evaluation.\n",
    "    \n",
    "    Expected format:\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"...\",\n",
    "            \"relevant_chunks\": [\"text1\", \"text2\"],\n",
    "            \"answer\": \"...\"\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    rows = []\n",
    "    for item in data:\n",
    "        rows.append({\n",
    "            \"question\": item[\"question\"],\n",
    "            \"reference\": item[\"answer\"],\n",
    "            \"relevant_chunk_texts\": item[\"relevant_chunks\"],\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def text_overlap_ratio(retrieved_text: str, ground_truth_text: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute how much of the ground truth text is contained in the retrieved text.\n",
    "    \n",
    "    Args:\n",
    "        retrieved_text: Text from retrieved chunk (typically longer)\n",
    "        ground_truth_text: Text from ground truth chunk (what we're looking for)\n",
    "        method: \"word\" for word-based, \"char\" for character-based, \"jaccard\" for Jaccard similarity\n",
    "        \n",
    "    Returns:\n",
    "        Ratio of ground truth covered by retrieved text (0 to 1)\n",
    "        - 1.0 means GT is fully contained in retrieved\n",
    "        - 0.5 means half of GT words are in retrieved\n",
    "        - 0.0 means no overlap\n",
    "    \"\"\"\n",
    "    # Normalize: lowercase and remove extra whitespace\n",
    "    retrieved_normalized = ' '.join(retrieved_text.lower().split())\n",
    "    gt_normalized = ' '.join(ground_truth_text.lower().split())\n",
    "    \n",
    "    if len(gt_normalized) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Check if GT appears as a substring (consecutive words)\n",
    "    if gt_normalized in retrieved_normalized:\n",
    "        return 1.0\n",
    "    \n",
    "    # Check for longest common substring ratio\n",
    "    matcher = SequenceMatcher(None, gt_normalized, retrieved_normalized)\n",
    "    match = matcher.find_longest_match(0, len(gt_normalized), 0, len(retrieved_normalized))\n",
    "    \n",
    "    # Return ratio of longest consecutive match\n",
    "    return match.size / len(gt_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can you play a development card the same turn you buy it?\n",
      "Retrieved 0: \n",
      "You may play 1 development card during your turn by placing it face up in your player area.  \n",
      "It may not be a card you built this turn. You may play a development card before rolling dice \n",
      "or at any time during the Action phase.\n",
      "GT: \n",
      "Not on the turn you bought it\n",
      "Overlap: \n",
      "0.10344827586206896\n",
      "Score: 0.8566725820415793\n",
      "\n",
      "Question: Can you play a development card the same turn you buy it?\n",
      "Retrieved 1: \n",
      "PLAY DEVELOPMENT CARDS\n",
      "• Not on the turn you bought it\n",
      "• Only 1 development card per turn\n",
      "• Before rolling dice or during the Action phase\n",
      "VP card exception: You may play multiple VP cards (even on the \n",
      "turn you buy them) in order to win the game.\n",
      "CN3081\n",
      "CATAN\n",
      "3 of 5\n",
      "v6.250401\n",
      "VP0\n",
      "VP1\n",
      "VPs2\n",
      "VPs?\n",
      "VP0\n",
      "GT: \n",
      "Not on the turn you bought it\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8393287640209977\n",
      "\n",
      "Question: Can you play a development card the same turn you buy it?\n",
      "Retrieved 2: \n",
      "PLAY DEVELOPMENT CARDS\n",
      "• Not on the turn you bought it\n",
      "• Only 1 development card per turn\n",
      "• Before rolling dice or during the Action phase\n",
      "VP card exception: You may play multiple VP cards (even on the \n",
      "turn you buy them) in order to win the game.\n",
      "PLAYER AID\n",
      "Production Phase\n",
      "GT: \n",
      "Not on the turn you bought it\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8276086105339368\n",
      "\n",
      "Question: Can you use Year of Plenty to take two of the same resource?\n",
      "Retrieved 0: \n",
      "make any number of 2:1 trades with the \n",
      "supply using that resource or commodity.\n",
      "© 2025 CATAN GmbH\n",
      "Name one resource. Each player \n",
      "must give you 2 cards of that  \n",
      "type if they have them. If  \n",
      "a player has only 1 card of that  \n",
      "type, they must give it to you.\n",
      "RESOURCE MONOPOLY\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply. Add them to your hand. They may be 2 of the same resource or 2 different resources.\n",
      "Overlap: \n",
      "0.006622516556291391\n",
      "Score: 0.7763168968838309\n",
      "\n",
      "Question: Can you use Year of Plenty to take two of the same resource?\n",
      "Retrieved 1: \n",
      "High Assembly\n",
      "You may trade any 2 identical commodities \n",
      "for any 1 other commodity or resource.\n",
      "If you receive no production, take 1 resource \n",
      "of your choice (except on a “7”).\n",
      "You may promote  strong knights (Level 2)  \n",
      "to mighty knights (Level 3).\n",
      "CN3087\n",
      "CITIES & KNIGHTS\n",
      "3 of 3\n",
      "v6.250401\n",
      "SCIENCE\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply. Add them to your hand. They may be 2 of the same resource or 2 different resources.\n",
      "Overlap: \n",
      "0.013245033112582781\n",
      "Score: 0.7661127233908583\n",
      "\n",
      "Question: Can you use Year of Plenty to take two of the same resource?\n",
      "Retrieved 2: \n",
      "High Assembly\n",
      "You may trade any 2 identical commodities \n",
      "for any 1 other commodity or resource.\n",
      "If you receive no production, take 1 resource \n",
      "of your choice (except on a “7”).\n",
      "You may promote  strong knights (Level 2)  \n",
      "to mighty knights (Level 3).\n",
      "CN3087\n",
      "CITIES & KNIGHTS\n",
      "3 of 3\n",
      "v6.250401\n",
      "SCIENCE\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply. Add them to your hand. They may be 2 of the same resource or 2 different resources.\n",
      "Overlap: \n",
      "0.013245033112582781\n",
      "Score: 0.7661127233908583\n",
      "\n",
      "Question: What does the Monopoly card do?\n",
      "Retrieved 0: \n",
      "how many cards you receive.\n",
      "Monopoly\n",
      "When you play this card, build 2 roads \n",
      "at no cost (i.e., without spending any \n",
      "resource cards).\n",
      "When you play a Knight card, you must \n",
      "“Activate the Robber” (see page 6).\n",
      "Reveal all your Victory Point cards, \n",
      "including any built this turn, if\n",
      "GT: \n",
      "When you play this card, announce one type of resource. Each player must give you all their resource cards of that type.\n",
      "Overlap: \n",
      "0.008333333333333333\n",
      "Score: 0.7860912989625378\n",
      "\n",
      "Question: What does the Monopoly card do?\n",
      "Retrieved 1: \n",
      "if they have it.\n",
      "TRADE MONOPOLY\n",
      "TRADE MONOPOL Y (2x)\n",
      "Announce one type of commodity. Each \n",
      "player must give you 1 commodity card  \n",
      "of that type if they have it.\n",
      "© 2025 CATAN GmbH\n",
      "INTRIGUE\n",
      "Take the “Displace a Knight” \n",
      "action without using your knight. \n",
      "The displaced knight must start\n",
      "GT: \n",
      "When you play this card, announce one type of resource. Each player must give you all their resource cards of that type.\n",
      "Overlap: \n",
      "0.008333333333333333\n",
      "Score: 0.7741062103857375\n",
      "\n",
      "Question: What does the Monopoly card do?\n",
      "Retrieved 2: \n",
      "a player has only 1 card of that  \n",
      "type, they must give it to you.\n",
      "RESOURCE MONOPOLY\n",
      "RESOURCE MONOPOL Y (4x)\n",
      "Announce one type of resource. Each player \n",
      "must give you 2 resource cards of that type \n",
      "if they have them. If a player only has one  \n",
      "of those resource cards, they must give it  \n",
      "to you.\n",
      "GT: \n",
      "When you play this card, announce one type of resource. Each player must give you all their resource cards of that type.\n",
      "Overlap: \n",
      "0.041666666666666664\n",
      "Score: 0.7738401373310881\n",
      "\n",
      "Question: Can you build multiple roads in one turn?\n",
      "Retrieved 0: \n",
      "Build 2 roads at no cost.\n",
      "ROAD BUILDING\n",
      "© 2025 CATAN GmbH\n",
      "Play any number \n",
      "of Victory Point cards \n",
      "(even on the turn you build \n",
      "them) to win the game.\n",
      "VICTORY POINT\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "Move the robber to a\n",
      "GT: \n",
      "During your turn, you may spend resources (aka \"build\") to place roads and buildings on the board, as well as to take development cards from the supply.\n",
      "Overlap: \n",
      "0.006578947368421052\n",
      "Score: 0.7677069279357829\n",
      "\n",
      "Question: Can you build multiple roads in one turn?\n",
      "Retrieved 1: \n",
      "Example: Blue may place a road on any of the \n",
      "edges with a check mark. They may not place \n",
      "a road on the edge with an “X” because they \n",
      "may not build on the other side of  \n",
      "Orange’s settlement.\n",
      "GT: \n",
      "During your turn, you may spend resources (aka \"build\") to place roads and buildings on the board, as well as to take development cards from the supply.\n",
      "Overlap: \n",
      "0.05263157894736842\n",
      "Score: 0.7665958105684902\n",
      "\n",
      "Question: Can you build multiple roads in one turn?\n",
      "Retrieved 2: \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "Starting with the last player and going in reverse order, each player places 1 city on an \n",
      "empty intersection of their choice and their second road on an empty adjacent edge.\n",
      "GT: \n",
      "During your turn, you may spend resources (aka \"build\") to place roads and buildings on the board, as well as to take development cards from the supply.\n",
      "Overlap: \n",
      "0.006578947368421052\n",
      "Score: 0.7652709243855502\n",
      "\n",
      "Question: Can you steal from a player with no resource cards?\n",
      "Retrieved 0: \n",
      "does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "GT: \n",
      "Steal 1 random resource card from a player who has a building on that hex.\n",
      "Overlap: \n",
      "0.5540540540540541\n",
      "Score: 0.8070155655211048\n",
      "\n",
      "Question: Can you steal from a player with no resource cards?\n",
      "Retrieved 1: \n",
      "choose one player to rob.\n",
      "Important: A hex with the robber does not \n",
      "produce resources when its number is rolled.\n",
      "Example: Orange rolls a 7. They move \n",
      "the robber to the pasture hex with a \n",
      "red settlement and a blue city. Orange \n",
      "chooses to steal a card from Blue and\n",
      "GT: \n",
      "Steal 1 random resource card from a player who has a building on that hex.\n",
      "Overlap: \n",
      "0.14864864864864866\n",
      "Score: 0.7966228481013188\n",
      "\n",
      "Question: Can you steal from a player with no resource cards?\n",
      "Retrieved 2: \n",
      "If you are stronger, receive 1 resource card of your choice \n",
      "from the supply.\n",
      "If you are tied with the pirate, nothing happens.\n",
      "RESOLVING A 7\n",
      "When a 7 is rolled, players with \n",
      "more than 7 resource cards lose half \n",
      "of them as normal. Then the player \n",
      "who rolled the 7 may steal\n",
      "GT: \n",
      "Steal 1 random resource card from a player who has a building on that hex.\n",
      "Overlap: \n",
      "0.05405405405405406\n",
      "Score: 0.7831786567024954\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 0: \n",
      "Reveal all your Victory Point cards, \n",
      "including any built this turn, if \n",
      "you can reach the number of VPs \n",
      "needed to win. Otherwise, you must \n",
      "keep Victory Point cards hidden in \n",
      "your player area.\n",
      "When you play this card, take any  \n",
      "2 resource cards from the supply. Add\n",
      "GT: \n",
      "Reveal all your Victory Point cards, including any built this turn, if you can reach the number of VPs needed to win.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8175892825818208\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 0: \n",
      "Reveal all your Victory Point cards, \n",
      "including any built this turn, if \n",
      "you can reach the number of VPs \n",
      "needed to win. Otherwise, you must \n",
      "keep Victory Point cards hidden in \n",
      "your player area.\n",
      "When you play this card, take any  \n",
      "2 resource cards from the supply. Add\n",
      "GT: \n",
      "Turn over any number of Victory Point cards, including ones built this turn, to show that you have reached 10 VPs.\n",
      "Overlap: \n",
      "0.10526315789473684\n",
      "Score: 0.8175892825818208\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 1: \n",
      "• Place Victory Point cards face up in your player area \n",
      "rather than discarding them (even if it is not your turn). \n",
      "They do not count toward your hand limit and may not \n",
      "be stolen with the Espionage card.\n",
      "• You may play a progress card during the same turn it \n",
      "is drawn.\n",
      "GT: \n",
      "Reveal all your Victory Point cards, including any built this turn, if you can reach the number of VPs needed to win.\n",
      "Overlap: \n",
      "0.008547008547008548\n",
      "Score: 0.8058542934040841\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 1: \n",
      "• Place Victory Point cards face up in your player area \n",
      "rather than discarding them (even if it is not your turn). \n",
      "They do not count toward your hand limit and may not \n",
      "be stolen with the Espionage card.\n",
      "• You may play a progress card during the same turn it \n",
      "is drawn.\n",
      "GT: \n",
      "Turn over any number of Victory Point cards, including ones built this turn, to show that you have reached 10 VPs.\n",
      "Overlap: \n",
      "0.008771929824561403\n",
      "Score: 0.8058542934040841\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 2: \n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "WINNING THE GAME\n",
      "If you have 10 or more VPs at any \n",
      "point during your turn, the game ends \n",
      "immediately and you are the winner! \n",
      "Turn over any number of Victory \n",
      "Point cards, including ones \n",
      "built this turn, to show that \n",
      "you have reached 10 VPs. \n",
      "Congratulations!\n",
      "GT: \n",
      "Reveal all your Victory Point cards, including any built this turn, if you can reach the number of VPs needed to win.\n",
      "Overlap: \n",
      "0.017094017094017096\n",
      "Score: 0.8042118728644632\n",
      "\n",
      "Question: Can you win by revealing multiple Victory Point cards mid-turn?\n",
      "Retrieved 2: \n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "WINNING THE GAME\n",
      "If you have 10 or more VPs at any \n",
      "point during your turn, the game ends \n",
      "immediately and you are the winner! \n",
      "Turn over any number of Victory \n",
      "Point cards, including ones \n",
      "built this turn, to show that \n",
      "you have reached 10 VPs. \n",
      "Congratulations!\n",
      "GT: \n",
      "Turn over any number of Victory Point cards, including ones built this turn, to show that you have reached 10 VPs.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8042118728644632\n",
      "\n",
      "Question: Can you trade after building?\n",
      "Retrieved 0: \n",
      "For all these actions, “buildings” refers to settlements and cities.\n",
      "TRADE\n",
      "Trading Commodities\n",
      "When trading with other players, you may trade any combination of \n",
      "resources and commodities. \n",
      "When trading with the supply, you may also trade resources for\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "0.017094017094017096\n",
      "Score: 0.7322722179313398\n",
      "\n",
      "Question: Can you trade after building?\n",
      "Retrieved 1: \n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "© 2025 CATAN GmbH\n",
      "ACTION PHASE\n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "TRADE\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.7278028607481233\n",
      "\n",
      "Question: Can you trade after building?\n",
      "Retrieved 2: \n",
      "Example: White wants 1 ore. On their turn, \n",
      "they may not trade 2 wheat for 1 ore because \n",
      "they do not have a building on the 2:1 wheat \n",
      "port. However, they may trade 3 wood for 1 ore \n",
      "because they have a settlement built on a 3:1 port.\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "0.008547008547008548\n",
      "Score: 0.726121812621842\n",
      "\n",
      "Question: Can you downgrade a city back to a settlement?\n",
      "Retrieved 0: \n",
      "pillage 1 or more cities. Reduce pillaged cities \n",
      "to settlements by replacing the city piece \n",
      "with a settlement. If a city with a city wall is \n",
      "pillaged, the city wall is also removed from the \n",
      "board. If you do not have any settlements in \n",
      "your supply, turn the city piece on its side and\n",
      "GT: \n",
      "Cities always replace settlements. To build a city, remove one of your settlements from the board, return it to your player area, and place your city where the settlement was located.\n",
      "Overlap: \n",
      "0.01639344262295082\n",
      "Score: 0.7625737368150955\n",
      "\n",
      "Question: Can you downgrade a city back to a settlement?\n",
      "Retrieved 1: \n",
      "board. If you do not have any settlements in \n",
      "your supply, turn the city piece on its side and \n",
      "treat it as a settlement. You must upgrade this \n",
      "settlement to a city before upgrading any other \n",
      "settlement.\n",
      "• The player(s) who contributed the lowest \n",
      "total strength of active knights has one\n",
      "GT: \n",
      "Cities always replace settlements. To build a city, remove one of your settlements from the board, return it to your player area, and place your city where the settlement was located.\n",
      "Overlap: \n",
      "0.00546448087431694\n",
      "Score: 0.7509496781426395\n",
      "\n",
      "Question: Can you downgrade a city back to a settlement?\n",
      "Retrieved 2: \n",
      "settlement.\n",
      "• The player(s) who contributed the lowest \n",
      "total strength of active knights has one \n",
      "of their cities pillaged. If that player is \n",
      "unable to pillage a city, then the player \n",
      "who contributed the next lowest total \n",
      "strength has one of their cities pillaged.\n",
      "GT: \n",
      "Cities always replace settlements. To build a city, remove one of your settlements from the board, return it to your player area, and place your city where the settlement was located.\n",
      "Overlap: \n",
      "0.01092896174863388\n",
      "Score: 0.7484692307406521\n",
      "\n",
      "Question: Can you use a harbor without building a settlement there?\n",
      "Retrieved 0: \n",
      "For this scenario, you may only build one continuous line of \n",
      "ships to the western islands. No branching is allowed. This \n",
      "line must begin at one of your coastal buildings on the \n",
      "eastern island. It must then lead to your beachhead marker.\n",
      "GT: \n",
      "If you have a building on a 3:1 port, you may put 3 of the same resource cards into the supply and take 1 card of a different resource from the supply.\n",
      "Overlap: \n",
      "0.006622516556291391\n",
      "Score: 0.7647567539071416\n",
      "\n",
      "Question: Can you use a harbor without building a settlement there?\n",
      "Retrieved 1: \n",
      "port (i.e., no coastal settlement or no valid edge for a port), set the port in front of you  \n",
      "until you have a valid option.\n",
      "Ports may be used immediately upon being placed.\n",
      "WINNING THE GAME\n",
      "If you have 13 or more VPs at any point during your turn, the game ends and you are the winner!\n",
      "GT: \n",
      "If you have a building on a 3:1 port, you may put 3 of the same resource cards into the supply and take 1 card of a different resource from the supply.\n",
      "Overlap: \n",
      "0.013245033112582781\n",
      "Score: 0.7482593183616436\n",
      "\n",
      "Question: Can you use a harbor without building a settlement there?\n",
      "Retrieved 2: \n",
      "If possible, place the port face up next to one of your coastal settlements. However, there \n",
      "must always be one edge between ports. If you do not have a valid option for placing the \n",
      "port (i.e., no coastal settlement or no valid edge for a port), set the port in front of you\n",
      "GT: \n",
      "If you have a building on a 3:1 port, you may put 3 of the same resource cards into the supply and take 1 card of a different resource from the supply.\n",
      "Overlap: \n",
      "0.006622516556291391\n",
      "Score: 0.74390833060247\n",
      "\n",
      "Question: Can you build a road through another player's settlement?\n",
      "Retrieved 0: \n",
      "Example: Blue may place a road on any of the \n",
      "edges with a check mark. They may not place \n",
      "a road on the edge with an “X” because they \n",
      "may not build on the other side of  \n",
      "Orange’s settlement.\n",
      "GT: \n",
      "You may not build a road starting on the other side of an opponent's building.\n",
      "Overlap: \n",
      "0.28205128205128205\n",
      "Score: 0.791479394223019\n",
      "\n",
      "Question: Can you build a road through another player's settlement?\n",
      "Retrieved 1: \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "Starting with the last player and going in reverse order, each player places 1 city on an \n",
      "empty intersection of their choice and their second road on an empty adjacent edge.\n",
      "GT: \n",
      "You may not build a road starting on the other side of an opponent's building.\n",
      "Overlap: \n",
      "0.01282051282051282\n",
      "Score: 0.7795114804869868\n",
      "\n",
      "Question: Can you build a road through another player's settlement?\n",
      "Retrieved 2: \n",
      "ROUND 1\n",
      "The first player places 1 settlement on an empty intersection of their choice and then \n",
      "places 1 road on an empty edge next to that settlement. The next player to the left does \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "GT: \n",
      "You may not build a road starting on the other side of an opponent's building.\n",
      "Overlap: \n",
      "0.02564102564102564\n",
      "Score: 0.7787065389007015\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 0: \n",
      "does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "GT: \n",
      "You must move the robber to a new hex.\n",
      "Overlap: \n",
      "0.13157894736842105\n",
      "Score: 0.819240870060993\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 0: \n",
      "does not move, and you may not steal a card from another player.\n",
      "Once the robber is placed on the board, it activates as usual. You must move \n",
      "it to a new hex and steal 1 random card from the hand (resource cards + \n",
      "commodity cards) of a player who has a building on that hex.\n",
      "GT: \n",
      "A hex with the robber does not produce resources when its number is rolled.\n",
      "Overlap: \n",
      "0.06666666666666667\n",
      "Score: 0.819240870060993\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 1: \n",
      "Note: The 2 land hexes next to the desert should not receive red number discs (6s and 8s).\n",
      "Finally, place the robber on one of the desert hexes.\n",
      "GT: \n",
      "You must move the robber to a new hex.\n",
      "Overlap: \n",
      "0.34210526315789475\n",
      "Score: 0.8086141744116988\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 1: \n",
      "Note: The 2 land hexes next to the desert should not receive red number discs (6s and 8s).\n",
      "Finally, place the robber on one of the desert hexes.\n",
      "GT: \n",
      "A hex with the robber does not produce resources when its number is rolled.\n",
      "Overlap: \n",
      "0.16\n",
      "Score: 0.8086141744116988\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 2: \n",
      "choose one player to rob.\n",
      "Important: A hex with the robber does not \n",
      "produce resources when its number is rolled.\n",
      "Example: Orange rolls a 7. They move \n",
      "the robber to the pasture hex with a \n",
      "red settlement and a blue city. Orange \n",
      "chooses to steal a card from Blue and\n",
      "GT: \n",
      "You must move the robber to a new hex.\n",
      "Overlap: \n",
      "0.02631578947368421\n",
      "Score: 0.8058433562968955\n",
      "\n",
      "Question: Can you move the robber onto a hex where you have a settlement?\n",
      "Retrieved 2: \n",
      "choose one player to rob.\n",
      "Important: A hex with the robber does not \n",
      "produce resources when its number is rolled.\n",
      "Example: Orange rolls a 7. They move \n",
      "the robber to the pasture hex with a \n",
      "red settlement and a blue city. Orange \n",
      "chooses to steal a card from Blue and\n",
      "GT: \n",
      "A hex with the robber does not produce resources when its number is rolled.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8058433562968955\n",
      "\n",
      "Question: Can you trade with another player during their turn?\n",
      "Retrieved 0: \n",
      "TRADE\n",
      "You may trade freely with other players and the supply to get the resources you need to build. During your turn, \n",
      "other players may only trade with you, not with each other or with the supply. \n",
      "There are three types of trades you may perform:\n",
      "TRADE WITH OTHER PLAYERS\n",
      "GT: \n",
      "During your turn, other players may only trade with you, not with each other or with the supply.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.7809345899003814\n",
      "\n",
      "Question: Can you trade with another player during their turn?\n",
      "Retrieved 1: \n",
      "1 wheat and wants 1 wool in return. Blue agrees and the players \n",
      "exchange cards. White also wants to trade for 1 wool, but Blue  \n",
      "no longer has any. White may not trade with Orange because  \n",
      "it is not Orange’s turn.\n",
      "GENERAL TRADE WITH THE SUPPL Y (4:1)\n",
      "GT: \n",
      "During your turn, other players may only trade with you, not with each other or with the supply.\n",
      "Overlap: \n",
      "0.15625\n",
      "Score: 0.740206151429273\n",
      "\n",
      "Question: Can you trade with another player during their turn?\n",
      "Retrieved 2: \n",
      "on the port into the supply and take  \n",
      "1 card of a different resource from  \n",
      "the supply.\n",
      "Example: It is Blue’s turn. They choose to trade 4 wood for 1 ore.\n",
      "Example: It is Blue’s turn, and they want 1 wheat. Orange will trade  \n",
      "1 wheat and wants 1 wool in return. Blue agrees and the players\n",
      "GT: \n",
      "During your turn, other players may only trade with you, not with each other or with the supply.\n",
      "Overlap: \n",
      "0.020833333333333332\n",
      "Score: 0.7391738486686956\n",
      "\n",
      "Question: Can you play multiple development cards on the same turn?\n",
      "Retrieved 0: \n",
      "PLAY DEVELOPMENT CARDS\n",
      "• Not on the turn you bought it\n",
      "• Only 1 development card per turn\n",
      "• Before rolling dice or during the Action phase\n",
      "VP card exception: You may play multiple VP cards (even on the \n",
      "turn you buy them) in order to win the game.\n",
      "CN3081\n",
      "CATAN\n",
      "3 of 5\n",
      "v6.250401\n",
      "VP0\n",
      "VP1\n",
      "VPs2\n",
      "VPs?\n",
      "VP0\n",
      "GT: \n",
      "You may play 1 development card during your turn by placing it face up in your player area.\n",
      "Overlap: \n",
      "0.02197802197802198\n",
      "Score: 0.8433156556925978\n",
      "\n",
      "Question: Can you play multiple development cards on the same turn?\n",
      "Retrieved 1: \n",
      "You may play 1 development card during your turn by placing it face up in your player area.  \n",
      "It may not be a card you built this turn. You may play a development card before rolling dice \n",
      "or at any time during the Action phase.\n",
      "GT: \n",
      "You may play 1 development card during your turn by placing it face up in your player area.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.8429654469916087\n",
      "\n",
      "Question: Can you play multiple development cards on the same turn?\n",
      "Retrieved 2: \n",
      "PLAY DEVELOPMENT CARDS\n",
      "• Not on the turn you bought it\n",
      "• Only 1 development card per turn\n",
      "• Before rolling dice or during the Action phase\n",
      "VP card exception: You may play multiple VP cards (even on the \n",
      "turn you buy them) in order to win the game.\n",
      "PLAYER AID\n",
      "Production Phase\n",
      "GT: \n",
      "You may play 1 development card during your turn by placing it face up in your player area.\n",
      "Overlap: \n",
      "0.23076923076923078\n",
      "Score: 0.8334381624224332\n",
      "\n",
      "Question: Can you build a settlement and upgrade it to a city in the same turn?\n",
      "Retrieved 0: \n",
      "board. If you do not have any settlements in \n",
      "your supply, turn the city piece on its side and \n",
      "treat it as a settlement. You must upgrade this \n",
      "settlement to a city before upgrading any other \n",
      "settlement.\n",
      "• The player(s) who contributed the lowest \n",
      "total strength of active knights has one\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "0.008547008547008548\n",
      "Score: 0.8026886646858551\n",
      "\n",
      "Question: Can you build a settlement and upgrade it to a city in the same turn?\n",
      "Retrieved 1: \n",
      "You may make any number of improvements on any track as \n",
      "long as you have at least 1 city built.\n",
      "Note: If you lose your last city (see “Barbarians Attack” on \n",
      "page 11), you do not lose your improvements. However, you \n",
      "may not make any new improvements until after you build \n",
      "a city.\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "0.008547008547008548\n",
      "Score: 0.7973068071589535\n",
      "\n",
      "Question: Can you build a settlement and upgrade it to a city in the same turn?\n",
      "Retrieved 2: \n",
      "settlement. The pirate fortress now counts as one of your settlements. It receives production resources and may be \n",
      "upgraded to a city as normal.\n",
      "Note: If all pirate fortresses are recaptured before the games ends, remove the pirate ship from the game.\n",
      "GT: \n",
      "You may take actions in this phase as often as you like and in any order, as long as you have the resources to do so.\n",
      "Overlap: \n",
      "0.02564102564102564\n",
      "Score: 0.777924272318915\n",
      "\n",
      "Question: Can you use a Knight card before rolling the dice?\n",
      "Retrieved 0: \n",
      "∙Take knight actions with activated knights:\n",
      " ▶Move knight(s), displace knight(s),  \n",
      "or chase away the robber\n",
      " ▶Knights always become inactive after taking  \n",
      "an action\n",
      "PLAYER AID\n",
      "TURN OVERVIEW\n",
      "Roll Dice Phase\n",
      " ∙Play Alchemy progress card (optional)\n",
      " ∙Roll dice\n",
      "GT: \n",
      "You may play a development card before rolling dice or at any time during the Action phase.\n",
      "Overlap: \n",
      "0.01098901098901099\n",
      "Score: 0.7884458537280232\n",
      "\n",
      "Question: Can you use a Knight card before rolling the dice?\n",
      "Retrieved 1: \n",
      "∙Take knight actions with activated knights:\n",
      " ▶Move knight(s), displace knight(s),  \n",
      "or chase away the robber\n",
      " ▶Knights always become inactive after taking  \n",
      "an action\n",
      "PLAYER AID\n",
      "TURN OVERVIEW\n",
      "Roll Dice Phase\n",
      " ∙Play Alchemy progress card (optional)\n",
      " ∙Roll dice\n",
      "GT: \n",
      "You may play a development card before rolling dice or at any time during the Action phase.\n",
      "Overlap: \n",
      "0.01098901098901099\n",
      "Score: 0.7884458537280232\n",
      "\n",
      "Question: Can you use a Knight card before rolling the dice?\n",
      "Retrieved 2: \n",
      "∙Recruit, promote, and activate knights\n",
      " ∙Take knight actions with activated knights:\n",
      " ▶Move knight(s), displace knight(s),  \n",
      "or chase away the robber\n",
      " ▶Knights always become inactive after taking  \n",
      "an action\n",
      "PLAYER AID\n",
      "TURN OVERVIEW\n",
      "Roll Dice Phase\n",
      " ∙Play Alchemy progress card (optional)\n",
      "GT: \n",
      "You may play a development card before rolling dice or at any time during the Action phase.\n",
      "Overlap: \n",
      "0.01098901098901099\n",
      "Score: 0.7858538436180275\n",
      "\n",
      "Question: What happens if two players tie for Longest Road?\n",
      "Retrieved 0: \n",
      "The first player to play 5 continuous roads receives this tile. If another player plays more, they immediately receive this tile. \n",
      "3:1\n",
      "2:1\n",
      "2:1\n",
      "6\n",
      "5\n",
      "5\n",
      "4\n",
      "CN3081CATAN5 of 5\n",
      "v6.250401\n",
      "LARGEST ARMY\n",
      "LONGEST ROUTE\n",
      "GT: \n",
      "If another player has more continuous roads in play, they immediately receive this tile.\n",
      "Overlap: \n",
      "0.011363636363636364\n",
      "Score: 0.7931894604545\n",
      "\n",
      "Question: What happens if two players tie for Longest Road?\n",
      "Retrieved 1: \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "Starting with the last player and going in reverse order, each player places 1 city on an \n",
      "empty intersection of their choice and their second road on an empty adjacent edge.\n",
      "GT: \n",
      "If another player has more continuous roads in play, they immediately receive this tile.\n",
      "Overlap: \n",
      "0.022727272727272728\n",
      "Score: 0.7888413808278737\n",
      "\n",
      "Question: What happens if two players tie for Longest Road?\n",
      "Retrieved 2: \n",
      "tile. If another player has more continuous roads in play, they \n",
      "immediately receive this tile. The Longest Route tile is worth \n",
      "2 VPs.\n",
      "If, when a player’s route is broken (see example), they no \n",
      "longer meet the requirements for the Longest Route, the tile\n",
      "GT: \n",
      "If another player has more continuous roads in play, they immediately receive this tile.\n",
      "Overlap: \n",
      "1.0\n",
      "Score: 0.7852991700181998\n",
      "\n",
      "Question: How are roads placed?\n",
      "Retrieved 0: \n",
      "Example: Blue may place a road on any of the \n",
      "edges with a check mark. They may not place \n",
      "a road on the edge with an “X” because they \n",
      "may not build on the other side of  \n",
      "Orange’s settlement.\n",
      "GT: \n",
      "Roads are placed on empty hex edges. A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.057692307692307696\n",
      "Score: 0.7684487546164231\n",
      "\n",
      "Question: How are roads placed?\n",
      "Retrieved 1: \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "Starting with the last player and going in reverse order, each player places 1 city on an \n",
      "empty intersection of their choice and their second road on an empty adjacent edge.\n",
      "GT: \n",
      "Roads are placed on empty hex edges. A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.019230769230769232\n",
      "Score: 0.7479883562725671\n",
      "\n",
      "Question: How are roads placed?\n",
      "Retrieved 2: \n",
      "ROUND 1\n",
      "The first player places 1 settlement on an empty intersection of their choice and then \n",
      "places 1 road on an empty edge next to that settlement. The next player to the left does \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "GT: \n",
      "Roads are placed on empty hex edges. A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.019230769230769232\n",
      "Score: 0.7460216578805032\n",
      "\n",
      "Question: Can a player build disconnected roads?\n",
      "Retrieved 0: \n",
      "Example: Blue may place a road on any of the \n",
      "edges with a check mark. They may not place \n",
      "a road on the edge with an “X” because they \n",
      "may not build on the other side of  \n",
      "Orange’s settlement.\n",
      "GT: \n",
      "A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.08955223880597014\n",
      "Score: 0.7515594166000465\n",
      "\n",
      "Question: Can a player build disconnected roads?\n",
      "Retrieved 1: \n",
      "the same until all players have 1 settlement and 1 road on the board.\n",
      "ROUND 2\n",
      "Starting with the last player and going in reverse order, each player places 1 city on an \n",
      "empty intersection of their choice and their second road on an empty adjacent edge.\n",
      "GT: \n",
      "A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.014925373134328358\n",
      "Score: 0.746162870896439\n",
      "\n",
      "Question: Can a player build disconnected roads?\n",
      "Retrieved 2: \n",
      "Remove an “open” road. If you remove another player’s road, return it to their supply. If you \n",
      "remove your own road, you may immediately build 1 road at no cost.\n",
      "A road is “open” if one of its ends is not next to one of your roads or buildings and if it is not part\n",
      "GT: \n",
      "A new road must connect to one of your existing roads or buildings.\n",
      "Overlap: \n",
      "0.014925373134328358\n",
      "Score: 0.7392568358134137\n",
      "\n",
      "Question: What does a Year of Plenty card do?\n",
      "Retrieved 0: \n",
      "Playing an irrigation card gives Orange  \n",
      "4 wheat cards (2 for each field hex).\n",
      "PROGRESS CARDS\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply.\n",
      "Overlap: \n",
      "0.1044776119402985\n",
      "Score: 0.694248469169721\n",
      "\n",
      "Question: What does a Year of Plenty card do?\n",
      "Retrieved 1: \n",
      "PRODUCTION PHASE\n",
      "GOLD FIELDS\n",
      "Each player with a settlement on a gold field hex that produces this turn receives 1 resource card of \n",
      "their choice (brick, wood, wool, wheat, or ore). Similarly, a player receives 2 resource cards  \n",
      "in any combination for each of their cities on that hex.\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply.\n",
      "Overlap: \n",
      "0.014925373134328358\n",
      "Score: 0.6881682594364181\n",
      "\n",
      "Question: What does a Year of Plenty card do?\n",
      "Retrieved 2: \n",
      "one receives any of that resource. However, if only \n",
      "one player is affected, give that player as many of \n",
      "those resource cards as remain in the supply.\n",
      "TURN OVERVIEW\n",
      "CATAN is played over a series of turns, starting with the first player, and moving clockwise around the table.\n",
      "GT: \n",
      "When you play this card, take any 2 resource cards from the supply.\n",
      "Overlap: \n",
      "0.014925373134328358\n",
      "Score: 0.6866081516919389\n",
      "\n",
      "Question: What is needed to upgrade a settlement to a city?\n",
      "Retrieved 0: \n",
      "board. If you do not have any settlements in \n",
      "your supply, turn the city piece on its side and \n",
      "treat it as a settlement. You must upgrade this \n",
      "settlement to a city before upgrading any other \n",
      "settlement.\n",
      "• The player(s) who contributed the lowest \n",
      "total strength of active knights has one\n",
      "GT: \n",
      "Cities cost 3 ore and 2 grain.\n",
      "Overlap: \n",
      "0.03333333333333333\n",
      "Score: 0.7938437902776316\n",
      "\n",
      "Question: What is needed to upgrade a settlement to a city?\n",
      "Retrieved 1: \n",
      "City Improvements\n",
      "City improvements give you access to progress cards \n",
      "in three disciplines–science, trade, and politics. You must \n",
      "have at least 1 city on the board to make city improvements. \n",
      "You may make any number of improvements on any track as \n",
      "long as you have at least 1 city built.\n",
      "GT: \n",
      "Cities cost 3 ore and 2 grain.\n",
      "Overlap: \n",
      "0.06666666666666667\n",
      "Score: 0.7851453550945593\n",
      "\n",
      "Question: What is needed to upgrade a settlement to a city?\n",
      "Retrieved 2: \n",
      "SETTLEMENTS\n",
      "Settlements are worth 1 VP.\n",
      "Settlements are placed on \n",
      "empty intersections. A new \n",
      "settlement must follow the Distance \n",
      "Rule and must connect to at least one \n",
      "of your existing roads. You have  \n",
      "5 settlement pieces. To continue building \n",
      "more settlements, you must first upgrade\n",
      "GT: \n",
      "Cities cost 3 ore and 2 grain.\n",
      "Overlap: \n",
      "0.03333333333333333\n",
      "Score: 0.7747441029379498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "# load_dotenv()\n",
    "\n",
    "# # Set path for where to get the original file and where to safe the chunks\n",
    "# CHROMA_PATH = \"chroma\"\n",
    "# DATA_PATH = \"data/BoardGamesRuleBook\"    # LLMs paper    \n",
    "qa_file = \"data/BoardGamesRuleBook/CATAN_train.json\"\n",
    "\n",
    "# chunks = split_text(documents, 300, 50)\n",
    "# # db = save_to_chroma(chunks, CHROMA_PATH, \"text-embedding-3-small\")\n",
    "# db = save_to_chroma(chunks, CHROMA_PATH, \"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "path = qa_file\n",
    "chroma = db\n",
    "k = 3\n",
    "overlap_threshold: float = 0.8,\n",
    "\n",
    "evaluation_rows = []\n",
    "\n",
    "# Read JSON Q&A file\n",
    "qa_rows = read_qa_file(path)\n",
    "\n",
    "for row in qa_rows:\n",
    "    question = row[\"question\"]\n",
    "    ground_truth = row[\"reference\"]\n",
    "    ground_truth_relevant_texts = row[\"relevant_chunk_texts\"]\n",
    "\n",
    "    # Retrieve context from database (with current chunking config)\n",
    "    context_results = chroma.similarity_search_with_relevance_scores(question, k=k)\n",
    "    retrieved_texts = [doc.page_content for doc, _score in context_results]\n",
    "    retrieved_scores = [score for _doc, score in context_results]\n",
    "    # TEXT-BASED MATCHING: Check which retrieved chunks overlap with ground truth\n",
    "    relevant_retrieved_indices = []\n",
    "    for i, retrieved_text in enumerate(retrieved_texts):\n",
    "        for gt_text in ground_truth_relevant_texts:\n",
    "            overlap = text_overlap_ratio(retrieved_text, gt_text)\n",
    "\n",
    "            print(f\"Question: {question}\\nRetrieved {i}: \\n{retrieved_text}\\nGT: \\n{gt_text}\\nOverlap: \\n{overlap}\\nScore: {retrieved_scores[i]}\\n\")\n",
    "            # if overlap >= overlap_threshold:\n",
    "            #     relevant_retrieved_indices.append(i)\n",
    "            #     break  # This retrieved chunk matches ground truth, move to next\n",
    "\n",
    "    # # Use indices as pseudo-IDs for metric computation\n",
    "    # retrieved_chunk_ids = list(range(len(retrieved_texts)))\n",
    "    # relevant_chunk_ids = relevant_retrieved_indices\n",
    "\n",
    "    # # Generate answer using LLM\n",
    "    # answer = generate_answer(question, context_results)\n",
    "\n",
    "    # evaluation_rows.append({\n",
    "    #     \"question\": question,\n",
    "    #     \"contexts\": retrieved_texts,\n",
    "    #     \"retrieved_chunk_ids\": retrieved_chunk_ids,  # [0, 1, 2, ...]\n",
    "    #     \"relevant_chunk_ids\": relevant_chunk_ids,    # [0, 2] if chunks 0 and 2 match GT\n",
    "    #     \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "    #     \"ground_truth\": ground_truth,\n",
    "    #     \"source\": row.get(\"board_game\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91211a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_in_eval_format(\n",
    "    path: str, \n",
    "    chroma, \n",
    "    k: int,\n",
    "    overlap_threshold: float = 0.5,\n",
    "    overlap_method: str = \"word\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generates answers in evaluation format compatible with RAGas and Hugging Face metrics.\n",
    "    Returns a list of dicts containing:\n",
    "        - question\n",
    "        - contexts (retrieved text)\n",
    "        - retrieved_chunk_ids (all retrieved)\n",
    "        - relevant_chunk_ids (ground truth relevant)\n",
    "        - answer (generated by LLM)\n",
    "        - reference (ground truth)\n",
    "        - source (board game / paper)\n",
    "    \"\"\"\n",
    "    evaluation_rows = []\n",
    "\n",
    "    # Read JSON Q&A file\n",
    "    qa_rows = read_qa_file(path)\n",
    "\n",
    "    for row in qa_rows:\n",
    "        question = row[\"question\"]\n",
    "        ground_truth = row[\"reference\"]\n",
    "        ground_truth_relevant_texts = row[\"relevant_chunk_texts\"]\n",
    "\n",
    "        # Retrieve context from database (with current chunking config)\n",
    "        context_results = chroma.similarity_search_with_relevance_scores(question, k=k)\n",
    "        retrieved_texts = [doc.page_content for doc, _score in context_results]\n",
    "\n",
    "        # TEXT-BASED MATCHING: Check which retrieved chunks overlap with ground truth\n",
    "        relevant_retrieved_indices = []\n",
    "        for i, retrieved_text in enumerate(retrieved_texts):\n",
    "            for gt_text in ground_truth_relevant_texts:\n",
    "                overlap = text_overlap_ratio(retrieved_text, gt_text, method=overlap_method)\n",
    "                if overlap >= overlap_threshold:\n",
    "                    relevant_retrieved_indices.append(i)\n",
    "                    break  # This retrieved chunk matches ground truth, move to next\n",
    "\n",
    "        # Use indices as pseudo-IDs for metric computation\n",
    "        retrieved_chunk_ids = list(range(len(retrieved_texts)))\n",
    "        relevant_chunk_ids = relevant_retrieved_indices\n",
    "\n",
    "        # Generate answer using LLM\n",
    "        answer = generate_answer(question, context_results)\n",
    "\n",
    "        evaluation_rows.append({\n",
    "            \"question\": question,\n",
    "            \"contexts\": retrieved_texts,\n",
    "            \"retrieved_chunk_ids\": retrieved_chunk_ids,  # [0, 1, 2, ...]\n",
    "            \"relevant_chunk_ids\": relevant_chunk_ids,    # [0, 2] if chunks 0 and 2 match GT\n",
    "            \"answer\": answer.content if hasattr(answer, 'content') else str(answer),\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"source\": row.get(\"board_game\", \"unknown\")\n",
    "        })\n",
    "\n",
    "    return evaluation_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cc0b3",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf25307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate answer in evaluation format\n",
    "# qa_file = \"data/papers/Beginner Q&A.txt\"\n",
    "qa_file = \"data/BoardGamesRuleBook/boardgame_retrieval_dataset.json\"\n",
    "evaluation_rows = generate_answer_in_eval_format(path=qa_file, chroma=db, k=2)\n",
    "\n",
    "# change it to Dataset format\n",
    "evaluation_dataset = Dataset.from_list(evaluation_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "# --- Retrieval Evaluation ---\n",
    "def run_retrieval_evaluation(\n",
    "    documents: List[str],\n",
    "    qa_file: str,\n",
    "    chunk_sizes: List[int],\n",
    "    chunk_overlaps: List[int],\n",
    "    embedding_models: List[str],\n",
    "    top_k_list: List[int],\n",
    "    chroma_path: str = \"chroma\"\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs Hugging Face retrieval evaluation (Precision@K, Recall@K, MRR, NDCG@K, Hit Rate@K).\n",
    "    \n",
    "    Parameters:\n",
    "        documents: List of raw document strings to split and embed.\n",
    "        qa_file: Path to JSON Q&A file for evaluation.\n",
    "        chunk_sizes: List of chunk sizes to test.\n",
    "        chunk_overlaps: List of overlap sizes to test.\n",
    "        embedding_models: List of embedding models to test.\n",
    "        top_k_list: List of top_k values to test for retrieval.\n",
    "        chroma_path: Directory to save Chroma embeddings.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with retrieval metrics for all tested configurations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize HF metrics\n",
    "    \n",
    "    hf_metrics = {\n",
    "        \"precision\": evaluate.load(\"precision\", module_type=\"metric\"),\n",
    "        \"recall\": evaluate.load(\"recall\", module_type=\"metric\"),\n",
    "        # \"mrr\": evaluate.load(\"mrr\", module_type=\"metric\"),\n",
    "        # NDCG is provided via \"ndcg\" metric if using module_type=\"metric\" with BEIR installed\n",
    "        # \"ndcg\": evaluate.load(\"ndcg\", module_type=\"metric\"),  \n",
    "        # Hit rate is not directly available, we can use recall@k as a proxy or compute manually\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for chunk_size, overlap, emb_model, top_k in itertools.product(\n",
    "        chunk_sizes, chunk_overlaps, embedding_models, top_k_list\n",
    "    ):\n",
    "        print(f\"\\n🔧 Retrieval config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "        chunks = split_text(documents, chunk_size, overlap)\n",
    "        db = save_to_chroma(chunks, chroma_path, emb_model)\n",
    "        evaluation_rows = generate_answer_in_eval_format(\n",
    "                qa_file, db, top_k\n",
    "            )\n",
    "\n",
    "        predictions = [row[\"retrieved_chunk_ids\"] for row in evaluation_rows]\n",
    "        references = [row[\"relevant_chunk_ids\"] for row in evaluation_rows]\n",
    "\n",
    "\n",
    "        hf_scores = {name: metric.compute(predictions=predictions, references=references, k=top_k)\n",
    "                     for name, metric in hf_metrics.items()}\n",
    "\n",
    "        results.append({\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"chunk_overlap\": overlap,\n",
    "            \"embedding_model\": emb_model,\n",
    "            \"top_k\": top_k,\n",
    "            \"precision_at_k\": hf_scores[\"precision\"][\"precision\"],\n",
    "            \"recall_at_k\": hf_scores[\"recall\"][\"recall\"],\n",
    "            \"mrr_at_k\": hf_scores[\"mrr\"][\"mrr\"],\n",
    "            \"ndcg_at_k\": hf_scores[\"ndcg\"][\"ndcg\"],\n",
    "            \"hit_rate_at_k\": hf_scores[\"hit_rate\"][\"hit_rate\"],\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"retrieval_eval_results.csv\", index=False)\n",
    "    print(\"\\n✅ Retrieval evaluation complete!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# # --- Generation Evaluation ---\n",
    "# def run_generation_evaluation(documents: List[str], qa_file: str, chunk_size: int, overlap: int, \n",
    "#                               embedding_model: str, top_k: int, chroma_path: str = \"chroma\") -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Runs RAGas generation evaluation (answer correctness, relevancy, faithfulness, context precision/recall).\n",
    "    \n",
    "#     Parameters:\n",
    "#         documents: List of raw document strings to split and embed.\n",
    "#         qa_file: Path to JSON Q&A file for evaluation.\n",
    "#         chunk_size: Chunk size for splitting documents.\n",
    "#         overlap: Overlap size for splitting documents.\n",
    "#         embedding_model: Embedding model name.\n",
    "#         top_k: Number of top chunks to retrieve for LLM context.\n",
    "#         chroma_path: Directory to save Chroma embeddings.\n",
    "        \n",
    "#     Returns:\n",
    "#         DataFrame with RAGas generation metrics.\n",
    "#     \"\"\"\n",
    "#     from ragas import evaluate\n",
    "#     from ragas.metrics import (\n",
    "#         answer_correctness,\n",
    "#         answer_relevancy,\n",
    "#         faithfulness,\n",
    "#         context_precision,\n",
    "#         context_recall,\n",
    "#     )\n",
    "#     from openai import ChatOpenAI\n",
    "\n",
    "#     chunks = split_text(documents, chunk_size, overlap)\n",
    "#     db = save_to_chroma(chunks, chroma_path, embedding_model)\n",
    "#     evaluation_rows = generate_answer_in_eval_format(qa_file, db, top_k)\n",
    "#     evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "#     llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "#     # --- Run RAGas evaluation ---\n",
    "#     scores = evaluate(\n",
    "#         evaluation_dataset,\n",
    "#         metrics=[\n",
    "#             answer_correctness,\n",
    "#             answer_relevancy,\n",
    "#             faithfulness,\n",
    "#             context_precision,\n",
    "#             context_recall,\n",
    "#         ],\n",
    "#         llm=llm,\n",
    "#     )\n",
    "\n",
    "#     # Save results\n",
    "#     df = pd.DataFrame([{\n",
    "#         \"chunk_size\": chunk_size,\n",
    "#         \"chunk_overlap\": overlap,\n",
    "#         \"embedding_model\": embedding_model,\n",
    "#         \"top_k\": top_k,\n",
    "#         \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "#         \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "#         \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "#         \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "#         \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "#     }])\n",
    "    \n",
    "#     df.to_csv(\"generation_eval_results.csv\", index=False)\n",
    "#     print(\"\\n✅ Generation evaluation complete!\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93fe383a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Retrieval config: chunk=300, overlap=50, model=text-embedding-3-small, top_k=2\n",
      "Split 48 pages into 510 chunks.\n",
      "\n",
      "Saved 510 chunks to chroma.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value('int32'), 'references': Value('int32')},\nInput predictions: [[0, 1], [0, 1], [0, 1], ..., [0, 1], [0, 1], [0, 1]],\nInput references: [[], [], [], ..., [], [], []]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m qa_file = \u001b[33m\"\u001b[39m\u001b[33mdata/BoardGamesRuleBook/CATAN_train.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m retrieval_df = \u001b[43mrun_retrieval_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqa_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqa_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_overlaps\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-3-small\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_text_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_to_chroma_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_to_chroma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerate_answer_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerate_answer\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m display(retrieval_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mrun_retrieval_evaluation\u001b[39m\u001b[34m(documents, qa_file, chunk_sizes, chunk_overlaps, embedding_models, top_k_list, split_text_func, save_to_chroma_func, generate_answer_func, chroma_path)\u001b[39m\n\u001b[32m     60\u001b[39m     predictions = [row[\u001b[33m\"\u001b[39m\u001b[33mretrieved_chunk_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m evaluation_rows]\n\u001b[32m     61\u001b[39m     references = [row[\u001b[33m\"\u001b[39m\u001b[33mrelevant_chunk_ids\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m evaluation_rows]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     hf_scores = {name: \u001b[43mmetric\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m                  \u001b[38;5;28;01mfor\u001b[39;00m name, metric \u001b[38;5;129;01min\u001b[39;00m hf_metrics.items()}\n\u001b[32m     67\u001b[39m     results.append({\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_size\u001b[39m\u001b[33m\"\u001b[39m: chunk_size,\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_overlap\u001b[39m\u001b[33m\"\u001b[39m: overlap,\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhit_rate_at_k\u001b[39m\u001b[33m\"\u001b[39m: hf_scores[\u001b[33m\"\u001b[39m\u001b[33mhit_rate\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mhit_rate\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     77\u001b[39m     })\n\u001b[32m     79\u001b[39m df = pd.DataFrame(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\evaluate\\module.py:455\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    452\u001b[39m compute_kwargs = {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs.values()):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize()\n\u001b[32m    458\u001b[39m \u001b[38;5;28mself\u001b[39m.cache_file_name = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Program\\GitHubProjects\\Do-my-history-exam\\test3.venv\\Lib\\site-packages\\evaluate\\module.py:546\u001b[39m, in \u001b[36mEvaluationModule.add_batch\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    540\u001b[39m     error_msg = (\n\u001b[32m    541\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredictions and/or references don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    542\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.selected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    543\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    544\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    545\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value('int32'), 'references': Value('int32')},\nInput predictions: [[0, 1], [0, 1], [0, 1], ..., [0, 1], [0, 1], [0, 1]],\nInput references: [[], [], [], ..., [], [], []]"
     ]
    }
   ],
   "source": [
    "    \n",
    "qa_file = \"data/BoardGamesRuleBook/CATAN_train.json\"\n",
    "\n",
    "retrieval_df = run_retrieval_evaluation(\n",
    "    documents=documents,\n",
    "    qa_file=qa_file,\n",
    "    chunk_sizes=[300, 500],\n",
    "    chunk_overlaps=[50, 100],\n",
    "    embedding_models=[\"text-embedding-3-small\"],\n",
    "    top_k_list=[2, 5, 10],\n",
    "    split_text_func=split_text,\n",
    "    save_to_chroma_func=save_to_chroma,\n",
    "    generate_answer_func=generate_answer\n",
    ")\n",
    "\n",
    "display(retrieval_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49982d1e",
   "metadata": {},
   "source": [
    "**Why RAGAS?** <br>\n",
    "Traditional NLP metrics like `BLEU` or `ROUGE` measure surface-level text similarity but ignore whether answers are grounded in evidence—critical for `RAG` systems where factual accuracy and traceability matter. `RAGAS` provides task-aware, automated evaluation that directly **measures how generated content connects to retrieved context**. Its modular design integrates seamlessly with `LangChain` and `Weights & Biases`, enabling scalable experimentation and real-time tracking. Essentially, `RAGAS` lets us **evaluate retrieval-generation alignment** systematically, leading to more interpretable and reliable `RAG` performance insights without constant human annotation.\n",
    "\n",
    "**The limitations?** <br>\n",
    "`RAGAS` relies on **LLM-as-judge evaluation**, meaning it **uses language models (often GPT-4) to score metrics** like `faithfulness` and `relevancy`. This introduces **cost** (API calls for every evaluation), **latency** (slower than traditional metrics), and **potential bias** (the judge model's own limitations affect scores). Additionally, `RAGAS` metrics **require ground truth datasets** for answer correctness, which aren't always available or easy to create for domain-specific applications. The framework also has a **learning curve**—understanding what each metric measures and how to interpret scores together requires familiarity with `RAG`-specific evaluation concepts. Despite these trade-offs, for most `RAG` applications, `RAGAS`'s ability to measure grounding and factuality outweighs the operational overhead compared to purely manual evaluation or metrics that ignore retrieval quality entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcd22998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/385 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   0%|          | 1/385 [00:03<24:28,  3.83s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 2/385 [00:08<26:02,  4.08s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   1%|          | 3/385 [00:26<1:07:40, 10.63s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   5%|▌         | 20/385 [01:10<18:59,  3.12s/it] LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   5%|▌         | 21/385 [01:24<32:32,  5.36s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   6%|▋         | 25/385 [01:31<20:20,  3.39s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   8%|▊         | 31/385 [01:46<16:39,  2.82s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  10%|█         | 39/385 [02:04<14:31,  2.52s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  11%|█▏        | 44/385 [02:17<14:45,  2.60s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  12%|█▏        | 45/385 [02:25<19:52,  3.51s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  14%|█▎        | 52/385 [02:45<15:57,  2.88s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  14%|█▍        | 55/385 [02:52<13:33,  2.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  16%|█▌        | 61/385 [03:14<19:21,  3.58s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  19%|█▉        | 75/385 [03:47<09:38,  1.87s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  21%|██▏       | 82/385 [04:09<11:17,  2.23s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  22%|██▏       | 85/385 [04:19<12:52,  2.58s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  23%|██▎       | 87/385 [04:24<12:13,  2.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  23%|██▎       | 90/385 [04:32<11:26,  2.33s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  25%|██▍       | 96/385 [04:50<11:38,  2.42s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  27%|██▋       | 103/385 [05:12<10:00,  2.13s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  29%|██▊       | 110/385 [05:34<12:31,  2.73s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  29%|██▉       | 113/385 [05:48<15:41,  3.46s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  30%|██▉       | 114/385 [05:53<17:56,  3.97s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|███▏      | 122/385 [06:12<10:27,  2.38s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  34%|███▎      | 129/385 [06:31<10:54,  2.56s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  34%|███▍      | 132/385 [06:42<12:37,  2.99s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  35%|███▌      | 135/385 [06:48<11:14,  2.70s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  38%|███▊      | 146/385 [07:20<10:18,  2.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  38%|███▊      | 147/385 [07:29<16:23,  4.13s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|████      | 154/385 [07:46<10:37,  2.76s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|████▎     | 164/385 [08:12<07:08,  1.94s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  43%|████▎     | 165/385 [08:15<08:26,  2.30s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  45%|████▍     | 172/385 [08:35<07:37,  2.15s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  46%|████▌     | 177/385 [08:52<10:03,  2.90s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  47%|████▋     | 181/385 [09:07<10:34,  3.11s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  48%|████▊     | 184/385 [09:16<10:52,  3.25s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  49%|████▉     | 190/385 [09:35<09:52,  3.04s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  52%|█████▏    | 200/385 [09:59<05:49,  1.89s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  57%|█████▋    | 219/385 [10:59<05:58,  2.16s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  58%|█████▊    | 222/385 [11:05<05:15,  1.93s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  59%|█████▉    | 228/385 [11:25<06:14,  2.39s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  63%|██████▎   | 244/385 [12:17<07:44,  3.29s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  66%|██████▌   | 255/385 [12:48<05:46,  2.66s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  70%|██████▉   | 268/385 [13:25<05:29,  2.81s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  73%|███████▎  | 281/385 [13:56<03:24,  1.97s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  75%|███████▍  | 287/385 [14:21<05:17,  3.24s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  76%|███████▋  | 294/385 [14:36<03:28,  2.29s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  79%|███████▉  | 305/385 [15:08<02:39,  2.00s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  79%|███████▉  | 306/385 [15:25<05:58,  4.54s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  85%|████████▍ | 326/385 [16:17<03:07,  3.18s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  88%|████████▊ | 337/385 [16:46<02:19,  2.91s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  90%|████████▉ | 346/385 [17:22<03:33,  5.49s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  94%|█████████▎| 360/385 [18:05<01:32,  3.72s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  95%|█████████▌| 366/385 [18:14<00:38,  2.05s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  98%|█████████▊| 379/385 [18:48<00:15,  2.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 385/385 [19:02<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset preview:\n",
      "1. Q: What is needed to buy a development card?\n",
      "   Answer: To buy a development card, you need to build or move one of your ships to an edge with a development card.\n",
      "   Reference: 1 wool, 1 grain, and 1 ore.\n",
      "   Contexts: ['DEVELOPMENT CARDS\\nWhen you build or move one of your ships to an edge with a development card, \\ntake the card. Use it as if you bought it this turn (i.e., you may play it on a future \\nturn or immediately reveal it to win the game).\\nPORTS', 'You may play 1 development card during your turn by placing it face up in your player area.  \\nIt may not be a card you built this turn. You may play a development card before rolling dice \\nor at any time during the Action phase.']\n",
      "\n",
      "2. Q: Can you play multiple development cards on the same turn?\n",
      "   Answer: No, you can only play one development card per turn according to the context provided.\n",
      "   Reference: No, only one development card per turn (except Victory Points).\n",
      "   Contexts: ['PLAY DEVELOPMENT CARDS\\n• Not on the turn you bought it\\n• Only 1 development card per turn\\n• Before rolling dice or during the Action phase\\nVP card exception: You may play multiple VP cards (even on the \\nturn you buy them) in order to win the game.\\nCN3081\\nCATAN\\n3 of 5\\nv6.250401\\nVP0\\nVP1\\nVPs2\\nVPs?\\nVP0', 'You may play 1 development card during your turn by placing it face up in your player area.  \\nIt may not be a card you built this turn. You may play a development card before rolling dice \\nor at any time during the Action phase.']\n",
      "\n",
      "3. Q: How are roads placed?\n",
      "   Answer: Players place roads on edges with a check mark, avoiding edges where they may not build due to other players' settlements.\n",
      "   Reference: Each road must connect to one of your existing roads, settlements, or cities.\n",
      "   Contexts: ['Example: Blue may place a road on any of the \\nedges with a check mark. They may not place \\na road on the edge with an “X” because they \\nmay not build on the other side of  \\nOrange’s settlement.', 'the same until all players have 1 settlement and 1 road on the board.\\nROUND 2\\nStarting with the last player and going in reverse order, each player places 1 city on an \\nempty intersection of their choice and their second road on an empty adjacent edge.']\n",
      "\n",
      "4. Q: Can a player refuse all trade offers?\n",
      "   Answer: Yes, a player can refuse all trade offers if they do not find any proposed trades beneficial to them.\n",
      "   Reference: Yes, all trades between players are voluntary.\n",
      "   Contexts: ['to trade. Other players may accept your proposal, make counteroffers, or make their own proposals.\\nImportant: You may not give away cards in any way, which includes trading matching resource cards \\n(for example, trying to trade 3 ore for 1 ore is not allowed). \\nPORT TRADE WITH THE SUPPL Y', 'There are three types of trades you may perform:\\nTRADE WITH OTHER PLAYERS\\nTo trade with other players, announce which resource(s) you want and which resource(s) you are willing \\nto trade. Other players may accept your proposal, make counteroffers, or make their own proposals.']\n",
      "\n",
      "5. Q: What happens when a player rolls a 7?\n",
      "   Answer: When a player rolls a 7, players with more than 7 resource cards lose half of them as normal, and then the player who rolled the 7 may steal from other players.\n",
      "   Reference: No resources are produced, and the robber is moved to a new hex.\n",
      "   Contexts: ['If you are stronger, receive 1 resource card of your choice \\nfrom the supply.\\nIf you are tied with the pirate, nothing happens.\\nRESOLVING A 7\\nWhen a 7 is rolled, players with \\nmore than 7 resource cards lose half \\nof them as normal. Then the player \\nwho rolled the 7 may steal', 'in your hand.\\n6   Choose the First Player \\nEach player rolls the dice. The player with the \\nhighest roll is the first player.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# Initialize the LLM for evaluation\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # sync interface compatible with RAGas\n",
    "\n",
    "# Run evaluation\n",
    "scores = evaluate(\n",
    "    evaluation_dataset,\n",
    "    metrics=[\n",
    "        answer_correctness,\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llm,  # pass the LLM explicitly\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"Evaluation dataset preview:\")\n",
    "for i, row in enumerate(evaluation_dataset):\n",
    "    if i >= 5: #only print the first 5 Q&A result\n",
    "        break\n",
    "    print(f\"{i+1}. Q: {row['question']}\")\n",
    "    print(f\"   Answer: {row['answer']}\")\n",
    "    print(f\"   Reference: {row['reference']}\")\n",
    "    print(f\"   Contexts: {row['contexts']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a1aff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS scores:\n",
      " Answer correctness: mean=0.4701, std=0.2603\n",
      " Answer relevancy:   mean=0.9503, std=0.1192\n",
      " Faithfulness:       mean=0.4361, std=0.3741\n",
      " Context precision:  mean=0.6948, std=0.4199\n",
      " Context recall:     mean=0.7403, std=0.4385\n"
     ]
    }
   ],
   "source": [
    "# print RAGAS result\n",
    "print(\n",
    "    f\"RAGAS scores:\"\n",
    "    f\"\\n Answer correctness: mean={np.mean(scores['answer_correctness']):.4f}, std={np.std(scores['answer_correctness']):.4f}\"\n",
    "    f\"\\n Answer relevancy:   mean={np.mean(scores['answer_relevancy']):.4f}, std={np.std(scores['answer_relevancy']):.4f}\"\n",
    "    f\"\\n Faithfulness:       mean={np.mean(scores['faithfulness']):.4f}, std={np.std(scores['faithfulness']):.4f}\"\n",
    "    f\"\\n Context precision:  mean={np.mean(scores['context_precision']):.4f}, std={np.std(scores['context_precision']):.4f}\"\n",
    "    f\"\\n Context recall:     mean={np.mean(scores['context_recall']):.4f}, std={np.std(scores['context_recall']):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f3ee04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Running config: chunk=300, overlap=0, model=text-embedding-3-small, top_k=2\n",
      "Split 48 pages into 469 chunks.\n",
      "\n",
      "Saved 469 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  32%|███▏      | 16/50 [00:56<01:27,  2.59s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  46%|████▌     | 23/50 [01:22<01:31,  3.38s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  54%|█████▍    | 27/50 [01:31<00:58,  2.53s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  66%|██████▌   | 33/50 [01:47<00:33,  1.99s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  76%|███████▌  | 38/50 [01:59<00:27,  2.32s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 50/50 [02:30<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Running config: chunk=300, overlap=50, model=text-embedding-3-small, top_k=2\n",
      "Split 48 pages into 510 chunks.\n",
      "\n",
      "Saved 510 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   2%|▏         | 1/50 [00:03<03:10,  3.90s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  38%|███▊      | 19/50 [01:02<01:13,  2.36s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  40%|████      | 20/50 [01:13<02:02,  4.09s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  58%|█████▊    | 29/50 [01:37<00:49,  2.34s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  68%|██████▊   | 34/50 [01:55<00:48,  3.02s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 50/50 [02:32<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Running config: chunk=300, overlap=100, model=text-embedding-3-small, top_k=2\n",
      "Split 48 pages into 620 chunks.\n",
      "\n",
      "Saved 620 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:   2%|▏         | 1/50 [00:04<03:16,  4.01s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  36%|███▌      | 18/50 [01:02<01:18,  2.44s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  46%|████▌     | 23/50 [01:14<00:55,  2.07s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  48%|████▊     | 24/50 [01:27<01:47,  4.14s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  74%|███████▍  | 37/50 [02:00<00:27,  2.12s/it]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|██████████| 50/50 [02:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation complete! Results saved to ragas_auto_eval_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>chunk_overlap</th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>answer_correctness_mean</th>\n",
       "      <th>answer_correctness_std</th>\n",
       "      <th>answer_relevancy_mean</th>\n",
       "      <th>answer_relevancy_std</th>\n",
       "      <th>faithfulness_mean</th>\n",
       "      <th>faithfulness_std</th>\n",
       "      <th>context_precision_mean</th>\n",
       "      <th>context_precision_std</th>\n",
       "      <th>context_recall_mean</th>\n",
       "      <th>context_recall_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>2</td>\n",
       "      <td>0.451644</td>\n",
       "      <td>0.217702</td>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.040333</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.320156</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.458258</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>50</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505532</td>\n",
       "      <td>0.245905</td>\n",
       "      <td>0.878463</td>\n",
       "      <td>0.293491</td>\n",
       "      <td>0.561667</td>\n",
       "      <td>0.346093</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.458258</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.458258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>2</td>\n",
       "      <td>0.410277</td>\n",
       "      <td>0.189378</td>\n",
       "      <td>0.983396</td>\n",
       "      <td>0.025447</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.361421</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.458258</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.458258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chunk_size  chunk_overlap         embedding_model  top_k  \\\n",
       "0         300              0  text-embedding-3-small      2   \n",
       "1         300             50  text-embedding-3-small      2   \n",
       "2         300            100  text-embedding-3-small      2   \n",
       "\n",
       "   answer_correctness_mean  answer_correctness_std  answer_relevancy_mean  \\\n",
       "0                 0.451644                0.217702               0.960114   \n",
       "1                 0.505532                0.245905               0.878463   \n",
       "2                 0.410277                0.189378               0.983396   \n",
       "\n",
       "   answer_relevancy_std  faithfulness_mean  faithfulness_std  \\\n",
       "0              0.040333           0.516667          0.320156   \n",
       "1              0.293491           0.561667          0.346093   \n",
       "2              0.025447           0.441667          0.361421   \n",
       "\n",
       "   context_precision_mean  context_precision_std  context_recall_mean  \\\n",
       "0                     0.7               0.458258                  0.9   \n",
       "1                     0.7               0.458258                  0.7   \n",
       "2                     0.7               0.458258                  0.7   \n",
       "\n",
       "   context_recall_std  \n",
       "0            0.300000  \n",
       "1            0.458258  \n",
       "2            0.458258  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Load environment variables that contains the OpenAI API key, LangChain can access it automatically\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"    # LLMs paper\n",
    "\n",
    "# Example parameter grid\n",
    "# chunk_sizes = [256, 512, 1024]\n",
    "# chunk_overlaps = [0, 50]\n",
    "# embedding_models = [\"text-embedding-3-small\", \"text-embedding-3-large\"]\n",
    "# retriever_top_k = [2, 5]\n",
    "\n",
    "chunk_sizes = [300]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "embedding_models = [\"text-embedding-3-small\"]\n",
    "retriever_top_k = [2]\n",
    "\n",
    "results = []\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "for chunk_size, overlap, emb_model, top_k in itertools.product(chunk_sizes, chunk_overlaps, embedding_models, retriever_top_k):\n",
    "    print(f\"\\n🔧 Running config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "\n",
    "    # --- Step 1: Build your retriever & RAG pipeline ---\n",
    "    chunks = split_text(documents, chunk_size, overlap)\n",
    "    db = save_to_chroma(chunks, CHROMA_PATH, emb_model)\n",
    "\n",
    "    # generate answer in evaluation format and change it to dataset\n",
    "    qa_file = \"data/BoardGamesRuleBook/CATAN_Q&A_Eval.txt\"\n",
    "    evaluation_rows = generate_answer_in_eval_format(qa_file, db, top_k)\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Explicitly pass to model kwargs)  \n",
    "\n",
    "    # Run evaluation\n",
    "    scores = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,  # pass the LLM explicitly\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Collect results ---\n",
    "    results.append({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"embedding_model\": emb_model,\n",
    "        \"top_k\": top_k,\n",
    "        \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "        \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "        \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "        \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "        \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "        \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "        \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "        \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "        \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "        \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "    })\n",
    "\n",
    "# --- Step 4: Save and inspect ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ragas_auto_eval_results.csv\", index=False)\n",
    "print(\"\\n✅ Evaluation complete! Results saved to ragas_auto_eval_results.csv\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "# ============================================================================\n",
    "# 🆕 NEW IMPORTS - Using Modern Libraries\n",
    "# ============================================================================\n",
    "import time\n",
    "import evaluate as hf_evaluate  # Hugging Face evaluate library\n",
    "from ranx import Qrels, Run, evaluate as ranx_evaluate\n",
    "\n",
    "# Load metrics once (more efficient)\n",
    "bleu_metric = hf_evaluate.load(\"bleu\")\n",
    "rouge_metric = hf_evaluate.load(\"rouge\")\n",
    "exact_match_metric = hf_evaluate.load(\"exact_match\")\n",
    "# BERTScore (optional - slower but more accurate)\n",
    "try:\n",
    "    bertscore_metric = hf_evaluate.load(\"bertscore\")\n",
    "    BERTSCORE_AVAILABLE = True\n",
    "except:\n",
    "    BERTSCORE_AVAILABLE = False\n",
    "    print(\"⚠️  BERTScore not available. Install with: pip install bert-score\")\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "# Load environment variables that contains the OpenAI API key\n",
    "load_dotenv()\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"\n",
    "\n",
    "# Example parameter grid\n",
    "chunk_sizes = [300]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "embedding_models = [\"text-embedding-3-small\"]\n",
    "retriever_top_k = [2]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 🆕 SIMPLIFIED HELPER FUNCTIONS (Much Cleaner!)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_doc_id_from_context(context_text):\n",
    "    \"\"\"\n",
    "    Helper to extract a document identifier from context\n",
    "    Using first 100 chars as unique identifier\n",
    "    \"\"\"\n",
    "    return context_text[:100]\n",
    "\n",
    "\n",
    "def identify_relevant_docs(contexts, reference_answer):\n",
    "    \"\"\"\n",
    "    Identify which retrieved contexts are actually relevant\n",
    "    Returns dict mapping query_id -> {doc_id: relevance_score}\n",
    "    \n",
    "    This is a heuristic - in production you'd want ground truth labels\n",
    "    \"\"\"\n",
    "    relevant_scores = {}\n",
    "    \n",
    "    # Simple heuristic: TF-IDF style relevance\n",
    "    ref_terms = set(reference_answer.lower().split())\n",
    "    \n",
    "    for ctx in contexts:\n",
    "        ctx_terms = set(ctx.lower().split())\n",
    "        # Overlap ratio as relevance score\n",
    "        overlap = len(ref_terms & ctx_terms) / len(ref_terms) if ref_terms else 0\n",
    "        \n",
    "        doc_id = extract_doc_id_from_context(ctx)\n",
    "        # Binary relevance: 1 if overlap > threshold, 0 otherwise\n",
    "        relevant_scores[doc_id] = 1 if overlap > 0.2 else 0\n",
    "    \n",
    "    return relevant_scores\n",
    "\n",
    "\n",
    "def calculate_retrieval_metrics(retrieved_contexts, reference_answer, k, query_id=\"q1\"):\n",
    "    \"\"\"\n",
    "    Calculate all retrieval metrics using ranx library\n",
    "    \n",
    "    Returns dict of metrics: precision@k, recall@k, mrr, ndcg@k, hit_rate@k\n",
    "    \"\"\"\n",
    "    # Build ground truth relevance (qrels)\n",
    "    # In production, you'd have pre-labeled relevant documents\n",
    "    relevant_scores = identify_relevant_docs(retrieved_contexts, reference_answer)\n",
    "    \n",
    "    qrels_dict = {query_id: relevant_scores}\n",
    "    qrels = Qrels(qrels_dict)\n",
    "    \n",
    "    # Build retrieval run results\n",
    "    # Using position-based scores (higher rank = higher score)\n",
    "    run_dict = {query_id: {}}\n",
    "    for i, ctx in enumerate(retrieved_contexts):\n",
    "        doc_id = extract_doc_id_from_context(ctx)\n",
    "        # Score decreases with rank (1.0, 0.9, 0.8, ...)\n",
    "        score = 1.0 - (i * 0.1)\n",
    "        run_dict[query_id][doc_id] = score\n",
    "    \n",
    "    run = Run(run_dict)\n",
    "    \n",
    "    # Calculate all metrics at once using ranx\n",
    "    metrics_to_calc = [\n",
    "        f\"precision@{k}\",\n",
    "        f\"recall@{k}\", \n",
    "        f\"ndcg@{k}\",\n",
    "        \"mrr\",\n",
    "        f\"hit_rate@{k}\"\n",
    "    ]\n",
    "    \n",
    "    results = ranx_evaluate(qrels, run, metrics_to_calc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_answer_metrics(reference, prediction):\n",
    "    \"\"\"\n",
    "    Calculate all answer quality metrics using Hugging Face evaluate\n",
    "    \n",
    "    Returns dict with bleu, rouge, exact_match, and optionally bertscore\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # BLEU score\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[[reference]]\n",
    "    )\n",
    "    metrics['bleu'] = bleu_result['bleu']\n",
    "    \n",
    "    # ROUGE scores (L, 1, 2)\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[reference]\n",
    "    )\n",
    "    metrics['rouge_l'] = rouge_result['rougeL']\n",
    "    metrics['rouge_1'] = rouge_result['rouge1']\n",
    "    metrics['rouge_2'] = rouge_result['rouge2']\n",
    "    \n",
    "    # Exact Match\n",
    "    em_result = exact_match_metric.compute(\n",
    "        predictions=[prediction],\n",
    "        references=[reference]\n",
    "    )\n",
    "    metrics['exact_match'] = em_result['exact_match']\n",
    "    \n",
    "    # BERTScore (optional - slower but captures semantic similarity)\n",
    "    if BERTSCORE_AVAILABLE:\n",
    "        try:\n",
    "            bert_result = bertscore_metric.compute(\n",
    "                predictions=[prediction],\n",
    "                references=[reference],\n",
    "                lang=\"en\"\n",
    "            )\n",
    "            metrics['bertscore_f1'] = bert_result['f1'][0]\n",
    "        except:\n",
    "            metrics['bertscore_f1'] = None\n",
    "    \n",
    "    # Simple token F1 (fast alternative to BERTScore)\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    \n",
    "    if len(pred_tokens) > 0 and len(ref_tokens) > 0:\n",
    "        common = ref_tokens & pred_tokens\n",
    "        precision = len(common) / len(pred_tokens)\n",
    "        recall = len(common) / len(ref_tokens)\n",
    "        token_f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    else:\n",
    "        token_f1 = 0.0\n",
    "    \n",
    "    metrics['token_f1'] = token_f1\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# ============================================================================\n",
    "# END OF HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "results = []\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "for chunk_size, overlap, emb_model, top_k in itertools.product(chunk_sizes, chunk_overlaps, embedding_models, retriever_top_k):\n",
    "    print(f\"\\n🔧 Running config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "\n",
    "    # --- Step 1: Build your retriever & RAG pipeline ---\n",
    "    chunks = split_text(documents, chunk_size, overlap)\n",
    "    db = save_to_chroma(chunks, CHROMA_PATH, emb_model)\n",
    "\n",
    "    # generate answer in evaluation format and change it to dataset\n",
    "    qa_file = \"data/BoardGamesRuleBook/CATAN_Q&A_Eval.txt\"\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 🆕 SIMPLIFIED EVALUATION LOOP\n",
    "    # ========================================================================\n",
    "    evaluation_rows = []\n",
    "    \n",
    "    # Aggregated metrics across all queries\n",
    "    all_retrieval_metrics = []\n",
    "    all_answer_metrics = []\n",
    "    retrieval_times = []\n",
    "    generation_times = []\n",
    "    total_tokens = []\n",
    "    \n",
    "    # Parse Q&A file\n",
    "    qa_rows = []\n",
    "    current_game = None\n",
    "    question = None\n",
    "\n",
    "    with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if line.lower().startswith(\"board_game:\"):\n",
    "                current_game = line[len(\"board_game:\"):].strip()\n",
    "            elif line.lower().startswith(\"q:\"):\n",
    "                question = line[len(\"q:\"):].strip()\n",
    "            elif line.lower().startswith(\"a:\"):\n",
    "                answer = line[len(\"a:\"):].strip()\n",
    "                if current_game is None:\n",
    "                    raise ValueError(f\"Answer found without specifying board game: {line}\")\n",
    "                if question is None:\n",
    "                    raise ValueError(f\"Answer found without a question: {line}\")\n",
    "                qa_rows.append({\n",
    "                    \"board_game\": current_game,\n",
    "                    \"question\": question,\n",
    "                    \"reference\": answer\n",
    "                })\n",
    "                question = None\n",
    "    \n",
    "    # Process each Q&A pair\n",
    "    for i, row in enumerate(qa_rows):\n",
    "        query = row[\"question\"]\n",
    "        ground_truth = row[\"reference\"]\n",
    "        board_game = row[\"board_game\"]\n",
    "        query_id = f\"q{i+1}\"\n",
    "        \n",
    "        # Measure retrieval time\n",
    "        retrieval_start = time.time()\n",
    "        context = db.similarity_search_with_relevance_scores(query, k=top_k)\n",
    "        retrieval_time = time.time() - retrieval_start\n",
    "        retrieval_times.append(retrieval_time)\n",
    "        \n",
    "        # Extract contexts\n",
    "        contexts = [doc.page_content for doc, _score in context]\n",
    "        \n",
    "        # Measure generation time\n",
    "        generation_start = time.time()\n",
    "        answer = generate_answer(query, context)\n",
    "        generation_time = time.time() - generation_start\n",
    "        generation_times.append(generation_time)\n",
    "        \n",
    "        # Track tokens (approximate)\n",
    "        tokens = sum(len(ctx.split()) for ctx in contexts) + len(answer.content.split())\n",
    "        total_tokens.append(tokens)\n",
    "        \n",
    "        # 🆕 Calculate retrieval metrics using ranx (one line!)\n",
    "        retrieval_metrics = calculate_retrieval_metrics(contexts, ground_truth, top_k, query_id)\n",
    "        all_retrieval_metrics.append(retrieval_metrics)\n",
    "        \n",
    "        # 🆕 Calculate answer metrics using HF evaluate (one line!)\n",
    "        answer_metrics = calculate_answer_metrics(ground_truth, answer.content)\n",
    "        all_answer_metrics.append(answer_metrics)\n",
    "        \n",
    "        # Add to evaluation dataset for RAGAs\n",
    "        evaluation_rows.append({\n",
    "            \"question\": query,\n",
    "            \"contexts\": contexts,\n",
    "            \"answer\": answer.content,\n",
    "            \"reference\": ground_truth,\n",
    "            \"source\": board_game\n",
    "        })\n",
    "    \n",
    "    evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "    # ========================================================================\n",
    "    # END OF EVALUATION LOOP\n",
    "    # ========================================================================\n",
    "\n",
    "    # sync interface compatible with RAGas\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    # Run RAGAs evaluation\n",
    "    scores = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # 🆕 AGGREGATE METRICS (Much Cleaner!)\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Helper to safely get metric values\n",
    "    def safe_mean(metric_list, key):\n",
    "        values = [m.get(key, 0) for m in metric_list if m.get(key) is not None]\n",
    "        return np.mean(values) if values else 0.0\n",
    "    \n",
    "    def safe_std(metric_list, key):\n",
    "        values = [m.get(key, 0) for m in metric_list if m.get(key) is not None]\n",
    "        return np.std(values) if values else 0.0\n",
    "    \n",
    "    results.append({\n",
    "        # Configuration\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"embedding_model\": emb_model,\n",
    "        \"top_k\": top_k,\n",
    "        \n",
    "        # RAGAs metrics (existing)\n",
    "        \"answer_correctness_mean\": np.mean(scores[\"answer_correctness\"]),\n",
    "        \"answer_correctness_std\": np.std(scores[\"answer_correctness\"]),\n",
    "        \"answer_relevancy_mean\": np.mean(scores[\"answer_relevancy\"]),\n",
    "        \"answer_relevancy_std\": np.std(scores[\"answer_relevancy\"]),\n",
    "        \"faithfulness_mean\": np.mean(scores[\"faithfulness\"]),\n",
    "        \"faithfulness_std\": np.std(scores[\"faithfulness\"]),\n",
    "        \"context_precision_mean\": np.mean(scores[\"context_precision\"]),\n",
    "        \"context_precision_std\": np.std(scores[\"context_precision\"]),\n",
    "        \"context_recall_mean\": np.mean(scores[\"context_recall\"]),\n",
    "        \"context_recall_std\": np.std(scores[\"context_recall\"]),\n",
    "        \n",
    "        # 🆕 Retrieval metrics (from ranx)\n",
    "        f\"precision@{top_k}_mean\": safe_mean(all_retrieval_metrics, f\"precision@{top_k}\"),\n",
    "        f\"precision@{top_k}_std\": safe_std(all_retrieval_metrics, f\"precision@{top_k}\"),\n",
    "        f\"recall@{top_k}_mean\": safe_mean(all_retrieval_metrics, f\"recall@{top_k}\"),\n",
    "        f\"recall@{top_k}_std\": safe_std(all_retrieval_metrics, f\"recall@{top_k}\"),\n",
    "        \"mrr_mean\": safe_mean(all_retrieval_metrics, \"mrr\"),\n",
    "        \"mrr_std\": safe_std(all_retrieval_metrics, \"mrr\"),\n",
    "        f\"hit_rate@{top_k}_mean\": safe_mean(all_retrieval_metrics, f\"hit_rate@{top_k}\"),\n",
    "        f\"hit_rate@{top_k}_std\": safe_std(all_retrieval_metrics, f\"hit_rate@{top_k}\"),\n",
    "        f\"ndcg@{top_k}_mean\": safe_mean(all_retrieval_metrics, f\"ndcg@{top_k}\"),\n",
    "        f\"ndcg@{top_k}_std\": safe_std(all_retrieval_metrics, f\"ndcg@{top_k}\"),\n",
    "        \n",
    "        # 🆕 Answer quality metrics (from HF evaluate)\n",
    "        \"bleu_mean\": safe_mean(all_answer_metrics, \"bleu\"),\n",
    "        \"bleu_std\": safe_std(all_answer_metrics, \"bleu\"),\n",
    "        \"rouge_l_mean\": safe_mean(all_answer_metrics, \"rouge_l\"),\n",
    "        \"rouge_l_std\": safe_std(all_answer_metrics, \"rouge_l\"),\n",
    "        \"rouge_1_mean\": safe_mean(all_answer_metrics, \"rouge_1\"),\n",
    "        \"rouge_2_mean\": safe_mean(all_answer_metrics, \"rouge_2\"),\n",
    "        \"token_f1_mean\": safe_mean(all_answer_metrics, \"token_f1\"),\n",
    "        \"token_f1_std\": safe_std(all_answer_metrics, \"token_f1\"),\n",
    "        \"exact_match_rate\": safe_mean(all_answer_metrics, \"exact_match\"),\n",
    "        \"bertscore_f1_mean\": safe_mean(all_answer_metrics, \"bertscore_f1\") if BERTSCORE_AVAILABLE else None,\n",
    "        \n",
    "        # 🆕 Performance metrics\n",
    "        \"avg_retrieval_time_ms\": np.mean(retrieval_times) * 1000,\n",
    "        \"avg_generation_time_ms\": np.mean(generation_times) * 1000,\n",
    "        \"avg_total_time_ms\": np.mean([r + g for r, g in zip(retrieval_times, generation_times)]) * 1000,\n",
    "        \"avg_tokens_per_query\": np.mean(total_tokens),\n",
    "        \"total_queries\": len(qa_rows),\n",
    "    })\n",
    "    # ========================================================================\n",
    "    # END OF METRICS COLLECTION\n",
    "    # ========================================================================\n",
    "\n",
    "# --- Step 4: Save and inspect ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ragas_auto_eval_results_enhanced.csv\", index=False)\n",
    "print(\"\\n✅ Evaluation complete! Results saved to ragas_auto_eval_results_enhanced.csv\")\n",
    "\n",
    "# ============================================================================\n",
    "# 🆕 ENHANCED DISPLAY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display key metrics grouped by category\n",
    "print(\"\\n🎯 RAGAS Semantic Metrics:\")\n",
    "ragas_cols = [\"chunk_size\", \"chunk_overlap\", \"top_k\",\n",
    "              \"answer_correctness_mean\", \"answer_relevancy_mean\", \n",
    "              \"faithfulness_mean\", \"context_precision_mean\", \"context_recall_mean\"]\n",
    "display(df[ragas_cols].round(3))\n",
    "\n",
    "print(\"\\n🔍 Retrieval Metrics (ranx):\")\n",
    "retrieval_cols = [\"chunk_size\", \"chunk_overlap\", \"top_k\",\n",
    "                  f\"precision@{retriever_top_k[0]}_mean\", \n",
    "                  f\"recall@{retriever_top_k[0]}_mean\",\n",
    "                  \"mrr_mean\", f\"ndcg@{retriever_top_k[0]}_mean\",\n",
    "                  f\"hit_rate@{retriever_top_k[0]}_mean\"]\n",
    "display(df[retrieval_cols].round(3))\n",
    "\n",
    "print(\"\\n📝 Answer Quality Metrics (HF evaluate):\")\n",
    "answer_cols = [\"chunk_size\", \"chunk_overlap\", \"top_k\",\n",
    "               \"bleu_mean\", \"rouge_l_mean\", \"rouge_1_mean\",\n",
    "               \"token_f1_mean\", \"exact_match_rate\"]\n",
    "display(df[answer_cols].round(3))\n",
    "\n",
    "print(\"\\n⏱️  Performance Metrics:\")\n",
    "perf_cols = [\"chunk_size\", \"chunk_overlap\", \"avg_retrieval_time_ms\", \n",
    "             \"avg_generation_time_ms\", \"avg_total_time_ms\", \"avg_tokens_per_query\"]\n",
    "display(df[perf_cols].round(2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 Installation reminder:\")\n",
    "print(\"   pip install evaluate ranx bert-score nltk rouge-score\")\n",
    "print(\"=\"*80)\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fca2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_correctness,\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Hugging Face retrieval metrics\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "# Load environment variables (OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/BoardGamesRuleBook\"  # LLMs paper\n",
    "\n",
    "# Parameter grid\n",
    "chunk_sizes = [300]\n",
    "chunk_overlaps = [0, 50, 100]\n",
    "embedding_models = [\"text-embedding-3-small\"]\n",
    "retriever_top_k = [2]\n",
    "\n",
    "# Load dataset and documents\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "# Initialize Hugging Face retrieval metrics\n",
    "hf_metrics = {\n",
    "    \"precision\": hf_evaluate.load(\"precision\", module_type=\"metric\"),\n",
    "    \"recall\": hf_evaluate.load(\"recall\", module_type=\"metric\"),\n",
    "    \"mrr\": hf_evaluate.load(\"mrr\", module_type=\"metric\"),\n",
    "    \"ndcg\": hf_evaluate.load(\"ndcg\", module_type=\"metric\"),\n",
    "    \"hit_rate\": hf_evaluate.load(\"hit_rate\", module_type=\"metric\"),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for chunk_size, overlap, emb_model, top_k in itertools.product(\n",
    "    chunk_sizes, chunk_overlaps, embedding_models, retriever_top_k\n",
    "):\n",
    "    print(f\"\\n🔧 Running config: chunk={chunk_size}, overlap={overlap}, model={emb_model}, top_k={top_k}\")\n",
    "\n",
    "    # --- Step 1: Build your retriever & RAG pipeline ---\n",
    "    chunks = split_text(documents, chunk_size, overlap)\n",
    "    db = save_to_chroma(chunks, CHROMA_PATH, emb_model)\n",
    "\n",
    "    # --- Step 2: Prepare evaluation dataset ---\n",
    "    qa_file = \"data/BoardGamesRuleBook/CATAN_Q&A_Eval.txt\"\n",
    "    evaluation_rows = generate_answer_in_eval_format(qa_file, db, top_k)\n",
    "    evaluation_dataset = Dataset.from_list(evaluation_rows)\n",
    "\n",
    "    # RAGas evaluation\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    ragas_scores = evaluate(\n",
    "        evaluation_dataset,\n",
    "        metrics=[\n",
    "            answer_correctness,\n",
    "            answer_relevancy,\n",
    "            faithfulness,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,\n",
    "    )\n",
    "\n",
    "    # --- Step 3: Hugging Face retrieval evaluation ---\n",
    "    # HF retrieval metrics require:\n",
    "    # - predictions: list of lists of top-k document ids\n",
    "    # - references: list of lists of relevant document ids\n",
    "    predictions = [row[\"retrieved_chunk_ids\"] for row in evaluation_rows]\n",
    "    references = [row[\"relevant_chunk_ids\"] for row in evaluation_rows]\n",
    "\n",
    "    hf_scores = {}\n",
    "    for name, metric in hf_metrics.items():\n",
    "        hf_scores[name] = metric.compute(predictions=predictions, references=references, k=top_k)\n",
    "\n",
    "    # --- Step 4: Collect results ---\n",
    "    results.append({\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"embedding_model\": emb_model,\n",
    "        \"top_k\": top_k,\n",
    "        # RAGas metrics\n",
    "        \"answer_correctness_mean\": np.mean(ragas_scores[\"answer_correctness\"]),\n",
    "        \"answer_correctness_std\": np.std(ragas_scores[\"answer_correctness\"]),\n",
    "        \"answer_relevancy_mean\": np.mean(ragas_scores[\"answer_relevancy\"]),\n",
    "        \"answer_relevancy_std\": np.std(ragas_scores[\"answer_relevancy\"]),\n",
    "        \"faithfulness_mean\": np.mean(ragas_scores[\"faithfulness\"]),\n",
    "        \"faithfulness_std\": np.std(ragas_scores[\"faithfulness\"]),\n",
    "        \"context_precision_mean\": np.mean(ragas_scores[\"context_precision\"]),\n",
    "        \"context_precision_std\": np.std(ragas_scores[\"context_precision\"]),\n",
    "        \"context_recall_mean\": np.mean(ragas_scores[\"context_recall\"]),\n",
    "        \"context_recall_std\": np.std(ragas_scores[\"context_recall\"]),\n",
    "        # HF retrieval metrics\n",
    "        \"precision_at_k\": hf_scores[\"precision\"][\"precision\"],\n",
    "        \"recall_at_k\": hf_scores[\"recall\"][\"recall\"],\n",
    "        \"mrr_at_k\": hf_scores[\"mrr\"][\"mrr\"],\n",
    "        \"ndcg_at_k\": hf_scores[\"ndcg\"][\"ndcg\"],\n",
    "        \"hit_rate_at_k\": hf_scores[\"hit_rate\"][\"hit_rate\"],\n",
    "    })\n",
    "\n",
    "# --- Step 5: Save results ---\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ragas_hf_eval_results.csv\", index=False)\n",
    "print(\"\\n✅ Evaluation complete! Results saved to ragas_hf_eval_results.csv\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0d683",
   "metadata": {},
   "source": [
    "Reference<br>\n",
    "[How to Evaluate Retrieval Quality in RAG Pipelines (part 2): Mean Reciprocal Rank (MRR) and Average Precision (AP)](https://towardsdatascience.com/how-to-evaluate-retrieval-quality-in-rag-pipelines-part-2-mean-reciprocal-rank-mrr-and-average-precision-ap/)<br>\n",
    "[How to Evaluate Retrieval Quality in RAG Pipelines: Precision@k, Recall@k, and F1@k](https://towardsdatascience.com/how-to-evaluate-retrieval-quality-in-rag-pipelines-precisionk-recallk-and-f1k/)<br>\n",
    "[RAG Evaluation: Don’t let customers tell you first](https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/)<br>\n",
    "[Demystifying NDCG](https://medium.com/data-science/demystifying-ndcg-bee3be58cfe0)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
