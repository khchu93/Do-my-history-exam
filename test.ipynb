{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9741f3",
   "metadata": {},
   "source": [
    "### Building a Local Knowledge Assistant with LangChain & OpenAI\n",
    "\n",
    "In this project, I implemented a `Retrieval-Augmented Generation (RAG)` pipeline using `LangChain`, `Chroma`, and `OpenAI embeddings`. The system retrieves relevant chunks of information from my document collection to provide more accurate and context-aware answers. By combining retrieval with generation, I can leverage a smaller, pre-trained LLM while still achieving detailed and precise responses, without the need for costly fine-tuning. This setup allows me to experiment with AI-driven Q&A in a practical, hands-on way.\n",
    "\n",
    "This project was inspired by [RAG + Langchain Python Project: Easy AI/Chat For Your Docs](https://www.youtube.com/watch?v=tcqEUSNCn8I) and adapted for personal learning and experimentation.\n",
    "\n",
    "\n",
    "### Why RAG Matters\n",
    "`Fine-tuning` an LLM can be extremely **expensive** and **resource-heavy** — it requires access to large compute clusters, massive datasets, and careful optimization.\n",
    "\n",
    "While techniques like `LoRA (Low-Rank Adaptation)` make fine-tuning more **efficient**, they still **demand GPU resources** and **model-specific expertise**.\n",
    "\n",
    "`RAG (Retrieval-Augmented Generation)`, on the other hand, is a **lightweight yet powerful alternative**.\n",
    "Instead of changing the model itself, it simply **augments the prompt** by adding relevant **external knowledge** retrieved from a local or online database.\n",
    "\n",
    "This makes RAG easy to implement, cost-effective, and highly adaptable — perfect for building intelligent assistants that can use your own data without retraining.\n",
    "\n",
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deae934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from \"RAG + Langchain Python Project: Easy AI/Chat For Your Docs\"\n",
    "# https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "# from langchain_classic.evaluation import load_evaluator\n",
    "import openai \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ca6c1",
   "metadata": {},
   "source": [
    "The project begins by **converting the target documents into LangChain** `Document` **objects** using the `DirectoryLoader` function. This preserves both the **text** and **metadata**, such as the source path and start index, which are required for LangChain's downstream functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the raw files into Document\n",
    "def load_documents(DATA_PATH):\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e062cf",
   "metadata": {},
   "source": [
    "Next, the documents are **split into smaller chunks**. This is necessary primarily for **retrieval precision in RAG systems**. Smaller chunks allow the system to retrieve **only the most relevant information** rather than entire documents, improving semantic similarity matching. **Large chunks** can **dilute semantic meaning** and introduce **irrelevant context**, while **smaller chunks** enable more **focused retrieval**. Additionally, chunking helps manage **LLM context window limitations**, though retrieval quality is the primary consideration.\n",
    "\n",
    "The 'RecursiveCharacterTextSplitter' controls how text is divided using two key parameters: 'chunk_size' and 'chunk_overlap'. 'chunk_size' defines the *maximum length* of each chunk, while 'chunk_overlap' *repeats a portion* of the previous chunk in the next one to **reduce the chance of splitting related information** across chunk boundaries. Choosing appropriate values for these parameters directly impacts retrieval quality, and thus the overall performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into small chunks\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,         #nb of characters in each chunk\n",
    "        chunk_overlap=100,      #nb of characters to overlap between chunks\n",
    "        length_function=len,    #decide how to measure the chunk, e.g., character, token, etc\n",
    "        add_start_index=True,   #add the starting index of the chunk\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\\n\")\n",
    "\n",
    "    print(f\"Print chunk 10 content: \")\n",
    "    document = chunks[10]\n",
    "    print(f\"Content: \\\"{document.page_content}\\\"\")\n",
    "    print(f\"Metadata: {document.metadata}\\n\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916d6bb",
   "metadata": {},
   "source": [
    "Once the documents are split, each chunk is converted into a **vector embedding** using OpenAI's 'text-embedding-3-small' model. These **embeddings** are then stored in a 'Chroma' vector database along with the **original chunk text and metadata**. The 'Chroma' database enables efficient **similarity search** by **comparing query embeddings against stored chunk embeddings**.\n",
    "\n",
    "It is crucial to use the **same embedding model** for both **indexing chunks** and **embedding queries** because different models produce **incompatible vector representations** that exist in different semantic spaces, making retrieval unreliable or impossible. For this project, I use OpenAI's cost-efficient 'text-embedding-3-small' model consistently for both indexing and query embedding, which balances functionality with experimentation costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33a2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vector embedding to chunks and save the embedding vector along with the content and metadata to database\n",
    "def save_to_chroma(chunks: list[Document], CHROMA_PATH):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    db = Chroma.from_documents(\n",
    "        documents=chunks, embedding=OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c29e2",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b93f75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 818 chunks.\n",
      "\n",
      "Print chunk 10 content: \n",
      "Content: \"So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\"\n",
      "Metadata: {'source': 'data\\\\books\\\\alice_in_wonderland.md', 'start_index': 1653}\n",
      "\n",
      "Saved 818 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables that contains the OpenAI API key\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key \n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# Set path for where to get the original file and where to safe the chunks\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "\n",
    "# split the document in chunks and save it to database along with its embedded vector\n",
    "documents = load_documents(DATA_PATH)\n",
    "chunks = split_text(documents)\n",
    "db = save_to_chroma(chunks, CHROMA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4ac1a",
   "metadata": {},
   "source": [
    "The **prompt template** determines how **retrieved context** and the **user query** are **structured** for the **language model**. While prompt quality is difficult to quantify precisely, adhering to **established prompting principles significantly improves model responses**. Effective LLM prompting requires **clear, specific instructions** - similar to providing detailed directions to a new team member. Well-defined prompts guide the model toward desired outputs, while vague or ambiguous instructions increase output unpredictability, often resulting in irrelevant or inaccurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5ed89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3d87c",
   "metadata": {},
   "source": [
    "Once the Chroma database and prompt template are configured, the system can process user queries through the following pipeline: First, the query is embedded using the **same embedding model** applied to the document chunks, ensuring vector space consistency. The system then retrieves the **top-k most similar chunks** from the vector database (using L2 (Euclidean) distance by default). These retrieved chunks are combined with the original query according to the prompt template structure and passed to the LLM for response generation.\n",
    "\n",
    "Implementing **quality safeguards** is essential for production systems. This includes **rejecting empty** or **malformed queries** and filtering results when similarity scores fall **below a confidence threshold**, as low-similarity retrievals typically indicate insufficient relevant context and lead to unreliable outputs. With these components in place, the RAG pipeline can effectively retrieve pertinent information and generate well-informed responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17466451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top k similarity:\n",
      "Top 1:\n",
      "Content: So Alice began telling them her adventures from the time when she first saw the White Rabbit. She was a little nervous about it just at first, the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide, but she gained courage as she went on. Her listeners\n",
      "L2 similarity: 0.8063871770832316\n",
      "\n",
      "Top 2:\n",
      "Content: “In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "L2 similarity: 0.8054134795155087\n",
      "\n",
      "Top 3:\n",
      "Content: “Is that the way you manage?” Alice asked.\n",
      "\n",
      "The Hatter shook his head mournfully. “Not I!” he replied. “We quarrelled last March—just before he went mad, you know—” (pointing with his tea spoon at the March Hare,) “—it was at the great concert given by the Queen of Hearts, and I had to sing\n",
      "L2 similarity: 0.7902825080190067\n",
      "\n",
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "So Alice began telling them her adventures from the time when she first saw the White Rabbit. She was a little nervous about it just at first, the two creatures got so close to her, one on each side, and opened their eyes and mouths so very wide, but she gained courage as she went on. Her listeners\n",
      "\n",
      "---\n",
      "\n",
      "“In that direction,” the Cat said, waving its right paw round, “lives a Hatter: and in that direction,” waving the other paw, “lives a March Hare. Visit either you like: they’re both mad.”\n",
      "\n",
      "“But I don’t want to go among mad people,” Alice remarked.\n",
      "\n",
      "---\n",
      "\n",
      "“Is that the way you manage?” Alice asked.\n",
      "\n",
      "The Hatter shook his head mournfully. “Not I!” he replied. “We quarrelled last March—just before he went mad, you know—” (pointing with his tea spoon at the March Hare,) “—it was at the great concert given by the Queen of Hearts, and I had to sing\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: How does Alice meet the Mad Hatter?\n",
      "\n",
      "Response: Alice meets the Mad Hatter when the Cheshire Cat points her in the direction of where he lives, and she decides to visit either the Hatter or the March Hare.\n",
      "Sources: ['data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md', 'data\\\\books\\\\alice_in_wonderland.md']\n"
     ]
    }
   ],
   "source": [
    "# query_text = input(\"Enter your query: \")\n",
    "query_text = \"How does Alice meet the Mad Hatter?\"\n",
    "\n",
    "# # Prepare the DB.\n",
    "# db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# Search the DB.\n",
    "results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "    # return\n",
    "\n",
    "print(\"Top k similarity:\")\n",
    "for i, k in enumerate(results):\n",
    "    print(f\"Top {i + 1}:\\nContent: {k[0].page_content}\\nL2 similarity: {k[1]}\\n\")\n",
    "\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "print(prompt)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "response_text = model.invoke(prompt)\n",
    "\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "formatted_response = f\"Response: {response_text.content}\\nSources: {sources}\"\n",
    "print(formatted_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
